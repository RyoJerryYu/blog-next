<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="description" content="The blog owned by Ryo, about Programing, Painting, and Gaming." data-next-head=""/><meta property="og:description" content="The blog owned by Ryo, about Programing, Painting, and Gaming." data-next-head=""/><meta name="twitter:description" content="The blog owned by Ryo, about Programing, Painting, and Gaming." data-next-head=""/><meta property="og:image" content="https://ryojerryyu.github.io/blog-next/img/home-bg-kasumi-hanabi.jpg" data-next-head=""/><meta name="twitter:image" content="https://ryojerryyu.github.io/blog-next/img/home-bg-kasumi-hanabi.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta property="og:url" content="https://blog.ryo-okami.xyz/tags/cloud-native" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@ryo_okami" data-next-head=""/><meta name="twitter:creator" content="@ryo_okami" data-next-head=""/><link rel="icon" href="/blog-next/favicon.ico" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1" data-next-head=""/><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests" data-next-head=""/><title data-next-head="">Cloud Native | Ryo&#x27;s Blog</title><meta property="og:title" content="Cloud Native" data-next-head=""/><meta property="og:site_name" content="Ryo&#x27;s Blog" data-next-head=""/><meta name="twitter:title" content="Cloud Native | Ryo&#x27;s Blog" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="192x192" href="/android-chrome-192x192.png"/><link rel="manifest" href="/site.webmanifest"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/blog-next/_next/static/css/34188d5bb2e356b9.css" as="style"/><link rel="stylesheet" href="/blog-next/_next/static/css/34188d5bb2e356b9.css" data-n-g=""/><link rel="preload" href="/blog-next/_next/static/css/f9baacb09663d66a.css" as="style"/><link rel="stylesheet" href="/blog-next/_next/static/css/f9baacb09663d66a.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/blog-next/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/blog-next/_next/static/chunks/webpack-3597c977fede260d.js" defer=""></script><script src="/blog-next/_next/static/chunks/framework-898697981b1ca118.js" defer=""></script><script src="/blog-next/_next/static/chunks/main-a0a5fc4e603d1ed2.js" defer=""></script><script src="/blog-next/_next/static/chunks/pages/_app-861c1dc7f33f622c.js" defer=""></script><script src="/blog-next/_next/static/chunks/4785-70b0f2db3df81cbd.js" defer=""></script><script src="/blog-next/_next/static/chunks/pages/tags/%5Btag%5D-ece8349242176668.js" defer=""></script><script src="/blog-next/_next/static/8XSB2OWvzgvsvl9OJIqWc/_buildManifest.js" defer=""></script><script src="/blog-next/_next/static/8XSB2OWvzgvsvl9OJIqWc/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="DefaultLayout_header__aepTD"><div class="DefaultLayout_icon__11sTk"><div class="DefaultLayout_textbox__H9FZG"><a class="DefaultLayout_textlink__EVwys" href="/blog-next">Ryo&#x27;s Blog</a></div></div><div class="DefaultLayout_navBar__gY4ra"><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/articles">Articles</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/ideas">Ideas</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/learn_from_ai">Learn from AI</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/tags">Tags</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/clips">Clips</a></div></div><div class="DefaultLayout_headerRight__0Kj26"><div class=" flex flex-row gap-8 items-center justify-center"><a title="Twitter" href="https://twitter.com/ryo_okami"><svg class=" h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a title="GitHub" href="https://github.com/RyoJerryYu"><svg class=" h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a title="Pixiv" href="https://www.pixiv.net/users/9159893"><svg class=" h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4.935 0A4.924 4.924 0 0 0 0 4.935v14.13A4.924 4.924 0 0 0 4.935 24h14.13A4.924 4.924 0 0 0 24 19.065V4.935A4.924 4.924 0 0 0 19.065 0zm7.81 4.547c2.181 0 4.058.676 5.399 1.847a6.118 6.118 0 0 1 2.116 4.66c.005 1.854-.88 3.476-2.257 4.563-1.375 1.092-3.225 1.697-5.258 1.697-2.314 0-4.46-.842-4.46-.842v2.718c.397.116 1.048.365.635.779H5.79c-.41-.41.19-.65.644-.779V7.666c-1.053.81-1.593 1.51-1.868 2.031.32 1.02-.284.969-.284.969l-1.09-1.73s3.868-4.39 9.553-4.39zm-.19.971c-1.423-.003-3.184.473-4.27 1.244v8.646c.988.487 2.484.832 4.26.832h.01c1.596 0 2.98-.593 3.93-1.533.952-.948 1.486-2.183 1.492-3.683-.005-1.54-.504-2.864-1.42-3.86-.918-.992-2.274-1.645-4.002-1.646Z"></path></svg></a></div></div></header><div class="DefaultLayout_headerBg__FStmg"></div><div class="max-w-3xl mx-auto p-2"><div class="DefaultLayout_contentHeight__DabjQ"><div class="max-w-3xl mx-auto p-2"><div class="p-2"><div class="TagsBox_tagsBox__WzhAf"><a href="/blog-next/tags/%E6%9D%82%E6%8A%80"><div class="TagsBox_tag__Rk32C">杂技</div></a><a href="/blog-next/tags/blog"><div class="TagsBox_tag__Rk32C">Blog</div></a><a href="/blog-next/tags/%E6%9D%82%E8%B0%88"><div class="TagsBox_tag__Rk32C">杂谈</div></a><a href="/blog-next/tags/c++"><div class="TagsBox_tag__Rk32C">C++</div></a><a href="/blog-next/tags/python"><div class="TagsBox_tag__Rk32C">Python</div></a><a href="/blog-next/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><div class="TagsBox_tag__Rk32C">数据结构</div></a><a href="/blog-next/tags/%E7%AE%97%E6%B3%95"><div class="TagsBox_tag__Rk32C">算法</div></a><a href="/blog-next/tags/%E6%8E%92%E5%BA%8F"><div class="TagsBox_tag__Rk32C">排序</div></a><a href="/blog-next/tags/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B"><div class="TagsBox_tag__Rk32C">算法竞赛</div></a><a href="/blog-next/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F"><div class="TagsBox_tag__Rk32C">设计模式</div></a><a href="/blog-next/tags/%E7%AC%94%E8%AE%B0"><div class="TagsBox_tag__Rk32C">笔记</div></a><a href="/blog-next/tags/github"><div class="TagsBox_tag__Rk32C">GitHub</div></a><a href="/blog-next/tags/aws"><div class="TagsBox_tag__Rk32C">AWS</div></a><a href="/blog-next/tags/ci-cd"><div class="TagsBox_tag__Rk32C">CI/CD</div></a><a href="/blog-next/tags/iac"><div class="TagsBox_tag__Rk32C">IaC</div></a><a href="/blog-next/tags/devops"><div class="TagsBox_tag__Rk32C">DevOps</div></a><a href="/blog-next/tags/vscode"><div class="TagsBox_tag__Rk32C">VSCode</div></a><a href="/blog-next/tags/hexo"><div class="TagsBox_tag__Rk32C">Hexo</div></a><a href="/blog-next/tags/javascript"><div class="TagsBox_tag__Rk32C">JavaScript</div></a><a href="/blog-next/tags/kubernetes"><div class="TagsBox_tag__Rk32C">Kubernetes</div></a><a href="/blog-next/tags/docker"><div class="TagsBox_tag__Rk32C">Docker</div></a><a href="/blog-next/tags/cloud-native"><div class="TagsBox_highlightedTag__cLTHz">Cloud Native</div></a><a href="/blog-next/tags/cursor"><div class="TagsBox_tag__Rk32C">Cursor</div></a><a href="/blog-next/tags/nextjs"><div class="TagsBox_tag__Rk32C">Nextjs</div></a><a href="/blog-next/tags/linux"><div class="TagsBox_tag__Rk32C">Linux</div></a><a href="/blog-next/tags/systemctl"><div class="TagsBox_tag__Rk32C">systemctl</div></a><a href="/blog-next/tags/journalctl"><div class="TagsBox_tag__Rk32C">journalctl</div></a><a href="/blog-next/tags/timedatectl"><div class="TagsBox_tag__Rk32C">timedatectl</div></a><a href="/blog-next/tags/basicknowledge"><div class="TagsBox_tag__Rk32C">BasicKnowledge</div></a><a href="/blog-next/tags/operation"><div class="TagsBox_tag__Rk32C">Operation</div></a><a href="/blog-next/tags/signal"><div class="TagsBox_tag__Rk32C">Signal</div></a><a href="/blog-next/tags/memory"><div class="TagsBox_tag__Rk32C">memory</div></a><a href="/blog-next/tags/schedule"><div class="TagsBox_tag__Rk32C">schedule</div></a><a href="/blog-next/tags/cloud-computing"><div class="TagsBox_tag__Rk32C">Cloud Computing</div></a><a href="/blog-next/tags/pytorch"><div class="TagsBox_tag__Rk32C">PyTorch</div></a><a href="/blog-next/tags/onnx"><div class="TagsBox_tag__Rk32C">ONNX</div></a><a href="/blog-next/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><div class="TagsBox_tag__Rk32C">深度学习</div></a><a href="/blog-next/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2"><div class="TagsBox_tag__Rk32C">模型部署</div></a><a href="/blog-next/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0"><div class="TagsBox_tag__Rk32C">学习笔记</div></a><a href="/blog-next/tags/opencv"><div class="TagsBox_tag__Rk32C">OpenCV</div></a><a href="/blog-next/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86"><div class="TagsBox_tag__Rk32C">图像处理</div></a></div></div><div class="PostList_postList__Tuobz"><div class="PostList_postListElement__qi6kp"><a href="/blog-next/articles/introduction-for-k8s-2"><h6 class="PostList_postTitle__vveJr">Kubernetes 入门 （2）</h6><div class="PostList_postDate__z_XQh"><time dateTime="2022-08-20T21:56:52.000Z">2022-08-20</time></div><div class="PostList_postAbstract__HtPA1"><p class="py-1">我们之前说的都是用于部署 Pod 的资源，我们接下来介绍与创建 Pod 不相关的资源：储存与网络。</p><p class="py-1">其实我们之前已经接触过储存相关的内容了：在讲 Stateful Set 时我们提过 Stateful Set 创建出来的 Pod 都会有相互独立的储存；而讲 Daemon Set 时我们提到 K8s 推荐只在 Daemon Set 的 Pod 中访问宿主机磁盘。但独立的储存具体指什么？除了访问宿主机磁盘以外还有什么其他的储存？</p><p class="py-1">在 Docker 中，我们可以把宿主机磁盘上的一个路径作为一个 Volume 来给容器绑定，或者直接使用 Docker Engine 管理的 Volume 来提供持久化存储或是容器间共享文件。在 K8s 里面也沿用了 Volume 这个概念，可以通过 Mount 绑定到容器内的路径，并通过实现 CSI 的各种引擎来提供更多样的存储。</p></div></a><div class="TagsBox_tagsBox__WzhAf py-4 md:py-1"><a href="/blog-next/tags/kubernetes"><div class="TagsBox_tag__Rk32C">Kubernetes</div></a><a href="/blog-next/tags/devops"><div class="TagsBox_tag__Rk32C">DevOps</div></a><a href="/blog-next/tags/docker"><div class="TagsBox_tag__Rk32C">Docker</div></a><a href="/blog-next/tags/cloud-native"><div class="TagsBox_tag__Rk32C">Cloud Native</div></a></div></div><div class="PostList_postListElement__qi6kp"><a href="/blog-next/articles/introduction-for-k8s"><h6 class="PostList_postTitle__vveJr">Kubernetes 入门 （1）</h6><div class="PostList_postDate__z_XQh"><time dateTime="2022-08-13T17:45:31.000Z">2022-08-13</time></div><div class="PostList_postAbstract__HtPA1"><p class="py-1">我们知道 K8s 利用了容器虚拟化技术。而说到容器虚拟化就要说 Docker 。可是，容器到底是什么？ Docker 又为我们做了些什么？我们又为什么要用 K8s ？</p><p class="py-1">&gt; 要把一个不知道打过多少个升级补丁，不知道经历了多少任管理员的系统迁移到其他机器上，毫无疑问会是一场灾难。 —— Chad Fowler 《Trash Your Servers and Burn Your Code》</p><p class="py-1">&quot;Write once, run anywhere&quot; 是 Java 曾经的口号。 Java 企图通过 JVM 虚拟机来实现一个可执行程序在多平台间的移植性。但我们现在知道， Java 语言并没能实现他的目标，会在操作系统调用、第三方依赖丢失、两个程序间依赖的冲突等各方面出现问题。</p></div></a><div class="TagsBox_tagsBox__WzhAf py-4 md:py-1"><a href="/blog-next/tags/kubernetes"><div class="TagsBox_tag__Rk32C">Kubernetes</div></a><a href="/blog-next/tags/devops"><div class="TagsBox_tag__Rk32C">DevOps</div></a><a href="/blog-next/tags/docker"><div class="TagsBox_tag__Rk32C">Docker</div></a><a href="/blog-next/tags/cloud-native"><div class="TagsBox_tag__Rk32C">Cloud Native</div></a></div></div><div class="PostList_postListElement__qi6kp"><a href="/blog-next/ideas/newest"><h6 class="PostList_postTitle__vveJr">Kubernetes 入门 （1）</h6><div class="PostList_postDate__z_XQh"><time dateTime="2022-08-13T17:45:31.000Z">2022-08-13</time></div><div class="PostList_postAbstract__HtPA1"><p class="py-1">这里是第一行，</p><p class="py-1">然后这里是第二行。</p><p class="py-1">这里是一些内容。</p></div></a><div class="TagsBox_tagsBox__WzhAf py-4 md:py-1"><a href="/blog-next/tags/kubernetes"><div class="TagsBox_tag__Rk32C">Kubernetes</div></a><a href="/blog-next/tags/devops"><div class="TagsBox_tag__Rk32C">DevOps</div></a><a href="/blog-next/tags/docker"><div class="TagsBox_tag__Rk32C">Docker</div></a><a href="/blog-next/tags/cloud-native"><div class="TagsBox_tag__Rk32C">Cloud Native</div></a><a href="/blog-next/tags/cloud-computing"><div class="TagsBox_tag__Rk32C">Cloud Computing</div></a></div></div></div></div></div></div><footer class="DefaultLayout_footer__n5339"><div class="max-w-3xl mx-auto p-2 w-full"><div class="flex flex-row justify-center items-center"><div class="DefaultLayout_footerLeft__j0yvY">© 2023 Ryo Jerry Yu. All rights reserved.</div><div class="DefaultLayout_footerRight___Dn67"><div class=" flex flex-row gap-8 items-center justify-center"><a title="Twitter" href="https://twitter.com/ryo_okami"><svg class="DefaultLayout_footerIcon__sgrmB h-8 w-8" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a title="GitHub" href="https://github.com/RyoJerryYu"><svg class="DefaultLayout_footerIcon__sgrmB h-8 w-8" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a title="Pixiv" href="https://www.pixiv.net/users/9159893"><svg class="DefaultLayout_footerIcon__sgrmB h-8 w-8" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4.935 0A4.924 4.924 0 0 0 0 4.935v14.13A4.924 4.924 0 0 0 4.935 24h14.13A4.924 4.924 0 0 0 24 19.065V4.935A4.924 4.924 0 0 0 19.065 0zm7.81 4.547c2.181 0 4.058.676 5.399 1.847a6.118 6.118 0 0 1 2.116 4.66c.005 1.854-.88 3.476-2.257 4.563-1.375 1.092-3.225 1.697-5.258 1.697-2.314 0-4.46-.842-4.46-.842v2.718c.397.116 1.048.365.635.779H5.79c-.41-.41.19-.65.644-.779V7.666c-1.053.81-1.593 1.51-1.868 2.031.32 1.02-.284.969-.284.969l-1.09-1.73s3.868-4.39 9.553-4.39zm-.19.971c-1.423-.003-3.184.473-4.27 1.244v8.646c.988.487 2.484.832 4.26.832h.01c1.596 0 2.98-.593 3.93-1.533.952-.948 1.486-2.183 1.492-3.683-.005-1.54-.504-2.864-1.42-3.86-.918-.992-2.274-1.645-4.002-1.646Z"></path></svg></a></div></div></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"allTagInfos":[{"tag":"杂技","slug":"杂技","path":"/tags/杂技","postSlugs":[{"postType":"articles","postSlug":"Building-this-blog","postPagePath":"/articles/Building-this-blog"},{"postType":"articles","postSlug":"hello-world","postPagePath":"/articles/hello-world"},{"postType":"articles","postSlug":"the-using-in-cpp","postPagePath":"/articles/the-using-in-cpp"}]},{"tag":"Blog","slug":"blog","path":"/tags/blog","postSlugs":[{"postType":"articles","postSlug":"Building-this-blog","postPagePath":"/articles/Building-this-blog"},{"postType":"articles","postSlug":"init-a-new-hexo-project","postPagePath":"/articles/init-a-new-hexo-project"},{"postType":"articles","postSlug":"create-blog-cicd-by-github","postPagePath":"/articles/create-blog-cicd-by-github"},{"postType":"articles","postSlug":"use-paste-image-and-vscode-memo","postPagePath":"/articles/use-paste-image-and-vscode-memo"},{"postType":"ideas","postSlug":"blog-in-next","postPagePath":"/ideas/blog-in-next"},{"postType":"ideas","postSlug":"blog-syntax","postPagePath":"/ideas/blog-syntax"}]},{"tag":"杂谈","slug":"杂谈","path":"/tags/杂谈","postSlugs":[{"postType":"articles","postSlug":"hello-world","postPagePath":"/articles/hello-world"},{"postType":"articles","postSlug":"try-cursor-and-thinking","postPagePath":"/articles/try-cursor-and-thinking"}]},{"tag":"C++","slug":"c++","path":"/tags/c++","postSlugs":[{"postType":"articles","postSlug":"the-using-in-cpp","postPagePath":"/articles/the-using-in-cpp"}]},{"tag":"Python","slug":"python","path":"/tags/python","postSlugs":[{"postType":"articles","postSlug":"python-dict","postPagePath":"/articles/python-dict"}]},{"tag":"数据结构","slug":"数据结构","path":"/tags/数据结构","postSlugs":[{"postType":"articles","postSlug":"python-dict","postPagePath":"/articles/python-dict"},{"postType":"articles","postSlug":"Sort-algorithm","postPagePath":"/articles/Sort-algorithm"},{"postType":"articles","postSlug":"Handy-heap-cheat-sheet","postPagePath":"/articles/Handy-heap-cheat-sheet"}]},{"tag":"算法","slug":"算法","path":"/tags/算法","postSlugs":[{"postType":"articles","postSlug":"Sort-algorithm","postPagePath":"/articles/Sort-algorithm"},{"postType":"articles","postSlug":"Handy-heap-cheat-sheet","postPagePath":"/articles/Handy-heap-cheat-sheet"}]},{"tag":"排序","slug":"排序","path":"/tags/排序","postSlugs":[{"postType":"articles","postSlug":"Sort-algorithm","postPagePath":"/articles/Sort-algorithm"}]},{"tag":"算法竞赛","slug":"算法竞赛","path":"/tags/算法竞赛","postSlugs":[{"postType":"articles","postSlug":"Handy-heap-cheat-sheet","postPagePath":"/articles/Handy-heap-cheat-sheet"}]},{"tag":"设计模式","slug":"设计模式","path":"/tags/设计模式","postSlugs":[{"postType":"articles","postSlug":"The-beauty-of-design-parten","postPagePath":"/articles/The-beauty-of-design-parten"}]},{"tag":"笔记","slug":"笔记","path":"/tags/笔记","postSlugs":[{"postType":"articles","postSlug":"The-beauty-of-design-parten","postPagePath":"/articles/The-beauty-of-design-parten"}]},{"tag":"GitHub","slug":"github","path":"/tags/github","postSlugs":[{"postType":"articles","postSlug":"create-blog-cicd-by-github","postPagePath":"/articles/create-blog-cicd-by-github"}]},{"tag":"AWS","slug":"aws","path":"/tags/aws","postSlugs":[{"postType":"articles","postSlug":"create-blog-cicd-by-github","postPagePath":"/articles/create-blog-cicd-by-github"}]},{"tag":"CI/CD","slug":"ci-cd","path":"/tags/ci-cd","postSlugs":[{"postType":"articles","postSlug":"create-blog-cicd-by-github","postPagePath":"/articles/create-blog-cicd-by-github"}]},{"tag":"IaC","slug":"iac","path":"/tags/iac","postSlugs":[{"postType":"articles","postSlug":"create-blog-cicd-by-github","postPagePath":"/articles/create-blog-cicd-by-github"}]},{"tag":"DevOps","slug":"devops","path":"/tags/devops","postSlugs":[{"postType":"articles","postSlug":"create-blog-cicd-by-github","postPagePath":"/articles/create-blog-cicd-by-github"},{"postType":"articles","postSlug":"introduction-for-k8s","postPagePath":"/articles/introduction-for-k8s"},{"postType":"articles","postSlug":"introduction-for-k8s-2","postPagePath":"/articles/introduction-for-k8s-2"},{"postType":"ideas","postSlug":"newest","postPagePath":"/ideas/newest"}]},{"tag":"VSCode","slug":"vscode","path":"/tags/vscode","postSlugs":[{"postType":"articles","postSlug":"use-paste-image-and-vscode-memo","postPagePath":"/articles/use-paste-image-and-vscode-memo"}]},{"tag":"Hexo","slug":"hexo","path":"/tags/hexo","postSlugs":[{"postType":"articles","postSlug":"use-paste-image-and-vscode-memo","postPagePath":"/articles/use-paste-image-and-vscode-memo"}]},{"tag":"JavaScript","slug":"javascript","path":"/tags/javascript","postSlugs":[{"postType":"articles","postSlug":"use-paste-image-and-vscode-memo","postPagePath":"/articles/use-paste-image-and-vscode-memo"}]},{"tag":"Kubernetes","slug":"kubernetes","path":"/tags/kubernetes","postSlugs":[{"postType":"articles","postSlug":"introduction-for-k8s","postPagePath":"/articles/introduction-for-k8s"},{"postType":"articles","postSlug":"introduction-for-k8s-2","postPagePath":"/articles/introduction-for-k8s-2"},{"postType":"ideas","postSlug":"newest","postPagePath":"/ideas/newest"}]},{"tag":"Docker","slug":"docker","path":"/tags/docker","postSlugs":[{"postType":"articles","postSlug":"introduction-for-k8s","postPagePath":"/articles/introduction-for-k8s"},{"postType":"articles","postSlug":"introduction-for-k8s-2","postPagePath":"/articles/introduction-for-k8s-2"},{"postType":"ideas","postSlug":"newest","postPagePath":"/ideas/newest"}]},{"tag":"Cloud Native","slug":"cloud-native","path":"/tags/cloud-native","postSlugs":[{"postType":"articles","postSlug":"introduction-for-k8s","postPagePath":"/articles/introduction-for-k8s"},{"postType":"articles","postSlug":"introduction-for-k8s-2","postPagePath":"/articles/introduction-for-k8s-2"},{"postType":"ideas","postSlug":"newest","postPagePath":"/ideas/newest"}]},{"tag":"Cursor","slug":"cursor","path":"/tags/cursor","postSlugs":[{"postType":"articles","postSlug":"try-cursor-and-thinking","postPagePath":"/articles/try-cursor-and-thinking"}]},{"tag":"Nextjs","slug":"nextjs","path":"/tags/nextjs","postSlugs":[{"postType":"ideas","postSlug":"blog-in-next","postPagePath":"/ideas/blog-in-next"},{"postType":"ideas","postSlug":"blog-syntax","postPagePath":"/ideas/blog-syntax"}]},{"tag":"Linux","slug":"linux","path":"/tags/linux","postSlugs":[{"postType":"ideas","postSlug":"Linux Systemd","postPagePath":"/ideas/Linux Systemd"},{"postType":"ideas","postSlug":"Linux 信号处理 —— Signal","postPagePath":"/ideas/Linux 信号处理 —— Signal"},{"postType":"ideas","postSlug":"Linux 内存 —— 内存分页、分段","postPagePath":"/ideas/Linux 内存 —— 内存分页、分段"},{"postType":"ideas","postSlug":"Linux 内存 —— 堆和栈","postPagePath":"/ideas/Linux 内存 —— 堆和栈"},{"postType":"ideas","postSlug":"Linux 内存 —— 虚拟内存","postPagePath":"/ideas/Linux 内存 —— 虚拟内存"},{"postType":"ideas","postSlug":"Linux 调度 —— 进程与线程","postPagePath":"/ideas/Linux 调度 —— 进程与线程"}]},{"tag":"systemctl","slug":"systemctl","path":"/tags/systemctl","postSlugs":[{"postType":"ideas","postSlug":"Linux Systemd","postPagePath":"/ideas/Linux Systemd"}]},{"tag":"journalctl","slug":"journalctl","path":"/tags/journalctl","postSlugs":[{"postType":"ideas","postSlug":"Linux Systemd","postPagePath":"/ideas/Linux Systemd"}]},{"tag":"timedatectl","slug":"timedatectl","path":"/tags/timedatectl","postSlugs":[{"postType":"ideas","postSlug":"Linux Systemd","postPagePath":"/ideas/Linux Systemd"}]},{"tag":"BasicKnowledge","slug":"basicknowledge","path":"/tags/basicknowledge","postSlugs":[{"postType":"ideas","postSlug":"Linux Systemd","postPagePath":"/ideas/Linux Systemd"},{"postType":"ideas","postSlug":"Linux 信号处理 —— Signal","postPagePath":"/ideas/Linux 信号处理 —— Signal"},{"postType":"ideas","postSlug":"Linux 内存 —— 内存分页、分段","postPagePath":"/ideas/Linux 内存 —— 内存分页、分段"},{"postType":"ideas","postSlug":"Linux 内存 —— 堆和栈","postPagePath":"/ideas/Linux 内存 —— 堆和栈"},{"postType":"ideas","postSlug":"Linux 内存 —— 虚拟内存","postPagePath":"/ideas/Linux 内存 —— 虚拟内存"},{"postType":"ideas","postSlug":"Linux 调度 —— 进程与线程","postPagePath":"/ideas/Linux 调度 —— 进程与线程"}]},{"tag":"Operation","slug":"operation","path":"/tags/operation","postSlugs":[{"postType":"ideas","postSlug":"Linux Systemd","postPagePath":"/ideas/Linux Systemd"},{"postType":"ideas","postSlug":"Linux 信号处理 —— Signal","postPagePath":"/ideas/Linux 信号处理 —— Signal"},{"postType":"ideas","postSlug":"Linux 内存 —— 内存分页、分段","postPagePath":"/ideas/Linux 内存 —— 内存分页、分段"},{"postType":"ideas","postSlug":"Linux 内存 —— 虚拟内存","postPagePath":"/ideas/Linux 内存 —— 虚拟内存"}]},{"tag":"Signal","slug":"signal","path":"/tags/signal","postSlugs":[{"postType":"ideas","postSlug":"Linux 信号处理 —— Signal","postPagePath":"/ideas/Linux 信号处理 —— Signal"}]},{"tag":"memory","slug":"memory","path":"/tags/memory","postSlugs":[{"postType":"ideas","postSlug":"Linux 内存 —— 内存分页、分段","postPagePath":"/ideas/Linux 内存 —— 内存分页、分段"},{"postType":"ideas","postSlug":"Linux 内存 —— 堆和栈","postPagePath":"/ideas/Linux 内存 —— 堆和栈"},{"postType":"ideas","postSlug":"Linux 内存 —— 虚拟内存","postPagePath":"/ideas/Linux 内存 —— 虚拟内存"}]},{"tag":"schedule","slug":"schedule","path":"/tags/schedule","postSlugs":[{"postType":"ideas","postSlug":"Linux 调度 —— 进程与线程","postPagePath":"/ideas/Linux 调度 —— 进程与线程"}]},{"tag":"Cloud Computing","slug":"cloud-computing","path":"/tags/cloud-computing","postSlugs":[{"postType":"ideas","postSlug":"newest","postPagePath":"/ideas/newest"}]},{"tag":"PyTorch","slug":"pytorch","path":"/tags/pytorch","postSlugs":[{"postType":"learn_from_ai","postSlug":"deep-learning-model-formats","postPagePath":"/learn_from_ai/deep-learning-model-formats"}]},{"tag":"ONNX","slug":"onnx","path":"/tags/onnx","postSlugs":[{"postType":"learn_from_ai","postSlug":"deep-learning-model-formats","postPagePath":"/learn_from_ai/deep-learning-model-formats"}]},{"tag":"深度学习","slug":"深度学习","path":"/tags/深度学习","postSlugs":[{"postType":"learn_from_ai","postSlug":"deep-learning-model-formats","postPagePath":"/learn_from_ai/deep-learning-model-formats"}]},{"tag":"模型部署","slug":"模型部署","path":"/tags/模型部署","postSlugs":[{"postType":"learn_from_ai","postSlug":"deep-learning-model-formats","postPagePath":"/learn_from_ai/deep-learning-model-formats"}]},{"tag":"学习笔记","slug":"学习笔记","path":"/tags/学习笔记","postSlugs":[{"postType":"learn_from_ai","postSlug":"deep-learning-model-formats","postPagePath":"/learn_from_ai/deep-learning-model-formats"},{"postType":"learn_from_ai","postSlug":"opencv-coordinate-system-conventions","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"}]},{"tag":"OpenCV","slug":"opencv","path":"/tags/opencv","postSlugs":[{"postType":"learn_from_ai","postSlug":"opencv-coordinate-system-conventions","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"}]},{"tag":"图像处理","slug":"图像处理","path":"/tags/图像处理","postSlugs":[{"postType":"learn_from_ai","postSlug":"opencv-coordinate-system-conventions","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"}]}],"selectedTagInfo":{"tag":"Cloud Native","slug":"cloud-native","path":"/tags/cloud-native","postSlugs":[{"postType":"articles","postSlug":"introduction-for-k8s","postPagePath":"/articles/introduction-for-k8s"},{"postType":"articles","postSlug":"introduction-for-k8s-2","postPagePath":"/articles/introduction-for-k8s-2"},{"postType":"ideas","postSlug":"newest","postPagePath":"/ideas/newest"}]},"posts":[{"pathMapping":{"filePath":"public/content/articles/2022-08-20-introduction-for-k8s-2.md","pagePath":"/articles/introduction-for-k8s-2","slug":"introduction-for-k8s-2"},"meta":{"content":"\n我们之前说的都是用于部署 Pod 的资源，我们接下来介绍与创建 Pod 不相关的资源：储存与网络。\n\n# 储存\n\n其实我们之前已经接触过储存相关的内容了：在讲 Stateful Set 时我们提过 Stateful Set 创建出来的 Pod 都会有相互独立的储存；而讲 Daemon Set 时我们提到 K8s 推荐只在 Daemon Set 的 Pod 中访问宿主机磁盘。但独立的储存具体指什么？除了访问宿主机磁盘以外还有什么其他的储存？\n\n在 Docker 中，我们可以把宿主机磁盘上的一个路径作为一个 Volume 来给容器绑定，或者直接使用 Docker Engine 管理的 Volume 来提供持久化存储或是容器间共享文件。在 K8s 里面也沿用了 Volume 这个概念，可以通过 Mount 绑定到容器内的路径，并通过实现 CSI 的各种引擎来提供更多样的存储。\n\n\u003e CSI: Container Storage Interface ，容器储存接口标准，是 K8s 提出的一种规范。不管是哪种储存引擎，只要编写一个对应的插件实现 CSI ，都可以在 K8s 中使用。\n\n### K8s 中使用 Volume 与可用的 Volume 类型\n\n其实 K8s 中使用 Volume 的例子我们一开始就已经接触过了。还记得一开始介绍 Pod 时的 Nginx 例子吗？\n\n```yaml\nmetadata:\n  name: simple-webapp\nspec:\n  containers:\n    - name: main-application\n      image: nginx\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n    - name: sidecar-container\n      image: busybox\n      command: [\"sh\",\"-c\",\"while true; do cat /var/log/nginx/access.log; sleep 30; done\"]\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n  volumes:\n    - name: shared-logs\n      emptyDir: {}\n```\n\n这个 Pod 描述中声明了一个种类为 `emptyDir` 的，名为 `shared-logs` 的 Volume ，然后 Pod 中的两个容器都分别 Mount 了这个 Volume 。\n\nK8s 中默认提供了几种 Volume ，比如：\n\n- emptyDir ：一个简单的空目录，一般用于储存临时数据或是 Pod 的容器之间共享数据。\n- hostPath ：绑定到节点宿主机文件系统上的路径，一般在 Daemon Set 中使用。\n- gitRepo ：这种 Volume 其实相当于 emptyDir ，不过在 Pod 启动时会从 Git 仓库 clone 一份内容作为默认数据。\n- configMap 、 secret ：一般用于配置文件加载，需要与 configMap 、 secret 这两种资源一同使用。会将 configMap 、 secret 中对应的内容拷贝一份作为 Volume 绑到容器。（下一节中会展开讨论）\n- nfs 、 glusterfs 、 ……：可以通过各种网络存储协议直接挂载一个网络存储\n- (deprecated!) gcePersistentDisk 、 awsElasticBlockStore ……：可以调用各个云平台的 API ，创建一个块储存硬件挂载到宿主机上，再将那个硬件挂载到容器中。\n- persistentVolumeClaim ：持久卷声明，用于把实际储存方式抽象化，使得 Pod 不需要关心具体的储存类型。这种类型会在下面详细介绍。\n\n我们可以注意到， Volume 的声明是 Pod 的一个属性，而不是一种单独的资源。 Volume 是 Pod 的一部分，因此不同的 Pod 之间永远不可能共享同一个 Volume 。\n\n\u003e 但是 Volume 所指向的位置可以相同，比如 HostPath 类型的 Volume 就可以两个 Pod 可以绑定到宿主机上同一个路径，因此 Volume 里的数据还是能通过一定方式在 Pod 间共享。但当然 K8s 不推荐这么做。\n\n另外，由于 Volume 是 Pod 的一部分， Volume 的生命周期也是跟随 Pod 的，当一个 Pod 被销毁时， Volume 也会被销毁，因此最主要还是用于 Pod 内容器间的文件共享。如果需要持久化储存，需要使用 Persistent Volume 。\n\n\u003e Volume 会被销毁不代表 Volume 指向的内容会被销毁。比如 hostPath 、 NFS 等类型 Volume 中的内容就会继续保留在宿主机或是 NAS 上。下面提到的 Presistent Volume Claim 也是，拥有 `persistentVolumeClaim` 类型 Volume 的 Pod 被删除后对应的 PVC 不一定会被删除。\n\n### Presistent Volume 、 Presistent Volume Claim 、 Storage Class\n\n如果需要在 Pod 声明中直接指定 NFS 、 awsElasticBlockStore 之类的信息，就需要应用的开发人员对真实可用的储存结构有所理解，违背了 K8s 的理念。因此 K8s 就弄出了小标题中的三种资源来将储存抽象化。\n\n一个 Persistent Volume (PV) 对应云平台提供的一个块存储，或是 NAS 上的一个路径。可以简单地理解为 **PV 直接描述了一块可用的物理存储** 。因为 PV 直接对应到硬件，因此 PV 跟节点一样，是名称空间无关的。\n\n而一个 **Persistent Volume Claim (PVC) 则是描述了怎样去使用储存** ：使用多少空间、只读还是读写等。一个 PVC 被创建后会且只会对应到一个 PV 。 PVC 从属于一个名称空间，并能被该名称空间下的 Pod 指定为一个 Volume 。\n\nPV 与 PVC 这两种抽象是很必要的。试想一下用自己的物理机搭建一个 K8s 集群的场景。你会提前给物理机插上许多个储存硬件，这时你就需要用 PV 来描述这些硬件，之后才能在 K8s 里利用这些硬件的储存。而实际将应用部署到 K8s 中时，你才需要用 PVC 来描述 Pod 中需要怎么样的储存卷，然后 K8s 就会自动挑一个合适 PV 给这个 PVC 绑定上。这样实际部署应用的时候就不用再特意跑去机房给物理机插硬件了。\n\n但是现在都云原生时代了，各供应商都有提供 API 可以直接创建一个块储存，还要想办法提前准备 PV 实在是太蠢了。于是便需要 Storage Class 这种资源。\n\n使用 Storage Class 前需要先安装各种云供应商提供的插件（当然使用云服务提供的 K8s 的话一般已经准备好了），然后再创建一个 Storage Class 类型的资源（当然一般也已经准备好了）。下面是 AWS 上的 EKS 服务中默认自带的 Storage Class ：\n\n```yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\n  name: gp2\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  fsType: ext4\n  type: gp2\n# 当 PVC 被删除时会同时删除 PV\nreclaimPolicy: Delete\n# 只有当 PVC 被绑定为一个 Pod 的 Volume 时才会创建一个 PV\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n可以看到 EKS 自带的 gp2 提供了一些默认的选项，我们也可以类似地去定义自己的 Storage Class 。有了 gp2 这个 Storage Class ，我们创建一个 PVC 后 K8s 就会调用 AWS 的 API ，创建一个块储存接到我们的节点上，然后 K8s 再自动创建一个 PV 并绑定到 PVC 上。\n\n例如，我们部署 Kafka 时会创建一个这样的 PVC ：\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-kafka-0\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: gp2\n```\n\nK8s 就会自动为我们创建出一个对应的 PV ：\n\n```sh\n# `pvc-` 开头这个是 AWS 自动给我们起的名字。它虽然是 `pvc-` 开头，但他其实是一个 PV 。\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                STORAGECLASS   REASON   AGE\npvc-3614c15f-5697-4d66-a13c-6ddf7eb89998   10Gi       RWO            Delete           Bound    kafka/data-kafka-0   gp2                     152d\n```\n\n要是打开 AWS Console 还会发现， K8s 调用了 AWS 的 API ，自动为我们创建了一个 EBS 块储存并绑定到了我们对应的宿主机上。\n\n可以用下面这张图来表示 Pod 中的 Volume 、 PVC 、 PV 之间的关系：\n\n```mermaid\nflowchart TD\n\nsubgraph Pod[Pod: Kafka-0]\nsubgraph Container[Container: docker.io/bitnami/kafka:3.1.0]\nvm[VolumeMount: /bitnami/kafka]\nend\nvolume[(Volume: data)]\nvm --\u003e volume\nend\n\npvc[pvc: data-kafka-0]\npv[pv: pvc-3614c15f-5697-4d66-a13c-6ddf7eb89998]\nebs[ebs: AWS 为我们创建的块储存硬件]\n\nvolume --\u003e pvc\npvc --\u003e pv\npv --\u003e ebs\n```\n\n而 Storage Class 在上图中则负责读取我们提交的 PVC ，然后创建 PV 与 EBS 。\n\n### 再说回 Stateful Set\n\n之前我们提到 Stateful Set 时说到 Stateful Set 创建的 Pod 拥有固定的储存，到底是什么意思呢？跟 Deployment 的储存又有什么区别呢？\n\n我们先来看看，如果要给 Deployment 创建出来的 Pod 挂载 PVC 需要怎么做。下面是一个部署 Nginx 的 Deployment 清单，其中 html 目录下的静态文件存放在 NFS 里，通过 PVC 挂载到 Pod 中：\n\n```yaml\n# 这里省略了 Service 相关的内容\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-dpl-with-nfs-pvc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts: #挂载容器中的目录到 pvc nfs 中的目录\n        - name: www\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: www\n        persistentVolumeClaim: #指定pvc\n          claimName: nfs-pvc-for-nginx\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc-for-nginx\n  namespace: default\nspec:\n  storageclassname: \"\" # 指定使用现有 PV ，不使用 StorageClass 创建 PV\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\n# 这个例子中需要挂载 NFS 上的特定路径，所以手动定义了一个 PV\n# 一般情况下我们不会手动创建 PV，而是使用 StorageClass 自动创建\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv-for-nginx\nspec:\n  capacity: \n    storage: 1Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /nfs/sharefolder/nginx\n    server: 81.70.4.171\n```\n\n这份清单我们主要关注前两个资源，我们可以看到除了一个 Deployment 资源以外我们还单独定义了一个 PVC 资源。然后在 Deployment 的 Pod 模板中声明并绑定了这个 PVC 。\n\n可这样 apply 了之后会发生什么情况呢？因为我们只声明了一份 PVC ，当然我们只会拥有一个 PVC 资源。但这个 Deployment 的副本数是 3 ，因此我们会有 3 个相同的 Pod 去绑定同一个 PVC 。也就是最终会在 3 个容器里访问同一个 NFS 的同一个目录。如果我们在其中一个容器里对这个目录作修改，也会影响到另外两个容器。\n\n\u003e 注：这一现象不一定在任何情况下都适用。比如 AWS 的 EBS 卷只支持单个 AZ 内的绑定。如果 Pod 因为 Node Affinity 等设定被部署到了多个区，没法绑定同一个 EBS 卷，就会在 Scedule 的阶段报错。\n\n很多时候我们都不希望多个 Pod 绑定到同一 PVC 。比如我们部署一个 DB 集群的时候，如果好不容易部署出来的多个实例居然用的是同一份储存，就会显得很呆。 Stateful Set 就是为了解决这种情况，会为其管理下的每个 Pod 都部署一个专用的 PVC 。\n\n下面是给 Stateful Set 创建出来的 Pod 挂载 PVC 的一份清单：\n\n```yaml\n# 这里省略了 Service 相关的内容\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"nginx\"\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n```\n\n我们可以看到，部署 Stateful Set 时我们不能另外单独定义一份 PVC 了，只能作为 Stateful Set 定义的一部分，在 volumeClaimTemplates 字段中定义 PVC 的模板。这样一来， Stateful Set 会根据这个模板，为每个 Pod 创建一个对应的 PVC ，并作为 Pod 的 Volume 绑定上：\n\n```bash\n# Stateful Set 创建出来的 Pod ，名字都是按顺序的\n$ kubectl get pods -l app=nginx\nNAME      READY     STATUS    RESTARTS   AGE\nweb-0     1/1       Running   0          1m\nweb-1     1/1       Running   0          1m\n\n# Stateful Set 创建出来的 PVC ，名字与 Pod 的名字一一对应\n$ kubectl get pvc -l app=nginx\nNAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nwww-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s\nwww-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s\n```\n\n这样， Stateful Set 的多个 Pod 就会拥有自己的储存，不会相互打架了。另外，如果我们事先定义了 StorageClass ，还能根据 Stateful Set 的副本数动态配置 PV 。\n\n### ConfigMap 与 Secret 挂载作为特殊的卷\n\n有时候我们需要使用配置文件来配置应用（比如 Nginx 的配置文件），而且我们有时候会需要不重启 Pod 就热更新配置。如果用 PVC 来加载配置文件略微麻烦，这时候可以使用 Config Map 。\n\n下面是 K8s 官网上 Config Map 的一个例子：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-demo\ndata:\n  # 一个 Key 可以对应一个值\n  player_initial_lives: \"3\"\n  ui_properties_file_name: \"user-interface.properties\"\n\n  # 一个 Key 也可以对应一个文件的内容\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5    \n  user-interface.properties: |\n    color.good=purple\n    color.bad=yellow\n    allow.textmode=true    \n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-demo-pod\nspec:\n  containers:\n    - name: demo\n      image: alpine\n      command: [\"sleep\", \"3600\"]\n      env:\n        # ConfigMap 的 Key 可以作为环境变量引用\n        - name: PLAYER_INITIAL_LIVES\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo           # 从这个 Config Map 里\n              key: player_initial_lives # 拿到这个 key 的值\n        - name: UI_PROPERTIES_FILE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo\n              key: ui_properties_file_name\n      volumeMounts:\n      - name: config\n        mountPath: \"/config\"\n        readOnly: true\n  volumes:\n    # 定义 Pod 的 Volume ，种类为 configMap\n    - name: config\n      configMap:\n        name: game-demo # ConfigMap的名字\n        # 需要作为文件放入 Volume 的 Key\n        items:\n        - key: \"game.properties\"\n          path: \"game.properties\"\n        - key: \"user-interface.properties\"\n          path: \"user-interface.properties\"\n```\n\n我们可以看到 ConfigMap 里的 Key 可以作为文件或是环境变量加载到 Pod 中。另外，作为环境变量加载后其实还能作为命令行参数传入应用，实现各种配置方式。如果修改 Config map 的内容，也可以自动更新 Pod 中的文件。\n\n然而， Config Map 的热更新有一些不太灵活的地方：\n\n1. 作为环境变量加载的 Config Map 数据不会被热更新。想要更新这一部分数据需要重启 Pod。（当然，命令行参数也不能热更新）\n2. 由于 Kubelet 会先将 Config Map 内容加载到本地作为缓存，因此修改 Config Map 后新的内容不会第一时间加载到 Pod 中。而且在旧版本的 K8s 中， Config Map 被更新直到缓存被刷新的时间间隔还会很长，新版本的 K8s 这一部分有了优化，可以设定刷新时间，但会导致 API Server 的负担加重。（这其实是一个 Known Issue ，被诟病多年： https://github.com/kubernetes/kubernetes/issues/22368 ）\n\n除 Config Map 以外， K8s 还提供了一种叫 Secret 的资源，用法和 Config Map 几乎一样。对比 Config Map ，Secret 有以下几个特点：\n\n1. 在 Pod 里， Secret 只会被加载到内存中，而永远不会被写到磁盘上。\n2. 用 `kubectl get` 之类的命令显示的 Secret 内容会被用 base64 编码。（不过， well ，众所周知 base64 可不算是什么加密）\n3. 可以通过 K8s 的 Service Account 等 RBAC 相关的资源来控制 Secret 的访问权限。\n\n不过，由于 Secret 也是以明文的形式被存储在 K8s 的主节点中的，因此需要保证 K8s 主节点的安全。\n\n\u003e **Downward API 挂载作为特殊的卷**\n\u003e \n\u003e 还有另外一种叫 Downward API 的东西，可以作为 Volume 或是环境变量被加载到 Pod 中。有一些参数我们很难事先在 Manifest 中定义（ e.g. Deployment 生成的 Pod 的名字），因此可以通过 Downward API 来实现。\n\u003e \n\u003e ```yaml\n\u003e apiVersion: v1\n\u003e kind: Pod\n\u003e metadata:\n\u003e     name: test-volume-pod\n\u003e     namespace: kube-system\n\u003e     labels:\n\u003e         k8s-app: test-volume\n\u003e         node-env: test\n\u003e spec:\n\u003e     containers:\n\u003e     - name: test-volume-pod-container\n\u003e       image: busybox:latest\n\u003e       env:\n\u003e       - name: POD_NAME # 将 Pod 的名字作为环境变量 POD_NAME 加载到 Pod 中\n\u003e         valueFrom:\n\u003e           fieldRef:\n\u003e             fieldPath: metadata.name\n\u003e       command: [\"sh\", \"-c\"]\n\u003e       args:\n\u003e       - while true; do\n\u003e           cat /etc/podinfo/labels | echo;\n\u003e           env | sort | echo;\n\u003e           sleep 3600;\n\u003e         done;\n\u003e       volumeMounts:\n\u003e       - name: podinfo\n\u003e         mountPath: /etc/podinfo\n\u003e     volumes:\n\u003e     - name: podinfo\n\u003e       downwardAPI: # Downward API 类型的卷\n\u003e         items:\n\u003e         - path: \"labels\" # 将 Pod 的标签作为  labels 文件挂载到 Pod 中\n\u003e           fieldRef:\n\u003e             fieldPath: metadata.labels\n\u003e ```\n\n\n\n# 网络\n\n其实 Pod 只要部署好了，就会被分配到一个集群内部的 IP 地址，流量就可以通过 IP 地址来访问 Pod 了。然而通过可能会有很大问题： **Pod 随时会被杀死。** 虽然通过用 Deployment 等资源可以在挂掉后重新创建一个 Pod ，但那毕竟是不同的 Pod ， IP 已经改变。\n\n另外， Deployment 等资源的就是为了能更方便的做到多副本部署及任意缩容扩容而存在的。如果在 K8s 中访问 Pod 还需要小心翼翼地去找到 Pod 的 IP 地址，或是去寻找 Pod 是否部署了新副本， Deployment 等资源就几乎没有存在价值了。\n\n\u003e 其实 Pod 部署好后不止会被分配 IP 地址，还会被分配到一个类似 `\u003cpod-ip\u003e.\u003cnamespace\u003e.pod.cluster.local` 的 DNS 记录。例如一个位于 default 名字空间，IP 地址为 172.17.0.3 的 Pod ，对应 DNS 记录为 `172-17-0-3.default.pod.cluster.local` 。\n\n### Service\n\n在古代，人们是通过注册中心、服务发现、负载均衡等中间件来解决上面这些问题的，但这样很不云原生。于是 K8s 引入了 Service 这种资源，来实现简易的服务发现、 DNS 功能。\n\n下面是一个经典的例子，部署了一个 Service 和一个 Deployment：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth-service\n  labels:\n    app: auth\nspec:\n  type: ClusterIP\n  selector:\n    app: auth # 指向 Deployment 创建的 Pod\n  ports:\n  - port: 80 # Service 暴露的端口\n    targetPort: 8080 # Pod 的端口\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name:  auth\n  labels:\n    app: auth\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: auth\n  template:\n    metadata:\n      name: auth\n      labels:\n        app: auth\n    spec:\n      containers:\n      - name: auth\n        image: xxxxx.dkr.ecr.ap-northeast-1.amazonaws.com/auth:xxxxx\n        ports:\n        - containerPort: 8080\n```\n\n根据前面的知识我们知道，这份文件会部署 Deployment 会创建 2 个相同的 Pod 副本。另外还会部署一个名为 auth-service 的 Service 资源。这个 Service 暴露了一个 80 端口，并且指向那两个 Pod 的 8080 端口。\n\n而这份文件部署后， Service 资源就会在集群中注册一个 DNS A 记录（或 AAAA 记录），集群内其他 Pod （为了辨别我们叫它 Client ）就可以通过相同的 DNS 名称来访问 Deployment 部署的这 2 个 Pod ：\n\n```sh\ncurl http://auth-service.\u003cnamespace\u003e.svc.cluster.local:80\n# 或者省略掉后面的一大串\ncurl http://auth-service.\u003cnamespace\u003e:80\n# 如果 Client 和 Service 在同一个 Namespace 中，还可以：\ncurl http://auth-service:80\n```\n\n像这样 Client 通过 Service 来访问时，会随机访问到其中一个 Pod ，这样一来无论 Deployment 到底创建了多少个副本，只要副本的标签相同，就能通过同一个 DNS 名称来访问，还能自动实现一些简单的负载均衡。\n\n\u003e **为什么 DNS 名称可以简化？**\n\u003e \n\u003e Pod 被部署时， kubelet 会为每个 Pod 注入一个类似如下的 `/etc/resolv.conf` 文件：\n\u003e \n\u003e ```\n\u003e nameserver 10.32.0.10\n\u003e search \u003cnamespace\u003e.svc.cluster.local svc.cluster.local cluster.local\n\u003e options ndots:5\n\u003e ```\n\u003e \n\u003e Pod 中进行 DNS 查询时，默认会先读取这个文件，然后按照 `search` 选项中的内容展开 DNS 。例如，在 test 名称空间中的 Pod ，访问 data 时的查询可能被展开为 data.test.svc.cluster.local 。\n\u003e 更多关于 `/etc/resolv.conf` 文件的内容可参考 https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html\n\n### Service 的种类\n\n我们上面的例子中，可以看到 Service 资源有个字段 `type:ClusterIP` 。其实 Service 资源有以下几个种类：\n\n| 种类           | 作用                                                                                                                                                                                                                                                                                             |\n| :------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `ClusterIP`    | 这个类型的 Service 会在集群内创建一条 DNS A 记录并通过一定方法将流量代理到其指向的 Pod 上。这种 Service 不会暴露到集群外。这是最基础的 Service 种类。                                                                                                                                            |\n| `NodePort`     | 这种 Service 会在 ClusterIP 的基础上，在所有节点上各暴露一个端口，并把端口的流量也代理到指向的 Pod 上。可以通过这种方法从集群外访问集群内的资源。                                                                                                                                                |\n| `LoadBalancer` | 这种 Service 会在 ClusterIP 的基础上，在所有节点上各暴露一个端口，并在集群外创建一个负载均衡器来将外部流量路由到暴露的端口，再把流量代理到指向的 Pod 上。这种 Service 一般需要调用云服务提供的 API 或是额外安装的插件。如果什么插件都没安装的话，这种 Service 部署后会与 `NodePort` 的表现一样。 |\n| `ExternalName` | 这种 Service 不需要 selector 字段指定后端，而是用 externalName 字段指定一个外部 DNS 记录，然后将流量全部指向外部服务。如果打算将集群内的服务迁移到集群外、或是集群外迁移到集群内，这种类型的 Service 可以实现无缝迁移。                                                                          |\n\n### 虚拟 IP 与 Headless Service\n\n如果你在集群内尝试对 Service 对应的 DNS 记录进行域名解析，会发现返回来的 IP 地址与 Service 指向的任何一个 Pod 对应的 IP 地址都不相同。如果你还尝试了去 Ping 这个 IP 地址，会发现不能 Ping 通。为什么会这样呢？\n\n原来，每个 Service 被部署后， K8s 都会给他分配一个集群内部的 IP 地址，也就是 Cluster IP （这也是最基础的 Service 种类会起名叫 Cluster IP 的原因）。\n\n但是这个 Cluster IP 不会绑定任何的网卡，是一个虚拟 IP 。然后 K8s 中有一个叫 kube-proxy 的组件（这里叫他做组件，是因为 kube-proxy 与 Service 、 Deployment 等不一样，不是一种资源而是 K8s 的一部分）， kube-proxy 通过修改 iptables ，将虚拟 IP 的流量经过一定的负载均衡规则后代理到 Pod 上。\n\n![K8s 官网上的虚拟 IP 图](https://d33wubrfki0l68.cloudfront.net/27b2978647a8d7bdc2a96b213f0c0d3242ef9ce0/e8c9b/images/docs/services-iptables-overview.svg)\n\n\u003e **为什么不使用 DNS 轮询？**\n\u003e \n\u003e 为什么 K8s 不配置多条 DNS A 记录，然后通过轮询名称来解析？为什么需要搞出虚拟 IP 这么复杂的东西？这个问题 K8s 官网上也有特别提到原因：\n\u003e \n\u003e - DNS 实现的历史由来已久，它不遵守记录 TTL，并且在名称查找结果到期后对其进行缓存。\n\u003e - 有些应用程序仅执行一次 DNS 查找，并无限期地缓存结果。\n\u003e - 即使应用和库进行了适当的重新解析，DNS 记录上的 TTL 值低或为零也可能会给 DNS 带来高负载，从而使管理变得困难。\n\n有些时候（比如想使用自己的服务发现机制或是自己的负载均衡机制时）我们确实也会想越过虚拟 IP ，直接获取背后 Pod 的 IP 地址。这时候我们可以将 Service 的 `spec.clusterIP` 字段指定为 `None` ，这样 K8s 就不会给这个 Service 分配一个 Cluster IP 。这样的 Service 被称为 **Headless Service** 。\n\nHeadless Service 资源会创建一组 A 记录直接指向背后的 Pod ，可以通过 DNS 轮询等方式直接获得其中一个 Pod 的 IP 地址。另外更重要的一点， Headless Service 还会创建一组 SRV 记录，包含了指向各个 Pod 的 DNS 记录，可以通过 SRV 记录来发现所有 Pod 。\n\n我们可以在集群里用 nsloopup 或 dig 命令去验证一下：\n\n```sh\n# 在集群的 Pod 内部运行\n$ nslookup kafka-headless.kafka.svc.cluster.local\nServer:     10.96.0.10\nAddress:    10.96.0.10#53\n\nName:   kafka-headless.kafka.svc.cluster.local\nAddress: 172.17.0.6\nName:   kafka-headless.kafka.svc.cluster.local\nAddress: 172.17.0.5\nName:   kafka-headless.kafka.svc.cluster.local\nAddress: 172.17.0.4\n\n$ dig SRV kafka-headless.kafka.svc.cluster.local\n# .....\n;; ANSWER SECTION:\nkafka-headless.kafka.svc.cluster.local.      30      IN      SRV     0 20 9092 kafka-0.kafka-headless.kafka.svc.cluster.local.\nkafka-headless.kafka.svc.cluster.local.      30      IN      SRV     0 20 9092 kafka-1.kafka-headless.kafka.svc.cluster.local.\nkakfa-headless.kafka.svc.cluster.local.      30      IN      SRV     0 20 9092 kafka-2.kafka-headless.kafka.svc.cluster.local.\n\n;; ADDITIONAL SECTION:\nkafka-0.kafka-headless.kafka.svc.cluster.local. 30 IN A  172.17.0.6\nkafka-1.kafka-headless.kafka.svc.cluster.local. 30 IN A  172.17.0.5\nkafka-2.kafka-headless.kafka.svc.cluster.local. 30 IN A  172.17.0.4\n```\n\n\u003e 拥有 Cluster IP 的 Service 其实也有 SRV 记录。但这种情况的 SRV 记录中对应的 Target 仍为 Service 自己的 FQDN 。\n\n### 第三次回到 Stateful Set\n\n在上面 Headless Service 的例子中，我们看到，各个 Pod 对应的 DNS A 记录格式为 `\u003cpod_name\u003e.\u003csvc_name\u003e.\u003cnamespace\u003e.svc.cluster.local` 。不对啊，之前的小知识里不是说过 Pod 被分配的 DNS A 记录格式应该是 `172-17-0-3.default.pod.cluster.local` 的吗？\n\n其实 Headless Service 还有一个众所周知的隐藏功能。 Pod 这种资源本身的参数中有 `subdomain` 字段和 `hostname` 字段，如果设置了这两个字段，这个 Pod 就拥有了形如 `\u003chostname\u003e.\u003csubdomain\u003e.\u003cnamespace\u003e.svc.cluster.local` 的 FQDN （全限定域名）。如果这时刚好在同一名称空间下有与 `subdomain` 同名的 Headless Service ， DNS 就会用为这个 Pod 用它的 FQDN 来创建一条 DNS A 记录。\n\n比如 Pod1 在 `kafka` 名称空间中， `hostname` 为 `kafka-1` ， `subdomain` 为 `kafka-headless` ，那么 Pod1 的 FQDN 就是 `kafka-1.kafka-headless.kakfa.svc.cluster.local` 。而同样在 `kafka` 名称空间中，刚好又有一个 `kafka-headless` 的 Headless Service ，那么 DNS 就会创建一条 A 记录，就可以通过 `kafka-1.kafka-headless.kafka.svc.cluster.local` 来访问 Pod1 了。当然，由于 DNS 展开，也可以用 `kafka-1.kafka-headless.kafka` 甚至是 `kafka-1.kafka-headless` 来访问这个 Pod 。\n\n其实这些 Pod 是用 Stateful Set 来部署的，这一部分其实是 Stateful Set 相关的功能。之前我们说到 Stateful Set 有唯一稳定的网络标识。我们现在就来详细讲讲，这“唯一稳定的网络标识”到底是在指什么。\n\n我们来看一下这个 kafka Stateful Set 到底是怎么部署的：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-headless\nspec:\n  clusterIP: None # 这是一个 headless service\n  ports:\n  - name: tcp-client\n    port: 9092\n    protocol: TCP\n    targetPort: kafka-client\n  selector:\n    select-label: kafka-label\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\nspec:\n  replicas: 3\n  serviceName: kafka-headless # 注意到这里有 serviceName 字段\n  selector:\n    matchLabels:\n      select-label: kafka-label\n  template:\n    metadata:\n      labels:\n        select-label: kafka-label\n    spec:\n      containers:\n      - name: kafka\n        image: docker.io/bitnami/kafka:3.1.0-debian-10-r52\n        # 接下来 Pod 相关部分省略\n  # 下面 Volume 相关部分也省略\n```\n\n我们看到， Stateful Set 的定义中必须要用 `spec.serviceName` 字段指定一个 Headless Service 。 Stateful Set 创建 Pod 时，会自动给 Pod 指定 `hostname` 和 `subdomain` 字段。这样一来，每个 Pod 才有了唯一固定的 hostname ，唯一固定的 FQDN ，以及通过与 Headless Service 共同部署而获得唯一固定的 A 记录。（此外，其实当 Pod 因为版本升级等原因被重新创建时，相同序号的 Pod 还会被分配到相同固定的集群内 IP 。）\n\n\u003e **关于 Stateful Set 中 `serviceName` 字段的争议**\n\u003e \n\u003e Stateful Set 中的 serviceName 字段是必填字段。这个字段唯一的作用其实就是给 Pod 指定 subdomain 。其实这样会有一些问题：\n\u003e \n\u003e 1. Stateful Set 部署时不会检查是否真的存在这么一个 Headless Service 。如果 serviceName 乱填一个值，会导致虽然 Pod 的 `hostname` 和 `subdomain` 都指定了却没有创建 A 记录的情况。\n\u003e 2. 有时 Stateful Set 的 Pod 不需要接收流量，也不需要相互发现，这时候还强行需要指定一个 serviceName 显得有点多余。\n\u003e \n\u003e 在 GitHub 上有关于这个问题的 Issue ： https://github.com/kubernetes/kubernetes/issues/69608\n\n### 从集群外部访问\n\n在 K8s 集群里把应用部署好了，可是如何让集群外部的客户端访问我们集群中的应用呢？这可能是大家最关心的问题。\n\n不过有认真听的同学估计已经有这个问题的答案了。之前我们讲过 NodePort 和 LoadBalancer 这两种 Service 类型。\n\n其中 NodePort Service 只是简单地在节点机器上各开一个端口，而如何路由、如何负载均衡等则一概不管。\n\n而 LoadBalancer Service 则是在 NodePort 的基础上再加一个一个负载均衡器，然后把节点暴露的端口注册到这个负载均衡器上。这样一来，集群外部的客户端就可以通过同一个 IP 来访问集群中的应用。但是要使用 LoadBalancer Service ，一般需要先安装云供应商提供的 Controller ，或是安装其他第三方的 Controller （比如 Nginx Controller ）。\n\n在 Service 之外还另有一种资源类型叫 Ingress ，也可以用来实现集群外部访问集群内部应用的功能。 Ingress 其实也会在集群外创建一个负载均衡器，因此也需要预先安装云供应商的 Controller 。但 Ingress 与 Service 不同的是，它还会管理一定的路由逻辑，接收流量后可以根据路由来分配给不同的 Service 。\n\n| 类型                 | OSI 模型工作层数 | 依赖于云平台或其他插件 |\n| :------------------- | :--------------- | :--------------------- |\n| NodePort Service     | 第四层           | 否                     |\n| LoadBalancer Service | 第四层           | 是                     |\n| Ingress              | 第七层           | 是                     |\n\n特别再详细说一下 Ingress 这种资源。 Ingress 本身不会在集群内的 DNS 上创建记录，一般也不会主动去路由集群内的流量（除非你在集群内强行访问 Ingress 的负载均衡器…… 不过一般也没什么理由要这样做对吧）。但 Ingress 可以根据 HTTP 的 hostname 和 path 来路由流量，把流量分发到不同的 Service 上。 Ingress 也是 K8s 的原生资源里唯一能看到 OSI 第七层的资源。\n\n下面是 AWS 的 EKS 服务中部署的一个 Ingress 的例子（集群中已安装 AWS Load Balancer Controller ）：\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/backend-protocol-version: GRPC\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/healthcheck-path: /grpc.health.v1.Health/Check\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/success-codes: 0,12\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:xxxxxxxxxx:certificate/xxxxxxxxxx\n\n    external-dns.alpha.kubernetes.io/hostname: sample.example.com\n  \n  name: gateway-ingress\nspec:\n  rules:\n  - host: sample.example.com\n    http:\n      paths:\n      - path: /grpc.health.v1.Health\n        pathType: Prefix\n        backend:\n          service:\n            name: health-service\n            port:\n              number: 50051\n      - path: /proto.sample.v1.Sample\n        pathType: Prefix\n        backend:\n          service:\n            name: sample-service\n            port:\n              number: 50051\n```\n\n可以看到， Ingress 资源可以通过 `spec.rules` 字段中定义各条规则，通过 hostname 或是 path 等第七层的信息来进行路由。 Ingress 部署下去后， AWS Load Balancer Controller 会读取会根据的配置，并在云上创建一个 AWS Application Load Balancer （ALB），而 `spec.rules` 会应用到 ALB 上，由 ALB 来负责流量的路由。\n\n我们也会注意到，怎么 `metadata.annotations` 里有这么多奇奇怪怪的字段！ Ingress 本身的功能都是 AWS Load Balancer Controller 调用 AWS 的 API 创建 ALB 来实现的。但 AWS 的 ALB 能实现的功能可不止 Ingress 字段定义的这些，比如安装 TLS 证书、 health check 等 spec 字段中描述不下的功能，就只能是通过 annotation 的形式来定义了。\n\n\u003e 小彩蛋：可以看到例子中的 Ingress 资源 annotation 字段里还有一行 `external-dns.alpha.kubernetes.io/hostname: sample.example.com` 。其实这个 K8s 集群中还安装了 external-dns 这个应用，它可以根据 annotation 来在外部 DNS 上直接创建 DNS 记录！有了这个插件我们可不用再慢慢打开公共 DNS 管理页面，再小心翼翼地记下 IP 地址去添加 A 记录了。\n\n# 更高级的部署方式（一）\n\n一路说道这里， K8s 中最基础的资源大部分都已经介绍了。但是，这么多资源之间又需要相互配合，只部署一种资源基本没什么生产能力。\n\n比如只部署 Deployment 的话，我们确实是能在一组多副本的 Pod 里跑起可执行程序，但这组 Pod 却几乎没办法接受集群里其他 Pod 的流量（只能通过制定 Pod 的 IP 来访问，但 Pod 的 IP 是会变的）。因此一般来说一个 Deployment 都会搭配一个 Service 来使用。这还是最简单的一种搭配了。\n\n假若我们现在要在自己的 K8s 里安装一个别人提供的应用。当然由于 K8s 是基于容器的，只要别人提供了他应用的 yaml 清单，我们只用把清单用 `kubectl apply -f` 提交给 K8s ，然后让 K8s 把清单中的镜像拉下来就能跑了。可如果我们需要根据环境来改一些参数呢？\n\n如果别人提供的 yaml 文件比较简单还好说，改改对应的字段就好了。如果别人的应用比较复杂，那改 yaml 文件可就是一个大难题了。比如 AWS 的 Load Balancer Controller ，它的 yaml 清单文件可是多达 939 行！\n\n[[aws-elb-controller-lines.png]]\n\n在这种复杂的场景下，我们就需要一些更高级的部署方式了。\n\n### Helm\n\n首先来介绍的是 Helm 。 Helm 是一个包管理工具，可以类比一下 CentOS 中的 yum 工具。它可以把一组 K8s 资源发布成一个 Chart ，然后我们可以用 Helm 来安装这个 Chart ，并且可以通过参数设值来改变 Chart 中的部分资源。利用 Helm 安装 Chart 后还可以管理 Chart 的升级、回滚、卸载。\n\n使用别人提供的 Helm Chart 前，需要先 add 一下 Chart 的仓库，然后再安装仓库里提供的 Chart 。比如我们要安装 bitnami 提供的 Kafka Chart 时：\n\n```bash\n# 添加 https://charts.bitnami.com/bitnami 这个仓库，命名为 bitnami\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# 在 kafka 名称空间里安装 bitnami 仓库里的 kafka Chart ，并通过参数设置为 3 个副本，并同时安装一个 3 副本的 Zookeeper\nhelm install kafka -n kafka \\\n  --set replicaCount=3 \\\n  --set zookeeper.enabled=true \\\n  --set zookeeper.replicaCount=3 \\\n  bitnami/kafka\n```\n\n命令执行后， helm 就会根据参数与 Chart 的内容，在 K8s 里安装 StatefulSet 、 Service 、 ConfigMap 等一切所需要的资源。\n\n```sh\n$ k -n kafka get all,cm\nNAME                    READY   STATUS    RESTARTS      AGE\npod/kafka-0             1/1     Running   1             46d\npod/kafka-1             1/1     Running   3             46d\npod/kafka-2             1/1     Running   3             46d\npod/kafka-zookeeper-0   1/1     Running   0             46d\npod/kafka-zookeeper-1   1/1     Running   0             46d\npod/kafka-zookeeper-2   1/1     Running   0             46d\n\nNAME                               TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nservice/kafka                      ClusterIP      172.20.1.196     \u003cnone\u003e           9092/TCP                     164d\nservice/kafka-headless             ClusterIP      None             \u003cnone\u003e           9092/TCP,9093/TCP            164d\nservice/kafka-zookeeper            ClusterIP      172.20.227.236   \u003cnone\u003e           2181/TCP,2888/TCP,3888/TCP   164d\nservice/kafka-zookeeper-headless   ClusterIP      None             \u003cnone\u003e           2181/TCP,2888/TCP,3888/TCP   164d\n\nNAME                               READY   AGE\nstatefulset.apps/kafka             3/3     164d\nstatefulset.apps/kafka-zookeeper   3/3     164d\n\nNAME                                DATA   AGE\nconfigmap/kafka-scripts             2      164d\nconfigmap/kafka-zookeeper-scripts   2      164d\nconfigmap/kube-root-ca.crt          1      165d\n```\n\n甚至， Helm 可以通过模板生成的 Pod 环境变量，来预先设置好 Kafka 的配置，让他找得到 Zookeeper 服务：\n\n```yaml\napiVersion: v1\nkind: Pod\n# 略去无关信息\nspec:\n  containers:\n  - name: kafka\n    command:\n    - /scripts/setup.sh\n    env:\n    - name: KAFKA_CFG_ZOOKEEPER_CONNECT\n      value: kafka-zookeeper\n    # ...\n```\n\n通过设置 `KAFKA_CFG_ZOOKEEPER_CONNECT` 这个环境变量，指定了 Kafka Broker 可以通过访问 `kafka-zookeeper` 来找到 zookeeper 服务。（还记得 zookeeper 的 Service 名字是 `kafka-zookeeper` 吗？ zookeeper 与 kafka 部署在同一个名称空间里，因此可以直接通过 Service 名访问。）\n\n如果我们打开这个 helm chart 对应的[代码仓库](https://github.com/bitnami/charts/tree/master/bitnami/kafka)，会发现原来有一组 go template 文件，以及一个 `values.yaml` 文件和 `Chart.yaml` 文件：\n\n```sh\n.\n├── Chart.lock\n├── Chart.yaml\n├── README.md\n├── templates\n│   ├── NOTES.txt # 这里定义的是 helm 工具的命令行信息\n│   ├── _helpers.tpl # 这里面是一些定义好的 go template 代码块可以供其他模板使用\n│   ├── configmap.yaml\n│   ├── statefulset.yaml\n│   ├── svc-headless.yaml\n│   ├── svc.yaml\n│   └── # 以下省略若干模板文件\n└── values.yaml\n```\n\n- `Chart.yaml` 中定义了这个 Chart 的基本信息，包括名称、版本、描述、依赖等。\n- `values.yaml` 中定义了这个 Chart 的默认参数，包括各种资源的默认配置、副本数量、镜像版本等。其中的值都可以通过 `helm install` 命令的 `--set` 参数来覆盖。\n- `templates/` 文件夹下的都是 go template 的模板文件。\n\n`helm install` 就是通过用 `values.yaml` 中预定义的参数，渲染 `templates/` 文件夹下的 go template 文件，生成最终的 yaml 文件，然后再通过 kubectl apply -f 的方式，将 yaml 文件里的资源部署到 K8s 里。然后通过忘资源里注入一些特殊 annotation 的方式来记住自己部署了那些资源，进而提供 `update` 、 `uninstall` 等功能。\n\n关于更多 Helm 的内容，可以参考[官方文档](https://helm.sh/docs/)。\n\n### Kustomize\n\n另一个部署工具是 Kustomize 。之前提到 Config Map 时的例子中，将配置文件的内容直接写进了 yaml 清单的一个字段里：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-demo\ndata:\n  # 一个 Key 可以对应一个值\n  player_initial_lives: \"3\"\n  ui_properties_file_name: \"user-interface.properties\"\n\n  # 一个 Key 也可以对应一个文件的内容\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5    \n  user-interface.properties: |\n    color.good=purple\n    color.bad=yellow\n    allow.textmode=true    \n```\n\n其实这样很不好，先不说这样写没办法在 IDE 里用配置文件自己的语法检查，每行还需要一定的缩进，如果配置文件有好几百行，你甚至会忘了这一行到底是哪个配置文件！此时我们就会自然而然的想把每个配置文件以单独文件的形式保存。\n\nKustomize 就是这样一个工具，它可以帮助我们把每个配置文件以单独文件的形式保存，然后再通过一个 `kustomization.yaml` 文件，将这些配置文件组合起来，生成最终的 yaml 文件。\n\n```yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  # 其他资源也可以单独使用一个文件定义\n  - deployment.yaml\n\n# 用 configMapGenerator 从文件中生成 ConfigMap\nconfigMapGenerator:\n  - name: game-demo\n    literals:\n      - \"ui_properties_file_name=user-interface.properties\"\n      - \"player_initial_lives=3\"\n    # 从文件中读取内容\n    files:\n      - game.properties\n      - user-interface.properties\n# 有多个 configMap 时，可以通过统一的 generatorOptions 来设置一些通用的选项\ngeneratorOptions:\n  disableNameSuffixHash: true\n```\n\n然后两个配置文件的内容可以单独用文件定义，此时可以结合 IDE 的语法检查，以及代码补全功能，来编写配置文件。\n\n```properties\n# user-interface.properties\ncolor.good=purple\ncolor.bad=yellow\nallow.textmode=true    \n```\n\n然后将 `kustomization.yaml` 和其他所需的文件都放在同一个目录下：\n\n```bash\n.\n├── kustomization.yaml\n├── deployment.yaml\n├── game.properties\n└── user-interface.properties\n```\n\n然后就可以通过 `kubectl apply -k ./` 来将整个 kustomize 文件夹转换为 yaml 清单直接部署到 K8s 中。\n（没错，现在 Kustomize 已经成为 kubectl 中的内置功能！可以不用先 `kustomize build` 生成 yaml 文件再 `kubectl apply` 两步走了！）\n\n值得提醒的是，虽然 `kustomization.yaml` 有 `apiVersion` 和 `kind` 字段，长得很像一个资源清单，但其实 K8s 的 API server 并不认识他。 Kustomize 的工作原理其实是先根据 `kustomization.yaml` 生成 K8s 认识的 yaml 资源清单，然后再通过 `kubectl apply` 来部署。\n\n除了可以直接将 ConfigMap 与 Secret 中的文件字段内容用单独的文件定义外， Kustomize 还有其他比如为部署的资源添加统一的名称前缀、添加统一字段等功能。这些大家可以阅读 Kustomize 的[官方文档](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/)来了解。\n\n### 各种工具的优缺点\n\n我们目前已经知道有三种在 K8s 中部署资源的方式： `kubectl apply`、Helm 和 Kustomize 。\n\n其中 `kubectl apply` 的优缺点很明确，优点是最简单直接，缺点是会导致要么 yaml 清单过长，要么需要分多文件多次部署，使集群中产生中间状态。\n\n而 Helm 与 Kustomize 我们上面也分析过，其实都是基于 `kubectl apply` 的。 Helm 是通过 go template 先生成 yaml 文件再 `kubectl apply` ，而 Kustomize 是通过 `kustomization.yaml` 中的定义用自己的一套逻辑生成 yaml 文件，然后再 `kubectl apply` 。\n\nHelm 的优点是 Helm Chart 安装时可以直接使用别人 Helm 仓库中已经上传好的 Chart ，只需要设置参数就可以使用。这也是 Kustomize 的缺点：如果想要使用别人提供的 Kustomization 而只修改其中的一些配置，必须要先把放 `kustomization.yaml` 的整个文件夹下载下来才能做修改。\n\n而 Helm 的缺点也是明显的， Helm 依赖于往资源里注入特殊的 annotation 来管理 Chart 生成的资源，这可能会很难与集群中现有的一些系统（比如 Service Mesh 或是 GitOps 系统等）放一起管理。而 Kustomize 生成的 yaml 清单就是很干净的 K8s 资源，原先的 K8s 资源该是什么表现就是什么表现，与现有的系统兼容一般会比较好。\n\n而另外，由于 Helm 与 Kustomize 都是基于 `kubectl apply` 的，因此他们有共同的缺点，就是不能做 `kubectl apply` 不能做的事情。\n\n什么叫 `kubectl apply` 不能做的事情呢？比如说我们要在 K8s 中部署 Redis 集群。聪明的你可能就想到要用 Stateful Set 、 PVC 、 Headless Service 来一套组合拳。这确实可以部署一个多节点、有状态的 Redis Cluster 。可是如果我们要往 Redis Cluster 里加一个节点呢？\n\n你当然可以把 Stateful Set 中的 `Replicas` 字段加个 1 然后用 `kubectl apply` 部署，可是这实际上只能增加一个一个 Redis 实例 —— 然后什么都没发生。其他节点不认识这个新的节点，访问这个新节点也不能拿到正确的数据。要知道往 Redis Cluster 里加节点，是要先让集群发现这个新节点，然后还要迁移 slot 的！ `kubectl apply` 可不会做这些事。\n\n\u003e Well, 其实这些也是可以通过增加 initContainer 、修改镜像增加启动脚本等方式，实现用 `kubectl apply` 部署的。可是，这会让整个 Pod 资源变得很难理解，也不好维护。而且，如果不是因为做不到，谁会想去修改别人的镜像呢？\n\n我们接下来会介绍 K8s 的核心架构，来理解我们之前讲的这些资源到底是怎么工作的。最后会引出一组新的概念： Operator 与自定义资源（ Custom Resource Definition ，简称 CRD ）。通过 Operator 与 CRD ，我们可以做到 `kubectl apply` 所不能做到的事，包括 Redis Cluster 的扩容。\n\n\u003e DIO: `kubectl apply` 的能力是有限的……\n\u003e 越是部署复杂的应用，就越会发现 `kubectl apply` 的能力是有极限的……除非超越 `kubectl apply` 。\n\u003e \n\u003e JOJO: 你到底想说什么？\n\u003e \n\u003e DIO: 我不用 `kubectl apply` 了！ JOJO ！\n\u003e （其实还是要用的）\n\n","title":"Kubernetes 入门 （2）","abstract":"我们之前说的都是用于部署 Pod 的资源，我们接下来介绍与创建 Pod 不相关的资源：储存与网络。\n其实我们之前已经接触过储存相关的内容了：在讲 Stateful Set 时我们提过 Stateful Set 创建出来的 Pod 都会有相互独立的储存；而讲 Daemon Set 时我们提到 K8s 推荐只在 Daemon Set 的 Pod 中访问宿主机磁盘。但独立的储存具体指什么？除了访问宿主机磁盘以外还有什么其他的储存？\n在 Docker 中，我们可以把宿主机磁盘上的一个路径作为一个 Volume 来给容器绑定，或者直接使用 Docker Engine 管理的 Volume 来提供持久化存储或是容器间共享文件。在 K8s 里面也沿用了 Volume 这个概念，可以通过 Mount 绑定到容器内的路径，并通过实现 CSI 的各种引擎来提供更多样的存储。","length":875,"created_at":"2022-08-20T21:56:52.000Z","updated_at":"2022-08-20T14:02:18.000Z","tags":["Kubernetes","DevOps","Docker","Cloud Native"],"license":true}},{"pathMapping":{"filePath":"public/content/articles/2022-08-13-introduction-for-k8s.md","pagePath":"/articles/introduction-for-k8s","slug":"introduction-for-k8s"},"meta":{"content":"\n# 容器， Docker 与 K8s\n\n我们知道 K8s 利用了容器虚拟化技术。而说到容器虚拟化就要说 Docker 。可是，容器到底是什么？ Docker 又为我们做了些什么？我们又为什么要用 K8s ？\n\n### 关于容器虚拟化\n\n\u003e 要把一个不知道打过多少个升级补丁，不知道经历了多少任管理员的系统迁移到其他机器上，毫无疑问会是一场灾难。 —— Chad Fowler 《Trash Your Servers and Burn Your Code》\n\n\"Write once, run anywhere\" 是 Java 曾经的口号。 Java 企图通过 JVM 虚拟机来实现一个可执行程序在多平台间的移植性。但我们现在知道， Java 语言并没能实现他的目标，会在操作系统调用、第三方依赖丢失、两个程序间依赖的冲突等各方面出现问题。\n\n要保证程序拉下来就能跑，最好的方法就是把程序和依赖打包到一起，然后将外部环境隔离起来。容器虚拟化技术就是为了解决这个。\n\n与常说的虚拟机不同， Docker 等各类容器是用隔离名称空间的方式进行资源隔离的。 Linux 系统的内核直接提供了名称空间隔离的能力，是针对进程设计的访问隔离机制，可以进行一些资源封装。\n\n| 名称空间     | 隔离内容                      | 内核版本 |\n| :----------- | :---------------------------- | :------- |\n| Mount        | 文件系统与路径等              | 2.4.19   |\n| UTS          | 主机的Hostname、Domain names  | 2.6.19   |\n| IPC          | 进程间通信管道                | 2.6.19   |\n| PID          | 独立的进程编号空间            | 2.6.24   |\n| Network      | 网卡、IP 地址、端口等网络资源 | 2.6.29   |\n| User         | 进程独立的用户和用户组        | 3.8      |\n| Cgroup       | CPU 时间片，内存分页等        | 4.6      |\n| Time \\\u003c- New! | 进程独立的系统时间            | 5.6      |\n\n值得注目的是， Linux 系统提供了 Cgroup 名称空间隔离的支持。通过隔离 Cgroup ，可以给单独一个进程分配 CPU 占用比率、内存大小、外设 I/O 访问权限等。再配合 IPC 、 PID 等的隔离，可以让被隔离的进程看不到同一实体机中其他进程的信息，就像是独享一整台机器一样。\n\n由于容器虚拟化技术直接利用了宿主机操作系统内核，因此远远要比虚拟机更轻量，也更适合用来给单个程序进行隔离。但也同样由于依赖了宿主机内核，在不同的架构、不同种类的操作系统间容器可能不能移植。\n\n### 关于 Docker\n\n在介绍 K8s 之前，我们要先搞清楚 Docker 是什么。或者说，我们平时说的“ Docker ”是什么？\n\n我们平时说的 Docker ，可能是以下几个东西：\n\n- Docker Engine: 在宿主机上跑的一个进程，专门用来管理各个容器的生命周期、网络连接等，还暴露出一些 API 供外部调用。有时会被称为 Docker Daemon 或是 dockerd 。\n- Docker Client: 命令行中的 `docker` 命令，其实只会跟 Docker Server 通信，不会直接创建销毁一个容器进程。\n- Docker Container: 宿主机上运行的一组被资源隔离的进程，在容器中看来像是独占了一台虚拟的机器，不需要考虑外部依赖。\n- Docker Image: 是一个打包好的文件系统，可以从一个 Image 运行出复数个 Container 。 Image 内部包含了程序运行所需的所有文件、库依赖，以及运行时的环境变量等。\n- Docker 容器运行时: 是 Docker Engine 中专门管理容器状态、生命周期等的那个组件，原来名为 libcontainer 。[《开放容器交互标准》](https://en.wikipedia.org/wiki/Open_Container_Initiative)制定后， Docker 公司将此部分重构为 [runC 项目](https://github.com/opencontainers/runc)，交给 Linux 基金会管理。而 Docker Engine 中与运行时进行交互的部分则抽象出来成为 [containerd 项目](https://containerd.io/)，捐献给了 CNCF 。\n\n我们平时在 linux 机上运行 `yum install docker` 之类的命令，安装的其实是 Docker Engine + Docker Client 。（而在 Windows 或 MacOS 上安装的 Docker Desktop 其实是一个定制过的 linux 虚拟机。）下面说的 Docker 的功能其实都是指 Docker Engine 的功能。\n\n而 Docker 提供给我们的功能，除了最基础的运行和销毁容器外，还包括了一些容器网络编排、重启策略、文件路径映射、端口映射等功能。\n\n而我认为 Docker 最大的贡献，还是容器的镜像与镜像仓库。有了镜像与镜像仓库，人们就可以把自己的程序与执行环境直接打包成镜像发布，也可以直接拿打包好的镜像来运行容器进行部署，而不需要额外下载或是安装一些东西，也不需要担心程序会与已经跑起来的其他程序冲突。\n\n### 为什么要用 K8s ？\n\n其实 Docker 有一个很强大的工具叫 docker-compose ，可以通过一个 manifest 对多个容器组成的网络进行编排。那为什么我们还需要 K8s 呢？换句话说，有什么事是 Docker 不能做的？而 K8s 设计出来的目标是为了解决什么问题？\n\n首先， Docker 做不到以下的功能：\n\n1. **Docker 不能做跨多主机的容器编排。** docker-compose 再方便，他也只能编排单台主机上的容器。对跨主机的集群编排无能为力。（实际上，用了 Docker-Swarm 后是可以多主机编排的，但一来 Docker-Swarm 出现的比 K8s 晚，而来 Docker-Swarm 功能不如 K8s ，因此用的人很少，我们下面就默认 Docker-Swarm 不存在了。）\n2. **Docker 提供的容器部署管理功能不够丰富。** Docker 有一些简单的容器重启策略，但也只是简单的失败后重启之类的，没有完整的应用状态检查等功能。同时，版本升级、缩扩容等策略选择的余地也不多。\n3. **Docker 缺乏高级网络功能。** 要让 Docker 的容器间进行网络通信，也只能是说把容器放到同一个网络下，然后再通过各自的 Hostname 来找到对方。但实际上，我们更会想要一些负载均衡、自定义域名、选择某些容器端口不暴露之类的功能。\n\nand more...\n\n总的来说， Docker 更关注单台主机上容器怎么跑，而对部署管理的功能则支持不多。而最大的痛点，就是 Docker 对多主机的集群部署支持的实再很差。然而，为了实现多区可用、负载均衡等功能，多主机集群的容器编排又是必不可少的。\n\nK8s 的出现，主要就是为了解决多主机集群上的容器编排问题。\n\n1. **K8s 可以进行多主机调度。** 用户只需要描述自己需要运行怎样的应用， K8s 就可以自己选择一个合适的节点进行部署，用户不需要关心自己的应用部署到哪个节点上。\n2. **K8s 中一切皆资源。** K8s 有完善的抽象资源机制，用户几乎不需要知道磁盘、网络等任何硬件信息，只需要对着统一的抽象资源进行操作。\n3. **K8s 能保证较强的可用性。** 除了能跨多主机调度实现多区可用外， K8s 还提供了很完善的缩扩容机制、健康检查机制以及自动恢复机制。\n\n可以说， K8s 是容器编排工具的主流选择。\n\n### K8s 与 Docker 的关系\n\nK8s 与 Docker 关系很复杂，是一个逐渐变化的过程。\n\n一开始 K8s 是完全依赖于 Docker Engine 进行容器启动与销毁的。后来[容器运行时接口（CRI）](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)、 [CRI-O 标准](https://github.com/cri-o/cri-o)、开放容器交互标准（OCI）等标准逐渐建立，可替代 Docker Engine 的工具越来越多， K8s 中已经完全可以不使用 Docker Engine 了。\n\n[《凤凰架构》](http://icyfenix.cn/)一书中有下面这样一张图来描述 K8s 与 Docker Engine 的关系：\n\n![K8s 与 Docker Engine 的关系](http://icyfenix.cn/assets/img/kubernetes.495f9eae.png)\n\n《凤凰架构》书中[这一章节](http://icyfenix.cn/immutable-infrastructure/container/history.html#%E5%B0%81%E8%A3%85%E9%9B%86%E7%BE%A4%EF%BC%9Akubernetes)详细介绍了 K8s 与 Docker 的历史，我这里就不再赘述。\n\n# 部署一个 Pod\n\n上面说了一堆概念，我们接下来实际上会怎样应用 K8s 。\n\n### Pod 示例\n\n\u003e Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。\n\u003e Pod 是一组容器；Pod 中的内容总是一同调度，在共享的上下文中运行。 Pod 中包含一个或多个应用容器，这些容器相对紧密地耦合在一起。在非云环境中，在相同的物理机或虚拟机上运行的应用类似于在同一逻辑主机上运行的云应用。\n\u003e —— Kubernetes 官方文档\n\nPod 是 K8s 的最小部署单位。\n\n因为 K8s 将硬件资源都抽象化了，用户不需要知道自己的应用部署到哪台机上。但是有些场景下两个主进程之间又必须相互协作才能完成任务，如果两个进程不确定会不会部署到同一个节点上会变得很麻烦。因此才需要 Pod 这种资源。\n\n下面是一个 Nginx Pod 的示例（这是 K8s manifest 文件，可以用 `kubectl apply -f \u003cfilepath\u003e` 进行部署）：\n\n```yaml\nmetadata:\n  name: simple-webapp\nspec:\n  containers:\n    - name: main-application\n      image: nginx\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n    - name: sidecar-container\n      image: busybox\n      command: [\"sh\",\"-c\",\"while true; do cat /var/log/nginx/access.log; sleep 30; done\"]\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n  volumes:\n    - name: shared-logs\n      emptyDir: {}\n```\n\n可以看到， Pod 中可以包含多个容器，这组容器总是以一定的逻辑一起部署，且总是部署在同一个节点。对 K8s 操作时，不能说只部署 Pod 中一个特定的容器，也不能说把 Pod 中一个容器部署在这个节点，另一个容器部署在另一个节点上。\n\n在上面这个例子中，我们看到 Pod 中除了 Nginx 容器以外还有一个 Sidecar 容器负责将 Nginx 的 access.log 日志输出到控制台。两个容器可以通过 mount 同一个路径来实现文件共享。这种场景下，单独跑一个 Sidecar 容器没有意义，而我们也不会希望两个容器部署在不同的节点上。 **两个容器同生共死** ，这样的模式被称为 **Sidecar 模式** 。 Jaeger Agent ，或是 Service Mesh 中常见的 Envoy Sidecar 都可以通过这种模式部署，这样业务容器中就可以不考虑 tracing 或是流量控制相关的问题。\n\n此外，由于同一个 Pod 中的容器默认共享了相同的 network 和 UTS 名称空间，不管是在 Pod 的内部还是外部来看，他们一定程度上就像是真的部署在同一主机上一样，有相同的 Hostname 与 ip 地址，在一个容器中也可以通过 localhost 来访问零一个容器的端口。\n\n另外 Pod 中可以定义若干个 initContainer ，这些容器会比 `spec.containers` 中的容器先运行，并且是顺序运行。下面是通过安装 bitnami 的 Kafka Helm Chart 得到的一个 Kafka Broker Pod （有所简化）:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-0\n  namespace: kafka\nspec:\n  containers:\n  - name: kafka\n    image: docker.io/bitnami/kafka:3.1.0-debian-10-r52\n    command:\n    - /scripts/setup.sh\n    volumeMounts:\n    - name: scripts\n      mountPath: /scripts/setup.sh\n      subPath: setup.sh\n    - name: shared\n      mountPath: /shared\n  initContainers:\n  - name: auto-discovery\n    image: docker.io/bitnami/kubectl:1.23.5-debian-10-r1\n    command:\n    - /scripts/auto-discovery.sh\n    volumeMounts:\n    - name: shared\n      mountPath: /shared\n    - name: scripts\n      mountPath: /scripts/auto-discovery.sh\n      subPath: auto-discovery.sh\n  volumes:\n  - name: scripts\n    configMap:\n      defaultMode: 493\n      name: kafka-scripts\n  - name: shared\n    emptyDir: {}\n```\n\n可以看到，在 `kafka` pod 启动前会先启动一个名为 `auto-discovery` 的 initContainer ，负责获得集群信息等准备工作。准备工作完成后，会将信息写入 `/shared` 目录下，然后再启动 `kafka` 容器 Mount 同一目录，就可以获取准备好的信息。\n\n**这样运行容器进行 Pod 初始化就叫 initContainer 模式** 。每个 initContainer 会运行到成功退出为止，如果有一个 initContainer 启动失败，则整个 Pod 启动失败，触发 K8s 的 Pod 重启策略。\n\n\n# 部署更多 Pod\n\n### Replica Set\n\n可是上面说了这么多，还只是单个 Pod 的部署，但我们希望能做多副本部署。\n\n其实，只要把 Pod 的 manifest 改一下 `metadata.name` 再部署一次，就能得到一模一样的两个 Pod ，就是一个简单的多副本部署了。（必须改 `metadata.name` ，不然 K8s 会以为你是想修改同一个 Pod ）\n\n可是这样做会有很多问题：\n\n- 要复制一下还要改名字多麻烦啊，我想用同一份模板，只定义一下副本数就能得到对应数量的 Pod 。\n- 缩容扩容还要对着 Pod 操作很危险，我想直接修改副本数就能缩容扩容。\n- 如果其中一些 Pod 挂掉了不能重启，现在是什么都不会做。我希望能自动建一些新的 Pod 顶上，来保证副本数不变。\n\n为了实现这些需求，就出现了 Replica Set 这种资源。下面是实际应用中一个 Replica Set 的例子：\n\n```yaml\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: gateway\n  name: gateway-9dc546658\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n      name: gateway\n    spec:\n      containers:\n        name: gateway\n        image: xxxxxxxx.amazonaws.com/gateway:xxxxxxx\n        ports:\n        - containerPort: 50051\n          protocol: TCP\n        readinessProbe:\n          initialDelaySeconds: 5\n          tcpSocket:\n            port: 50051\n        startupProbe:\n          failureThreshold: 60\n          tcpSocket:\n            port: 50051\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: gateway\n              topologyKey: topology.kubernetes.io/zone\n            weight: 80\n```\n\n我们可以看到， `spec.template` 中就是我们要的 Pod 的模板，在 metadata 里带上了 `app:gateway` 标签。而在 `spec.replicas` 中定义了我们需要的 Pod 数量， `spec.selector` 中描述了我们要对带 `app:gateway` 标签的 Pod 进行控制。把这份 manifest 部署后，我们就会得到除名字以外几乎一摸一样的两个 Pod ：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  generateName: gateway-9dc546658-\n  labels:\n    app: gateway\n    pod-template-hash: 9dc546658\n  name: gateway-9dc546658-6c9qs\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: true\n    controller: true\n    kind: ReplicaSet\n    name: gateway-9dc546658\n    uid: 6633f89c-377c-4c90-bd08-3be5bc7b21bd\n  resourceVersion: \"49793842\"\n  uid: f927db88-a39a-4623-852d-4f150a6d853b\nspec:\n  containers:\n    name: gateway\n    image: xxxxxxxx.amazonaws.com/gateway:xxxxxxx\n    ports:\n    # 后续省略\n\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubernetes.io/psp: eks.privileged\n  creationTimestamp: \"2022-08-09T08:51:25Z\"\n  generateName: gateway-9dc546658-\n  labels:\n    app: gateway\n    pod-template-hash: 9dc546658\n  name: gateway-9dc546658-8trcs\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: true\n    controller: true\n    kind: ReplicaSet\n    name: gateway-9dc546658\n    uid: 6633f89c-377c-4c90-bd08-3be5bc7b21bd\n  resourceVersion: \"49793745\"\n  uid: 0918e3ed-2965-4237-8828-421a7831c9ed\nspec:\n  containers:\n    image: xxxxxxxx.amazonaws.com/gateway:xxxxxxx\n    name: gateway\n    ports:\n    # 后续省略\n```\n\n可以看到，创建出来的 Pod 自动生成了两个后缀（ `6c9qs` 与 `8trcs` ），带上了 Replica Set 的信息（在 `metadata.ownerReferences` ），其他部分基本一模一样。如果其中一个 Pod 挂掉了， K8s 会帮我们从模板中重新创建一个 Pod 。而且由于我们在 Pod 模板定义了 affinity ， K8s 还会按照我们的要求自动筛选合适的节点。例如在上面 Replica Set 的例子中，创建出来的 Pod 就会尽量部署在不同的节点上。\n\n\u003e **K8s 中对 Pod 的生存状态检查机制**\n\u003e \n\u003e 除了线程直接错误退出以外，还有出现死锁等等各种可能性使得容器中的应用不能正常工作。这些情况下虽然是不健康状态，但容器却不一定会挂掉。因此 K8s 提供了一些探针检查的机制来判断 Pod 是否健康。\n\u003e K8s 主要提供了三种探针：\n\u003e 1. **存活探针（ liveness probe ）** : Pod 运行时 K8s 会循环执行 liveness probe 检查容器是否健康。如果检查失败， K8s 会认为这个容器不健康，就会尝试重启容器。\n\u003e 2. **就绪探针（ readiness probe ）** : 程序可能会有一段时间不能提供服务（比如正在加载数据等）。这时可能既不想杀死应用，也不想给它发送请求，这时就需要 readiness probe 。如果 readiness probe 检查失败， K8s 就会将这个 Pod 从 Service 上摘下来，直到 readiness probe 成功重新加入 Service 。\n\u003e 3. **启动探针（ startup probe ）** : 有些程序会有非常长的启动时间，会有较长时间不能提供服务。这时如果 liveness probe 失败了导致重启毫无必要，此时就需要 startup probe 。 startup probe 只会在容器启动时检查直到第一次成功。直到 startup probe 成功为止， liveness probe 与 readiness probe 都不会开始执行检查。\n\u003e \n\u003e 而检测方式主要有：\n\u003e 1. httpGet: 对指定的端口路径执行 HTTP GET 请求，如果返回 2xx 或 3xx 就是成功。\n\u003e 2. tcpSocket: 尝试与容器的端口建立连接，如果不能成功建立连接就是失败。\n\u003e 3. exec: 在容器内执行一段命令，如果退出时状态码不为 0 就是失败。\n\u003e 4. grpc (New!): K8s 1.24 新出的检查方式，直接用 [GRPC Health Checking Protocol](https://github.com/grpc/grpc/blob/master/doc/health-checking.md) 对 GRPC Server 进行检查。\n\n此外， Replica Set 还提供了简易的缩容扩容功能。 kubectl 中提供了 scale 命令：\n\n```bash\nkubectl scale replicaset gateway --replicas=10\n```\n\n执行上述命令，就可以将名为 gateway 的 Replica Set 对应的副本数扩容到 10 份。当然，你也可以直接修改 Replica Set 的 `spec.replicas` 字段来实现缩容扩容。\n\n然而， Replica Set 的功能还是有限的。实际上， Replica Set 只关心跟它的 selector 匹配的 Pod 的数量。而至于匹配的 Pod 是否真的是跟 template 字段中描述的一样， Replica Set 就不关心了。因此如果单用 Replica Set ，更新 Pod 就会变得究极麻烦。\n\n### Deployment\n\n为了解决 Pod 的更新问题，我们需要有 Deployment 这种资源。实际上， Replica Set 的主要用途是提供给 Deployment 作为控制 Pod 数量，以及创建、删除 Pod 的一种机制。我们一般不会直接使用 Replica Set 。\n\n下面是实际应用中一个 Deployment 的例子：\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: gateway\n  name: gateway\nspec:\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n      name: gateway\n    spec:\n      containers:\n        name: gateway\n        image: xxxxxxxx.amazonaws.com/gateway:xxxxxxx\n        ports:\n        # 下略\n```\n\n可以看到 Deployment 的 manifest 跟 Replica Set 很像。但实际上， Deployment 不会直接创建 Pod ，而是创建出一个 Replica Set ，再由 Replica Set 来创建 Pod ：\n\n\n```mermaid\nflowchart TB\n\nDeployment1[Deployment]\nReplicaSet11[Replica Set]\nPod11[Pod1]\nPod12[Pod2]\nDeployment1 --\u003e ReplicaSet11\nReplicaSet11 --\u003e Pod11\nReplicaSet11 --\u003e Pod12\n```\n\n比如在上面的例子中，名为 gateway 的 Deployment 创建后，就会有如下 ReplicaSet 和 Pod ：\n\n```sh\n# Replica Set:\n$ kubectl get rc -l app=gateway\nNAME                 DESIRED   CURRENT   READY   AGE\ngateway-9dc546658    2         2         2       5d3h\n\n# Pod:\n$ kubectl get po -l app=gateway\nNAME                      READY   STATUS    RESTARTS   AGE\ngateway-9dc546658-6c9qs   1/1     Running   0          5d3h\ngateway-9dc546658-8trcs   1/1     Running   0          5d3h\n```\n\n可以看到，gateway Deployment 创建了一个 Replica Set ，然后随机给了它一个 `9dc546658` 后缀。然后 gateway-9dc546658 这个 Replica Set 又根据 template 中创建了两个 Pod ，再在自己名字的基础上加上两个后缀 `6c9qs` 与 `8trcs` 。\n\n接下来就是 Deployment 的重点了： Replica Set 只会根据 template 创建出 Pod ，而不管匹配的 Pod 到底是不是跟 template 中描述的一样。而 **Deployment 则会专门关注 template 的内容变更。**\n\n假如我们现在更新了 Deployment 的 template 中的内容提交给 K8s ， Deployment 就会感知到 template 被修改了， Pod 需要更新。\n感知到更新之后， Deployment 就会创建一个新的 Replica Set 。然后逐渐将旧的 Replica Set 缩容到 0 ，并同时将新的 Replica Set 扩容到目标值。最后，所有旧版本的 Pod 将会被更新成新版本的 Pod 。如下图所示：\n\n```mermaid\nflowchart TB\n\nsubgraph A\ndirection TB\nDeployment1[Deployment]\nReplicaSet11[Replica Set]\nReplicaSet12[New Replica Set]\nPod11[Pod1]\nPod12[Pod2]\nDeployment1 --\u003e ReplicaSet11\nDeployment1 --\u003e ReplicaSet12\nReplicaSet11 --\u003e Pod11\nReplicaSet11 --\u003e Pod12\nend\n\nsubgraph B\ndirection TB\nDeployment2[Deployment]\nReplicaSet21[Replica Set]\nReplicaSet22[New Replica Set]\nPod21[New Pod1]\nPod22[Pod2]\nDeployment2 --\u003e ReplicaSet21\nDeployment2 --\u003e ReplicaSet22\nReplicaSet21 --\u003e Pod22\nReplicaSet22 --\u003e Pod21\nend\n\nsubgraph C\ndirection TB\nDeployment3[Deployment]\nReplicaSet31[Replica Set]\nReplicaSet32[New Replica Set]\nPod31[New Pod1]\nPod32[New Pod2]\nDeployment3 --\u003e ReplicaSet31\nDeployment3 --\u003e ReplicaSet32\nReplicaSet32 --\u003e Pod31\nReplicaSet32 --\u003e Pod32\nend\n\nA --\u003e B --\u003e C\n```\n\n整个过程完成后， Deployment 还不会将旧的 Replica Set 删除掉。我们注意到 Deployment 的声明中有这么一个字段： `revisionHistoryLimit: 10` ，表示 Deployment 会保留历史中 最近的 10 个 Replica Set ，这样在必要的时候可以立刻将 Deployment 回滚到上个版本。而超出 10 个的 Replica Set 才会被从 K8s 中删除。\n\n```sh\n# 实际中被 scale 到 0 但还没被删除的 Replica Set\n$ kubectl get rs -l app=gateway\nNAME                 DESIRED   CURRENT   READY   AGE\ngateway-5c4cdf957d   0         0         0       5d4h\ngateway-5c56f6d487   0         0         0       17d\ngateway-65857cfc78   0         0         0       10d\ngateway-6bddbdd85f   0         0         0       16d\ngateway-6cc9bb5b4c   0         0         0       13d\ngateway-6f4664bc65   0         0         0       17d\ngateway-7bd667cb79   0         0         0       9d\ngateway-7d658d57f5   0         0         0       13d\ngateway-84df97d4c8   0         0         0       6d4h\ngateway-9998f4689    0         0         0       13d\ngateway-9dc546658    2         2         2       5d4h\n```\n\n### Stateful Set\n\nDeployment 中默认了我们不关心自己访问的是哪个 Pod ，因为各个 Pod 的功能是一样的，访问哪个没有差别。\n\n实际上这也符合大多数情况：试想一个 HTTP Server ，如果其所有数据都存放到同一个的数据库中，那这个 HTTP Server 不管部署在哪台主机、不管有多少个实例、不管你访问的是哪个实例，都察觉不出有什么差别。而有了这种默认，我们就能更放心地对 Pod 进行负载均衡、缩扩容等操作。\n\n但实际上我们总会遇到需要保存自己状态的 Pod 。比如我们在 K8s 里部署一个 Kafka 集群，每个 Kafka broker 都需要保存自己的分区数据，而且还要往 Zookeeper 里写入自己的名字来实现选举等功能。如果简单地用 Deployment 来部署， broker 之间可能就会分不清到底哪块是自己的分区，而且由 Deployment 生成出来的 Pod 名字是随机的，升级后 Pod 的名字会变，导致 Kafka 升级后名字与 Zookeeper 里的名字不一致，被以为是一个新的 broker 。\n\nStateful Set 就是为了解决有状态应用的部署而出现的。下面是 用 bitnami 的 Kafka Helm Chart 部署的一个 Kafka Stateful Set 的例子：\n\n```yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: kafka\n  name: kafka\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kafka\n  serviceName: kafka-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kafka\n    spec:\n      containers:\n      - name: kafka\n        image: docker.io/bitnami/kafka:3.1.0-debian-10-r52\n        command:\n        - /scripts/setup.sh\n        ports:\n        - containerPort: 9092\n          name: kafka-client\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /bitnami/kafka\n          name: data\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      resources:\n        requests:\n          storage: 10Gi\n      storageClassName: gp2\n```\n\n可以看到其实 Stateful Set 类似 Deployment ，也可以通过 replicas 字段定义实例数，如果更新 template 部分， Stateful Set 也会以一定的策略对 Pod 进行更新。\n\n而其创建出来的 Pod 如下所示：\n```sh\n$ kubectl get po -l app.kubernetes.io/name=kafka\nNAME      READY   STATUS    RESTARTS   AGE\nkafka-0   1/1     Running   1          26d\nkafka-1   1/1     Running   3          26d\nkafka-2   1/1     Running   3          26d\n```\n\n与 Replica Set 创建出来的 Pod 相比名字上会有很大差别。 Stateful Set 创建出来的 Pod 会固定的以 `-0` 、 `-1` 、 `-2` 结尾而不是随机生成：\n\n```mermaid\nflowchart TB\nrs[Replica Set A]\nrs --\u003e A-qwert\nrs --\u003e A-asdfg\nrs --\u003e A-zxcvb\n\nss[Stateful Set A]\nss --\u003e A-0\nss --\u003e A-1\nss --\u003e A-2\n```\n\n这样一来，更新时将 Pod 更换之后，新的 Pod 仍能够跟旧的 Pod 保持相同的名字。此外，与 Deployment 相比， Stateful Set 更新后同名的 Pod 仍能保持原来的 IP ，拿到同一个持久化卷，而且不同的 Pod 还能通过独立的 DNS 记录相互区分。这些内容后面还会详细介绍。\n\n\u003e **宠物与牛（ Cattle vs Pets ）的比喻**\n\u003e \n\u003e Deployment 更倾向于将 Pod 看作是牛：我们不会去关心每一个 Pod 个体，如果有一个 Pod 出现了问题，我们只需要把他杀掉并替换成新的 Pod 就好。\n\u003e \n\u003e 但 Stateful Set 更倾向于将 Pod 看作是宠物：弄来一直完全一模一样的宠物并不是容易的事，我们对待这些宠物必须小心翼翼。我们要给他们各自一个专属的名字，替换掉一只宠物时，必须要保证它的花色、名字、行为举止都与之前那只宠物一模一样。\n\n### Daemon Set\n\n不管是 Deployment 还是 Stateful Set ，一般都不会在意自己的 Pod 部署到哪个节点。而假如你不在意自己 Pod 的数量，但需要保证每个节点上都运行一个 Pod 时，就需要 Daemon Set 了。\n\n需要保证每个节点上有且只有一个 Pod 在运行这种情况，经常会在基础结构相关的操作中出现。比如我需要在集群中部署 fluentd 采集 log ，一般来说需要在 Pod 里直接挂载节点磁盘上的文件路径。这种时候如果有一个节点上没有运行 Pod ，那个节点的 log 就采集不到；另一方面，一个节点上运行多个 Pod 毫无意义，而且可能还会导致 log 重复等冲突。\n\n这种需求下简单地使用 Replica Set 或是 Stateful Set 都是不能达到要求的，这两种资源都只能通过亲和性达到“尽量不部署在同一个节点”，做不到绝对。而且当节点数有变更时还需要手动更改设置。\n\n下面是一个用 fluent-bit helm chart 部署的 fluent-bit Daemon Set 的例子：\n\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/name: fluent-bit\n  name: fluent-bit\n  namespace: fluent-bit\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: fluent-bit\n      app.kubernetes.io/name: fluent-bit\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: fluent-bit\n        app.kubernetes.io/name: fluent-bit\n    spec:\n      containers:\n      - image: cr.fluentbit.io/fluent/fluent-bit:1.9.5\n        volumeMounts:\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: etcmachineid\n          mountPath: /etc/machine-id\n          readOnly: true\n      volumes:\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n          type: \"\"\n      - name: etcmachineid\n        hostPath:\n          path: /etc/machine-id\n          type: File\n```\n\nSelector 之类的都是一样的了，而 Daemon Set 不能指定 replicas 。另外可以看到一个比较刺激的地方： Volume 里使用了 `hostPath` 这种 Volume ，在 Pod 里直接指定了宿主机磁盘上的路径。\n\nK8s 认为经过抽象后， Pod 不应该去关心自己在哪台宿主机上，一般来说是不推荐在 Pod 里直接访问宿主机路径的（不过也没有强制禁止）。不过 Daemon Set 是个特例，由于 Daemon Set 生成的 Pod 与节点强相关， K8s 十分推荐在且仅在 Daemon Set 的 Pod 中访问宿主机路径。\n\n### Job 与 CronJob\n\nReplica Set ， Stateful Set ， Daemon Set 的 Pod 中运行的一般是持续运行的程序，因此这些 Pod 运行终止后会有相应的机制重启这些 Pod 。而 Job 与 Cron Job 这两种资源则专门负责调度不会持续运行的程序。\n\n下面是 《Kubernetes in Action》 书中的一个例子：\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  completions: 5\n  parallelism: 2\n  template:\n    spec:\n      containers:\n      - name: pi\n        image: perl:5.34.0\n        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n```\n\n可以看到，这个 Job 描述了一个会输出 PI 小数点后 2000 位的 Pod 模板。这个 Job 部署后，一共会以这个模板跑完 5 个 Pod ，其中最多并行跑 2 个，并在其中一个成功终止后再跑剩下的 Pod 。可以通过调整 `completions` 与 `parallelism` 字段调整并行与穿行数量。\n\n顺带一提，在 Job 定义中一般不会出现 selector ，但其实 Job 有 selector 字段，一般会由 K8s 为每个 Job 生成一个 uuid 作为 selector 。\n\n另外，可以通过部署 CronJob 这种资源来定时执行 Job 。下面是 《Kubernetes in Action》 书中关于 CronJob 的例子：\n\n```yaml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pi\nspec:\n  schedule: \"0 0 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: pi\n            image: perl:5.34.0\n            command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n          restartPolicy: Never\n```\n\n这个例子中， CronJob 会在每天的 0 点创建一个只运行一个 Pod 的 Job 。 CronJob 不会直接创建 Pod ，而是创建一个 Job ，再由 Job 创建 Pod （就像 Deployment 与 Replica Set 的关系）。另外， CronJob 创建的 Job 会限制 `completions` 与 `parallelism` 都只能等于 1 。\n\n\u003e 关于资源的名称空间\n\u003e \n\u003e 在 K8s 中，各资源都是不能重名的。不能部署两个都叫 `gateway` 的 Pod ，资源之间有可能因为名字冲突而导致部署不成功。（部署一个叫 `gateway` 的 Pod 和一个叫 `gateway` 的 Deployment 倒是可以，因为 `gateway` 不是他们两个的全名，他们的全名分别叫 `pod/gateway` 及 `deployment/gateway` 。）\n\u003e 另外我们已经知道 Deployment 等资源一般会通过标签等来管理自己创建的资源，那两份不相关的应用完全有可能会撞标签，这时候部署逻辑就有可能会出问题。\n\u003e \n\u003e K8s 中提供了名称空间这种资源，用于进行资源隔离。K8s 中大部分资源都从属于一个且仅从属于一个名称空间， Deployment 等资源一般只能控制在同一名称空间下的资源，而不会影响其他名称空间。\n\u003e \n\u003e 另外，也有一些资源是名称空间无关的，比如节点 `Node` 。\n\n\n","title":"Kubernetes 入门 （1）","abstract":"我们知道 K8s 利用了容器虚拟化技术。而说到容器虚拟化就要说 Docker 。可是，容器到底是什么？ Docker 又为我们做了些什么？我们又为什么要用 K8s ？\n\u003e 要把一个不知道打过多少个升级补丁，不知道经历了多少任管理员的系统迁移到其他机器上，毫无疑问会是一场灾难。 —— Chad Fowler 《Trash Your Servers and Burn Your Code》\n\"Write once, run anywhere\" 是 Java 曾经的口号。 Java 企图通过 JVM 虚拟机来实现一个可执行程序在多平台间的移植性。但我们现在知道， Java 语言并没能实现他的目标，会在操作系统调用、第三方依赖丢失、两个程序间依赖的冲突等各方面出现问题。","length":644,"created_at":"2022-08-13T17:45:31.000Z","updated_at":"2022-08-20T14:02:18.000Z","tags":["Kubernetes","DevOps","Docker","Cloud Native"],"license":true}},{"pathMapping":{"filePath":"public/content/ideas/newest.mdx","pagePath":"/ideas/newest","slug":"newest"},"meta":{"content":"\n这里是第一行，\n然后这里是第二行。\n\n这里是一些内容。\n再来一行。\n第三行。\n\n\u003cLicense /\u003e\n\n| 名称空间      | 隔离内容                      | 内核版本 |\n| :------------ | :---------------------------- | :------- |\n| Mount         | 文件系统与路径等              | 2.4.19   |\n| UTS           | 主机的 Hostname、Domain names | 2.6.19   |\n| IPC           | 进程间通信管道                | 2.6.19   |\n| PID           | 独立的进程编号空间            | 2.6.24   |\n| Network       | 网卡、IP 地址、端口等网络资源 | 2.6.29   |\n| User          | 进程独立的用户和用户组        | 3.8      |\n| Cgroup        | CPU 时间片，内存分页等        | 4.6      |\n| Time \\\u003c- New! | 进程独立的系统时间            | 5.6      |\n\n```mermaid\nsequenceDiagram\nAlice-\u003e\u003eJohn: Hello John, how are you?\nloop Healthcheck\n    John-\u003e\u003eJohn: Fight against hypochondria\nend\nNote right of John: Rational thoughts!\nJohn--\u003e\u003eAlice: Great!\nJohn-\u003e\u003eBob: How about you?\nBob--\u003e\u003eJohn: Jolly good!\n```\n\n```mermaid\npie\n\"Dogs\" : 386\n\"Cats\" : 85\n\"Rats\" : 15\n```\n\n```mermaid\ngraph LR\n\nohmy--\u003ecoll\n\n```\n\nnew lines!\n","title":"Kubernetes 入门 （1）","abstract":"这里是第一行，\n然后这里是第二行。\n这里是一些内容。","length":49,"created_at":"2022-08-13T17:45:31.000Z","updated_at":"2022-08-20T14:02:18.000Z","tags":["Kubernetes","DevOps","Docker","Cloud Native","Cloud Computing"],"license":true}}]},"__N_SSG":true},"page":"/tags/[tag]","query":{"tag":"cloud-native"},"buildId":"8XSB2OWvzgvsvl9OJIqWc","assetPrefix":"/blog-next","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>