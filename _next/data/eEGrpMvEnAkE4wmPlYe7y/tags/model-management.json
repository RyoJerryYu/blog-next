{"pageProps":{"allTagInfos":[{"tag":"杂技","slug":"杂技","path":"/tags/杂技","postSlugs":[{"postType":"articles","postPagePath":"/articles/Building-this-blog"},{"postType":"articles","postPagePath":"/articles/hello-world"},{"postType":"articles","postPagePath":"/articles/the-using-in-cpp"}]},{"tag":"Blog","slug":"blog","path":"/tags/blog","postSlugs":[{"postType":"articles","postPagePath":"/articles/Building-this-blog"},{"postType":"articles","postPagePath":"/articles/init-a-new-hexo-project"},{"postType":"articles","postPagePath":"/articles/create-blog-cicd-by-github"},{"postType":"articles","postPagePath":"/articles/use-paste-image-and-vscode-memo"},{"postType":"ideas","postPagePath":"/ideas/blog-in-next"},{"postType":"ideas","postPagePath":"/ideas/blog-syntax"}]},{"tag":"杂谈","slug":"杂谈","path":"/tags/杂谈","postSlugs":[{"postType":"articles","postPagePath":"/articles/hello-world"},{"postType":"articles","postPagePath":"/articles/try-cursor-and-thinking"}]},{"tag":"C++","slug":"c++","path":"/tags/c++","postSlugs":[{"postType":"articles","postPagePath":"/articles/the-using-in-cpp"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/cpp-rvo-and-rust-move-semantics"}]},{"tag":"Python","slug":"python","path":"/tags/python","postSlugs":[{"postType":"articles","postPagePath":"/articles/python-dict"}]},{"tag":"数据结构","slug":"数据结构","path":"/tags/数据结构","postSlugs":[{"postType":"articles","postPagePath":"/articles/python-dict"},{"postType":"articles","postPagePath":"/articles/Sort-algorithm"},{"postType":"articles","postPagePath":"/articles/Handy-heap-cheat-sheet"}]},{"tag":"算法","slug":"算法","path":"/tags/算法","postSlugs":[{"postType":"articles","postPagePath":"/articles/Sort-algorithm"},{"postType":"articles","postPagePath":"/articles/Handy-heap-cheat-sheet"}]},{"tag":"排序","slug":"排序","path":"/tags/排序","postSlugs":[{"postType":"articles","postPagePath":"/articles/Sort-algorithm"}]},{"tag":"算法竞赛","slug":"算法竞赛","path":"/tags/算法竞赛","postSlugs":[{"postType":"articles","postPagePath":"/articles/Handy-heap-cheat-sheet"}]},{"tag":"设计模式","slug":"设计模式","path":"/tags/设计模式","postSlugs":[{"postType":"articles","postPagePath":"/articles/The-beauty-of-design-parten"}]},{"tag":"笔记","slug":"笔记","path":"/tags/笔记","postSlugs":[{"postType":"articles","postPagePath":"/articles/The-beauty-of-design-parten"}]},{"tag":"GitHub","slug":"github","path":"/tags/github","postSlugs":[{"postType":"articles","postPagePath":"/articles/create-blog-cicd-by-github"}]},{"tag":"AWS","slug":"aws","path":"/tags/aws","postSlugs":[{"postType":"articles","postPagePath":"/articles/create-blog-cicd-by-github"}]},{"tag":"CI/CD","slug":"ci-cd","path":"/tags/ci-cd","postSlugs":[{"postType":"articles","postPagePath":"/articles/create-blog-cicd-by-github"}]},{"tag":"IaC","slug":"iac","path":"/tags/iac","postSlugs":[{"postType":"articles","postPagePath":"/articles/create-blog-cicd-by-github"}]},{"tag":"DevOps","slug":"devops","path":"/tags/devops","postSlugs":[{"postType":"articles","postPagePath":"/articles/create-blog-cicd-by-github"},{"postType":"articles","postPagePath":"/articles/introduction-for-k8s"},{"postType":"articles","postPagePath":"/articles/introduction-for-k8s-2"},{"postType":"ideas","postPagePath":"/ideas/newest"}]},{"tag":"VSCode","slug":"vscode","path":"/tags/vscode","postSlugs":[{"postType":"articles","postPagePath":"/articles/use-paste-image-and-vscode-memo"}]},{"tag":"Hexo","slug":"hexo","path":"/tags/hexo","postSlugs":[{"postType":"articles","postPagePath":"/articles/use-paste-image-and-vscode-memo"}]},{"tag":"JavaScript","slug":"javascript","path":"/tags/javascript","postSlugs":[{"postType":"articles","postPagePath":"/articles/use-paste-image-and-vscode-memo"}]},{"tag":"Kubernetes","slug":"kubernetes","path":"/tags/kubernetes","postSlugs":[{"postType":"articles","postPagePath":"/articles/introduction-for-k8s"},{"postType":"articles","postPagePath":"/articles/introduction-for-k8s-2"},{"postType":"ideas","postPagePath":"/ideas/newest"}]},{"tag":"Docker","slug":"docker","path":"/tags/docker","postSlugs":[{"postType":"articles","postPagePath":"/articles/introduction-for-k8s"},{"postType":"articles","postPagePath":"/articles/introduction-for-k8s-2"},{"postType":"ideas","postPagePath":"/ideas/newest"}]},{"tag":"Cloud Native","slug":"cloud-native","path":"/tags/cloud-native","postSlugs":[{"postType":"articles","postPagePath":"/articles/introduction-for-k8s"},{"postType":"articles","postPagePath":"/articles/introduction-for-k8s-2"},{"postType":"ideas","postPagePath":"/ideas/newest"}]},{"tag":"Cursor","slug":"cursor","path":"/tags/cursor","postSlugs":[{"postType":"articles","postPagePath":"/articles/try-cursor-and-thinking"}]},{"tag":"可视化","slug":"可视化","path":"/tags/可视化","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog 画图 Iframe 测试"}]},{"tag":"数学","slug":"数学","path":"/tags/数学","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog 画图 Iframe 测试"}]},{"tag":"工具","slug":"工具","path":"/tags/工具","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog 画图 Iframe 测试"}]},{"tag":"Nextjs","slug":"nextjs","path":"/tags/nextjs","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog-in-next"},{"postType":"ideas","postPagePath":"/ideas/blog-syntax"}]},{"tag":"Tag1","slug":"tag1","path":"/tags/tag1","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog-syntax"}]},{"tag":"Tag2","slug":"tag2","path":"/tags/tag2","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog-syntax"}]},{"tag":"单行Tag","slug":"单行tag","path":"/tags/单行tag","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog-syntax"}]},{"tag":"Tag","slug":"tag","path":"/tags/tag","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog-syntax"}]},{"tag":"Tag/Tag2","slug":"tag-tag2","path":"/tags/tag-tag2","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog-syntax"}]},{"tag":"Tag3","slug":"tag3","path":"/tags/tag3","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog-syntax"}]},{"tag":"Linux","slug":"linux","path":"/tags/linux","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/blog-syntax"},{"postType":"ideas","postPagePath":"/ideas/Linux Systemd"},{"postType":"ideas","postPagePath":"/ideas/Linux 信号处理 —— Signal"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 内存分页、分段"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 堆和栈"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 虚拟内存"},{"postType":"ideas","postPagePath":"/ideas/Linux 调度 —— 进程与线程"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/executable-file-formats"}]},{"tag":"systemctl","slug":"systemctl","path":"/tags/systemctl","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/Linux Systemd"}]},{"tag":"journalctl","slug":"journalctl","path":"/tags/journalctl","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/Linux Systemd"}]},{"tag":"timedatectl","slug":"timedatectl","path":"/tags/timedatectl","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/Linux Systemd"}]},{"tag":"BasicKnowledge","slug":"basicknowledge","path":"/tags/basicknowledge","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/Linux Systemd"},{"postType":"ideas","postPagePath":"/ideas/Linux 信号处理 —— Signal"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 内存分页、分段"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 堆和栈"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 虚拟内存"},{"postType":"ideas","postPagePath":"/ideas/Linux 调度 —— 进程与线程"}]},{"tag":"Operation","slug":"operation","path":"/tags/operation","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/Linux Systemd"},{"postType":"ideas","postPagePath":"/ideas/Linux 信号处理 —— Signal"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 内存分页、分段"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 虚拟内存"}]},{"tag":"Signal","slug":"signal","path":"/tags/signal","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/Linux 信号处理 —— Signal"}]},{"tag":"memory","slug":"memory","path":"/tags/memory","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 内存分页、分段"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 堆和栈"},{"postType":"ideas","postPagePath":"/ideas/Linux 内存 —— 虚拟内存"}]},{"tag":"schedule","slug":"schedule","path":"/tags/schedule","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/Linux 调度 —— 进程与线程"}]},{"tag":"Cloud Computing","slug":"cloud-computing","path":"/tags/cloud-computing","postSlugs":[{"postType":"ideas","postPagePath":"/ideas/newest"}]},{"tag":"PyTorch","slug":"pytorch","path":"/tags/pytorch","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/deep-learning-model-formats"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-backpropagation-mechanism"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-model-save-and-load"}]},{"tag":"ONNX","slug":"onnx","path":"/tags/onnx","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/deep-learning-model-formats"}]},{"tag":"深度学习","slug":"深度学习","path":"/tags/深度学习","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/deep-learning-model-formats"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-backpropagation-mechanism"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"模型部署","slug":"模型部署","path":"/tags/模型部署","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/deep-learning-model-formats"}]},{"tag":"学习笔记","slug":"学习笔记","path":"/tags/学习笔记","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/deep-learning-model-formats"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/executable-file-formats"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-backpropagation-mechanism"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/mqtt-protocol-principles-applications"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"OpenCV","slug":"opencv","path":"/tags/opencv","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"}]},{"tag":"图像处理","slug":"图像处理","path":"/tags/图像处理","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"}]},{"tag":"Rust","slug":"rust","path":"/tags/rust","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/cpp-rvo-and-rust-move-semantics"}]},{"tag":"编程语言","slug":"编程语言","path":"/tags/编程语言","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/cpp-rvo-and-rust-move-semantics"}]},{"tag":"Windows","slug":"windows","path":"/tags/windows","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/executable-file-formats"}]},{"tag":"macOS","slug":"macos","path":"/tags/macos","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/executable-file-formats"}]},{"tag":"可执行文件","slug":"可执行文件","path":"/tags/可执行文件","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/executable-file-formats"}]},{"tag":"LoRA","slug":"lora","path":"/tags/lora","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"参数高效微调","slug":"参数高效微调","path":"/tags/参数高效微调","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"模型训练","slug":"模型训练","path":"/tags/模型训练","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"反向传播","slug":"反向传播","path":"/tags/反向传播","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-backpropagation-mechanism"}]},{"tag":"自动微分","slug":"自动微分","path":"/tags/自动微分","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-backpropagation-mechanism"}]},{"tag":"GoLang","slug":"golang","path":"/tags/golang","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/golang-new-and-memory-management"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/golang-scheduler-preemption"}]},{"tag":"Memory Management","slug":"memory-management","path":"/tags/memory-management","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/golang-new-and-memory-management"}]},{"tag":"Deep Learning","slug":"deep-learning","path":"/tags/deep-learning","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-model-save-and-load"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/resnet-math-and-gradient-vanishing"}]},{"tag":"Model Management","slug":"model-management","path":"/tags/model-management","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-model-save-and-load"}]},{"tag":"Neural Networks","slug":"neural-networks","path":"/tags/neural-networks","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/resnet-math-and-gradient-vanishing"}]},{"tag":"ResNet","slug":"resnet","path":"/tags/resnet","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/resnet-math-and-gradient-vanishing"}]},{"tag":"Mathematics","slug":"mathematics","path":"/tags/mathematics","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/resnet-math-and-gradient-vanishing"}]},{"tag":"协议","slug":"协议","path":"/tags/协议","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/mqtt-protocol-principles-applications"}]},{"tag":"IoT","slug":"iot","path":"/tags/iot","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/mqtt-protocol-principles-applications"}]},{"tag":"物联网","slug":"物联网","path":"/tags/物联网","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/mqtt-protocol-principles-applications"}]},{"tag":"消息队列","slug":"消息队列","path":"/tags/消息队列","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/mqtt-protocol-principles-applications"}]},{"tag":"Stable Diffusion","slug":"stable-diffusion","path":"/tags/stable-diffusion","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"Concurrency","slug":"concurrency","path":"/tags/concurrency","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/golang-scheduler-preemption"}]},{"tag":"Scheduler","slug":"scheduler","path":"/tags/scheduler","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/golang-scheduler-preemption"}]},{"tag":"Goroutine","slug":"goroutine","path":"/tags/goroutine","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/golang-scheduler-preemption"}]},{"tag":"test","slug":"test","path":"/tags/test","postSlugs":[{"postType":"testwiki","postPagePath":"/testwiki/page1"},{"postType":"testwiki","postPagePath":"/testwiki/page1/subpage1"},{"postType":"testwiki","postPagePath":"/testwiki/page2"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage2"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage3"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage3/subpage34"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage1"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage2"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage3"}]},{"tag":"wiki","slug":"wiki","path":"/tags/wiki","postSlugs":[{"postType":"testwiki","postPagePath":"/testwiki/page1"},{"postType":"testwiki","postPagePath":"/testwiki/page1/subpage1"},{"postType":"testwiki","postPagePath":"/testwiki/page2"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage2"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage3"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage3/subpage34"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage1"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage2"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage3"}]},{"tag":"page1","slug":"page1","path":"/tags/page1","postSlugs":[{"postType":"testwiki","postPagePath":"/testwiki/page1"},{"postType":"testwiki","postPagePath":"/testwiki/page1/subpage1"}]},{"tag":"subpage1","slug":"subpage1","path":"/tags/subpage1","postSlugs":[{"postType":"testwiki","postPagePath":"/testwiki/page1/subpage1"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage1"}]},{"tag":"page2","slug":"page2","path":"/tags/page2","postSlugs":[{"postType":"testwiki","postPagePath":"/testwiki/page2"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage2"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage3"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage3/subpage34"}]},{"tag":"subpage2","slug":"subpage2","path":"/tags/subpage2","postSlugs":[{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage2"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage2"}]},{"tag":"subpage3","slug":"subpage3","path":"/tags/subpage3","postSlugs":[{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage3"},{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage3/subpage34"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage3"}]},{"tag":"subpage34","slug":"subpage34","path":"/tags/subpage34","postSlugs":[{"postType":"testwiki","postPagePath":"/testwiki/page2/subpage3/subpage34"}]},{"tag":"page3","slug":"page3","path":"/tags/page3","postSlugs":[{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage1"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage2"},{"postType":"testwiki","postPagePath":"/testwiki/page3/subpage3"}]}],"selectedTagInfo":{"tag":"Model Management","slug":"model-management","path":"/tags/model-management","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-model-save-and-load"}]},"posts":[{"pathMapping":{"filePath":"public/content/learn_from_ai/2025-03-07-pytorch-model-save-and-load.md","pagePath":"/learn_from_ai/pytorch-model-save-and-load","slug":"pytorch-model-save-and-load"},"meta":{"content":"\n> 本文为与 DeepSeek 对话记录，深入探讨 PyTorch 中模型的保存和加载机制，包括不同的保存方式、最佳实践以及常见问题的解决方案。\n\n## Q: PyTorch 中所谓的保存模型/加载模型，指的是保存/加载什么？是 state_dict 吗？\n\n> [!reasoning]-\n>\n> 嗯，我现在要弄清楚 PyTorch 中保存和加载模型到底是怎么回事。用户问是不是保存和加载 state_dict，我需要仔细想一想这个问题。\n>\n> 首先，我记得在 PyTorch 里训练模型的时候，模型的可学习参数，比如各层的权重和偏置，是存放在模型的 parameters 中的。而 state_dict 应该是一个保存这些参数的东西。对，每个 torch.nn.Module 都有 state_dict，它是一个字典，将每个层映射到对应的参数张量。\n>\n> 那保存模型的时候，有两种常见的方法。一种是直接保存整个模型，使用 torch.save(model, 'model.pth')。这样的话，保存的是整个模型对象，包括结构定义和参数。不过这种方法可能有缺点，比如当模型定义所在的文件路径变化时，加载的时候可能会找不到类定义，导致错误。所以通常更推荐另一种方法，就是只保存 state_dict，这样需要的时候再加载到一个定义好的模型结构中。\n>\n> 所以用户的问题是说保存模型是不是保存 state_dict，对吧？是的，通常最佳做法是保存 state_dict，而不是整个模型对象。因为 state_dict 只包含参数，不包含模型结构，这样在加载的时候更灵活。比如，当你想在不同项目中使用同一个模型参数，或者模型类有修改时，加载 state_dict 会更方便。\n>\n> 那加载的时候，就需要先实例化模型，然后用 load_state_dict 方法加载参数。比如：\n>\n> model = MyModelClass(\\*args, \\*\\*kwargs)\n>\n> model.load_state_dict(torch.load('model.pth'))\n>\n> 这样参数就被正确加载到模型里了。而如果保存的是整个模型，加载的时候可以直接用 model = torch.load('model.pth')，但这时候模型类必须在当前作用域可访问，否则会出错。\n>\n> 所以总结一下，保存模型通常指的是保存 state_dict，而加载模型就是加载这个 state_dict 到已经定义好的模型结构中。当然，PyTorch 也允许保存整个模型，但不够灵活，所以推荐用 state_dict 的方式。\n>\n> 另外，可能还有其他需要保存的内容，比如优化器的 state_dict，这样在恢复训练的时候可以接着之前的优化器状态继续训练。这时候保存的 checkpoint 可能包括 model_state_dict、optimizer_state_dict、epoch、loss 等信息。加载的时候需要分别处理这些部分。\n>\n> 但用户的问题主要是关于保存和加载模型本身，所以核心答案应该是 state_dict。不过要说明保存整个模型的情况也存在，但 state_dict 更推荐。所以回答的时候应该涵盖这两种情况，但强调 state_dict 是更常用的方法。\n\n在 PyTorch 中，保存和加载模型的**核心是 `state_dict`**，但具体操作可以灵活选择。以下是详细说明：\n\n---\n\n### 1. **`state_dict` 是什么？**\n\n- **`state_dict`** 是一个 Python 字典对象，它将模型的每一层（如卷积层、全连接层等）映射到对应的参数张量（权重和偏置）。\n- 例如，一个简单的神经网络可能包含：\n  ```python\n  model.state_dict() = {\n      'conv1.weight': tensor(...),\n      'conv1.bias': tensor(...),\n      'fc1.weight': tensor(...),\n      'fc1.bias': tensor(...),\n      ...\n  }\n  ```\n- **优化器**（如 `torch.optim.SGD`）也有自己的 `state_dict`，保存优化器的状态（如动量缓冲等）。\n\n---\n\n### 2. **保存模型的两种方式**\n\n#### **方式 1：仅保存 `state_dict`（推荐）**\n\n- **代码示例**：\n  ```python\n  torch.save(model.state_dict(), 'model_weights.pth')\n  ```\n- **优点**：\n  - 文件体积小，只保存参数，不保存模型结构。\n  - 加载灵活：需先实例化模型结构，再加载参数。\n  - 避免因代码变更（如类名修改）导致的加载失败。\n\n#### **方式 2：保存整个模型（不推荐）**\n\n- **代码示例**：\n  ```python\n  torch.save(model, 'entire_model.pth')\n  ```\n- **缺点**：\n  - 文件体积大，包含模型结构和参数。\n  - 依赖原始代码环境（模型类的定义必须存在），否则加载时会报错。\n\n---\n\n### 3. **加载模型的对应方法**\n\n#### **加载 `state_dict`（需先构建模型结构）**\n\n```python\nmodel = MyModelClass()  # 必须与保存时的模型结构一致\nmodel.load_state_dict(torch.load('model_weights.pth'))\nmodel.eval()  # 切换到推理模式（关闭 dropout 等）\n```\n\n#### **加载整个模型（直接使用）**\n\n```python\nmodel = torch.load('entire_model.pth')\nmodel.eval()\n```\n\n---\n\n### 4. **实际场景中的最佳实践**\n\n- **保存训练检查点（Checkpoint）**：\n  通常需要保存 `state_dict` 之外的信息（如优化器状态、训练步数等）：\n\n  ```python\n  checkpoint = {\n      'model_state_dict': model.state_dict(),\n      'optimizer_state_dict': optimizer.state_dict(),\n      'epoch': 100,\n      'loss': loss,\n  }\n  torch.save(checkpoint, 'checkpoint.pth')\n  ```\n\n- **加载检查点**：\n  ```python\n  checkpoint = torch.load('checkpoint.pth')\n  model.load_state_dict(checkpoint['model_state_dict'])\n  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n  epoch = checkpoint['epoch']\n  ```\n\n---\n\n### 5. **关键总结**\n\n- **保存模型**：本质是保存模型的 `state_dict`（推荐），但也可以保存整个模型。\n- **加载模型**：\n  - 若保存的是 `state_dict`，需先实例化模型结构，再加载参数。\n  - 若保存的是整个模型，直接加载但需保证代码兼容性。\n- **灵活性**：使用 `state_dict` 更安全、更灵活，适合跨项目或模型结构可能变化的场景。\n\n## Q: model 的 config 又是什么东西？是超参吗？\n\n> [!reasoning]-\n>\n> 嗯，用户之前问过 PyTorch 中保存和加载模型的概念，现在又接着问\"model 的 config 是什么东西？是超参吗？\"我需要仔细分析这个问题，确保理解用户的真实需求。\n>\n> 首先，用户已经了解 state_dict，现在转向模型配置（config）。可能他们在实际项目中遇到了需要保存或管理模型结构的情况，或者在阅读代码时遇到了 config 这个术语。用户想知道 config 具体指什么，是否包括超参数。\n>\n> 接下来，我需要明确\"model config\"的定义。在机器学习中，模型配置通常指的是定义模型结构的参数，比如神经网络的层数、每层的神经元数量、激活函数类型等。而超参数可能包括学习率、批量大小、优化器类型等训练过程中的设置。但有时候这两者之间的界限并不严格，特别是当某些结构参数也被视为超参数时。\n>\n> 用户可能混淆了模型配置和超参数，需要区分这两者。需要举例说明，比如在 Transformer 模型中，config 可能包含隐藏层大小、头数等，而超参数则是训练时的设置。但有时，例如在 HuggingFace 的库中，模型配置类（如 BertConfig）确实包含了模型结构的所有参数，这可能让用户产生疑问。\n>\n> 另外，用户可能想知道为什么需要单独保存 config。可能他们在保存模型时，除了 state_dict 之外，还需要保存模型结构的信息，以便重新实例化模型。比如，如果只保存 state_dict，加载时需要先有模型类的定义和 config 参数，才能正确创建模型实例，然后加载权重。\n>\n> 需要解释 config 的作用：它定义了模型的结构，是重建模型实例的必要信息。与超参数不同，config 主要涉及模型架构，而超参数涉及训练过程。但有些情况下，两者可能有重叠，比如学习率可能被包含在训练配置中，而层数在模型配置中。\n>\n> 还需要考虑用户可能的实际应用场景。比如，使用 HuggingFace 的 Transformers 库时，模型配置被明确分离，用户需要保存 config.json 和 model.bin。这时候，config 保存模型结构，model.bin 保存 state_dict。用户可能在这种情况下遇到 config 的概念，需要解释这种设计的原因和好处。\n>\n> 此外，用户可能想知道如何管理 config，比如使用 YAML 或 JSON 文件来存储模型结构参数，方便版本控制和实验复现。这也是为什么分开保存 config 和 state_dict 更灵活，因为修改结构时只需调整 config 而不影响训练好的权重。\n>\n> 最后，要确保回答结构清晰，分点说明，给出代码示例，并比较 config 与超参数的区别和联系。同时，注意用户可能的技术水平，避免使用过于专业的术语，保持解释的易懂性。\n\n在机器学习中，**模型的 `config`（配置）** 通常指**定义模型结构的所有参数**，而超参（Hyperparameters）更多指**控制训练过程的参数**。这两者有时会有重叠，但核心区别在于作用对象。以下是详细解释：\n\n---\n\n### 1. **什么是模型的 `config`？**\n\n- **模型配置**（Model Configuration）是定义**模型结构**的一组参数，例如：\n\n  - 神经网络的层数、每层的神经元数量\n  - 卷积核大小、步长（Stride）、填充（Padding）\n  - 激活函数类型（如 ReLU、Sigmoid）\n  - 注意力机制的头数（如 Transformer 中的 `num_heads`）\n  - 词嵌入维度（如 NLP 模型中的 `hidden_size`）\n\n- **示例**（以 Transformer 模型为例）：\n  ```python\n  config = {\n      \"num_layers\": 12,          # Transformer 层数\n      \"hidden_size\": 768,         # 隐藏层维度\n      \"num_attention_heads\": 12,  # 注意力头数\n      \"intermediate_size\": 3072,  # FeedForward 层维度\n      \"vocab_size\": 30522,        # 词表大小\n  }\n  ```\n\n---\n\n### 2. **模型 `config` 与超参数的区别**\n\n| **类别**     | **模型配置（Config）**                 | **超参数（Hyperparameters）**                                  |\n| ------------ | -------------------------------------- | -------------------------------------------------------------- |\n| **作用对象** | 模型结构                               | 训练过程                                                       |\n| **典型参数** | 层数、维度、注意力头数、卷积核大小     | 学习率、批量大小（Batch Size）、优化器类型、训练轮数（Epochs） |\n| **保存目的** | 重建模型结构（如加载 `state_dict` 前） | 复现训练过程（如调参、实验对比）                               |\n| **修改频率** | 低频（模型结构稳定后很少改动）         | 高频（需反复调整优化）                                         |\n\n---\n\n### 3. **为什么需要单独管理 `config`？**\n\n- **场景 1：重建模型**  \n  若仅保存 `state_dict`（模型权重），加载时必须**先根据 `config` 重建模型结构**，才能加载权重：\n\n  ```python\n  # 保存\n  torch.save(model.state_dict(), \"model_weights.pth\")\n  with open(\"model_config.json\", \"w\") as f:\n      json.dump(config, f)\n\n  # 加载\n  with open(\"model_config.json\", \"r\") as f:\n      config = json.load(f)\n  model = MyModel(**config)  # 根据 config 实例化模型\n  model.load_state_dict(torch.load(\"model_weights.pth\"))\n  ```\n\n- **场景 2：版本控制与实验复现**  \n  将 `config` 保存为 JSON/YAML 文件，便于记录不同模型结构的实验效果。\n\n- **场景 3：共享模型**  \n  开源社区常用模式（如 HuggingFace Transformers）：\n  - `config.json`：定义模型结构\n  - `pytorch_model.bin`：保存 `state_dict`\n\n---\n\n### 4. **`config` 与超参数的重叠情况**\n\n某些参数可能同时影响模型结构和训练，例如：\n\n- **Dropout 比率**：既是模型结构的一部分，也被视为超参数。\n- **词嵌入维度**：影响模型结构，但可能因训练资源调整（如显存不足时调小维度）。\n\n此时，可以**将两者统一管理**，但建议分开存储：\n\n```python\n# 统一配置文件（config.yaml）\nmodel:\n  num_layers: 12\n  hidden_size: 768\ntraining:\n  learning_rate: 1e-4\n  batch_size: 32\n```\n\n---\n\n### 5. **实际代码示例**\n\n#### 定义模型时使用 `config`：\n\n```python\nclass TransformerModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embedding = nn.Embedding(config[\"vocab_size\"], config[\"hidden_size\"])\n        self.layers = nn.ModuleList([TransformerBlock(config) for _ in range(config[\"num_layers\"])])\n        # ...\n\n# 初始化模型\nconfig = {\"vocab_size\": 30000, \"hidden_size\": 512, \"num_layers\": 6}\nmodel = TransformerModel(config)\n```\n\n#### 保存与加载完整模型（含结构和权重）：\n\n```python\n# 保存（不推荐，但某些场景需要）\nfull_model = {\"config\": config, \"state_dict\": model.state_dict()}\ntorch.save(full_model, \"full_model.pth\")\n\n# 加载\nloaded = torch.load(\"full_model.pth\")\nmodel = TransformerModel(loaded[\"config\"])\nmodel.load_state_dict(loaded[\"state_dict\"])\n```\n\n---\n\n### 6. **总结**\n\n- **模型 `config`**：定义模型结构的参数（如层数、维度），是**重建模型**的必需信息。\n- **超参数**：控制训练过程的参数（如学习率、批量大小），影响模型如何学习。\n- **最佳实践**：\n  - 始终保存 `config`（如 JSON 文件），确保模型可复现。\n  - 分开管理模型配置和训练超参数，避免混淆。\n\n> [!info] 亮曰\n>\n> - Config 主要是定义模型结构。会根据 Config 决定有多少层等。\n> - 超参主要是训练过程中的学习率、 Epochs 等，复现训练过程时需要，但与模型结构无关，只与训练过程有关。\n\n## Q: 所谓\"权重\"就是指 state_dict 中的内容吗？\n\n> [!reasoning]-\n>\n> 嗯，用户问：\"所谓'权重'就是指 state_dict 中的内容吗？\"这个问题看起来是想确认 PyTorch 中的权重和 state_dict 之间的关系。首先，我需要回顾一下之前的对话，确保回答的一致性。\n>\n> 之前已经解释了 state_dict 是保存模型参数的核心，里面包含了各层的权重和偏置。用户可能现在对\"权重\"这个术语的具体指向有疑问，需要明确权重是否等同于 state_dict 中的所有内容。\n>\n> 首先，我应该明确\"权重\"在机器学习中的定义。通常，权重指的是模型中的可学习参数，比如线性层中的权重矩阵和偏置向量。而 state_dict 不仅包括这些权重，还可能包括其他参数，比如 BatchNorm 层的均值和方差，或者优化器的状态。因此，state_dict 的内容可能比单纯的\"权重\"更广泛。\n>\n> 接下来，需要区分 state_dict 中的不同部分。模型和优化器都有自己的 state_dict。模型的 state_dict 包含各层的参数，而优化器的 state_dict 包含优化器的状态，如动量缓存等。用户的问题可能集中在模型的 state_dict 中的权重部分，而是否包括其他参数需要澄清。\n>\n> 另外，用户可能对如何正确保存和加载模型有更深入的理解需求。比如，保存 state_dict 时，是否只保存了权重，还是还有其他必要的参数？需要说明 state_dict 中的内容对于模型重建的重要性，以及权重在其中的角色。\n>\n> 可能用户在实际操作中遇到了问题，比如加载模型时结构不匹配，或者参数未正确加载，因此需要明确 state_dict 中的内容。需要举例说明，比如一个简单的线性模型，展示其 state_dict 中的键值对，明确哪些是权重，哪些是偏置，或者其他类型的参数。\n>\n> 还需要考虑用户可能的误解，比如认为 state_dict 仅包含权重，而忽略了其他必要的参数，导致模型无法正确运行。因此，强调 state_dict 包含所有必要的参数，而权重是其中的一部分，但还有其他如偏置、BatchNorm 的参数等。\n>\n> 此外，用户可能关心如何仅保存或操作权重，比如迁移学习中的部分参数加载。这时候需要说明如何从 state_dict 中选择性地提取或加载特定的权重部分。\n>\n> 总结来说，回答需要明确以下几点：\n>\n> 1. 权重是模型中的可学习参数，如线性层的权重矩阵。\n>\n> 2. state_dict 包含模型的所有参数，包括权重、偏置以及其他层特定的参数。\n>\n> 3. 模型的 state_dict 与优化器的 state_dict 不同，后者包含优化器状态。\n>\n> 4. 正确加载模型需要确保 state_dict 的结构与当前模型结构匹配。\n>\n> 5. 示例说明 state_dict 中的具体内容，帮助用户直观理解。\n>\n> 同时，要注意用词准确，避免混淆，比如\"参数\"和\"权重\"的区别，确保用户能清晰理解两者的包含关系。可能还需要提到在保存和加载时的最佳实践，比如如何结合 config 来重建模型结构，再加载 state_dict 中的参数，以确保模型的正确性。\n>\n> 最后，检查是否有遗漏点，比如是否提到 eval()模式的影响，或者训练/推理时加载参数的注意事项，但根据问题范围，可能不需要展开，除非用户有进一步的问题。\n\n在 PyTorch 中，**\"权重\"确实是 `state_dict` 中的核心内容**，但 `state_dict` 包含的信息可能比\"权重\"更广泛。以下是详细解释：\n\n---\n\n### 1. **\"权重\"是什么？**\n\n- **权重（Weights）** 是模型中**可学习的参数**，例如：\n  - 全连接层（`nn.Linear`）中的权重矩阵（`weight`）和偏置（`bias`）。\n  - 卷积层（`nn.Conv2d`）的卷积核参数。\n  - 嵌入层（`nn.Embedding`）的词向量矩阵。\n- 这些参数通过反向传播和优化器（如 SGD、Adam）在训练过程中逐步更新。\n\n---\n\n### 2. **`state_dict` 中的内容**\n\n`state_dict` 是 PyTorch 中模型或优化器的**状态字典**，具体包含：\n\n- **对模型而言**：\n  - 所有权重（`weight`）、偏置（`bias`）。\n  - 其他**与层相关的参数**，例如：\n    - `BatchNorm` 层的 `running_mean` 和 `running_var`（在推理时使用的均值和方差）。\n    - `LSTM` 或 `GRU` 中的隐藏状态缓存（如果模型设计时显式存储了这些状态）。\n  ```python\n  # 示例：一个简单模型的 state_dict\n  model = nn.Sequential(\n      nn.Linear(10, 20),\n      nn.ReLU(),\n      nn.BatchNorm1d(20)\n  )\n  print(model.state_dict().keys())\n  # 输出：\n  # ['0.weight', '0.bias', '2.weight', '2.bias', '2.running_mean', '2.running_var', '2.num_batches_tracked']\n  ```\n- **对优化器而言**：\n  - 优化器的状态（如动量缓存、历史梯度等）。\n  ```python\n  optimizer = torch.optim.Adam(model.parameters())\n  print(optimizer.state_dict().keys())\n  # 输出：\n  # ['state', 'param_groups']\n  ```\n\n---\n\n### 3. **\"权重\"与 `state_dict` 的关系**\n\n- **`state_dict` 包含权重，但不仅限于权重**：\n  - 权重是 `state_dict` 中的一部分（如 `conv1.weight`, `fc.bias`）。\n  - `state_dict` 还可能包含**非可学习的参数**（如 `BatchNorm` 的统计量 `running_mean`）。\n  - 对于复杂模型（如 Transformer），还可能包含位置编码、注意力掩码等参数。\n- **总结**：\n  - **权重是 `state_dict` 的核心内容**，但 `state_dict` 是更全面的模型状态表示。\n\n---\n\n### 4. **为什么需要 `state_dict` 而不仅仅是权重？**\n\n- **完整恢复模型状态**：\n  如果仅保存权重，忽略 `BatchNorm` 的 `running_mean` 等参数，在推理时会导致统计量错误，影响模型性能。\n- **兼容性**：\n  加载模型时需确保 `state_dict` 的键（如层名称）与当前模型结构严格匹配，否则会报错 `Missing keys` 或 `Unexpected keys`。\n- **灵活性**：\n  可以手动修改 `state_dict`，例如迁移学习中仅加载部分层的权重。\n\n---\n\n### 5. **操作 `state_dict` 的常见场景**\n\n#### **(1) 查看模型权重**\n\n```python\nfor name, param in model.named_parameters():\n    print(f\"层名: {name}, 参数形状: {param.shape}\")\n```\n\n#### **(2) 选择性加载部分权重（迁移学习）**\n\n```python\npretrained_dict = torch.load('pretrained.pth')\nmodel_dict = model.state_dict()\n\n# 筛选出名称和形状匹配的权重\npretrained_dict = {\n    k: v for k, v in pretrained_dict.items()\n    if k in model_dict and v.shape == model_dict[k].shape\n}\nmodel_dict.update(pretrained_dict)\nmodel.load_state_dict(model_dict)\n```\n\n#### **(3) 手动修改权重**\n\n```python\n# 将第一层卷积的权重置零\nwith torch.no_grad():\n    model.conv1.weight.copy_(torch.zeros_like(model.conv1.weight))\n```\n\n---\n\n### 6. **总结**\n\n- **权重**是模型的可学习参数，是 `state_dict` 的核心内容。\n- **`state_dict`** 是模型状态的完整表示，包括：\n  - 所有权重、偏置。\n  - 非可学习的运行时参数（如 `BatchNorm` 的统计量）。\n- **最佳实践**：\n  - 保存模型时使用 `torch.save(model.state_dict(), ...)`。\n  - 加载时需先构建相同结构的模型，再调用 `model.load_state_dict()`。\n  - 若需调整模型结构，可选择性加载匹配的权重。\n\n> [!info] 亮曰\n>\n> state_dict 包括 Parameter 与 Buffer ，模型的可学习参数——**权重**主要指其中的 Parameter 。\n","title":"PyTorch 模型的保存与加载机制详解","abstract":"> 本文为与 DeepSeek 对话记录，深入探讨 PyTorch 中模型的保存和加载机制，包括不同的保存方式、最佳实践以及常见问题的解决方案。\n> [!reasoning]-\n>","length":433,"created_at":"2025-03-07T13:00:00.000Z","updated_at":"2025-03-07T13:00:00.000Z","tags":["Deep Learning","PyTorch","Model Management"],"license":true,"headingTrees":[{"key":"q-pytorch-中所谓的保存模型加载模型指的是保存加载什么是-state_dict-吗","href":"#q-pytorch-中所谓的保存模型加载模型指的是保存加载什么是-state_dict-吗","heading":2,"title":"Q: PyTorch 中所谓的保存模型/加载模型，指的是保存/加载什么？是 state_dict 吗？","children":[{"key":"1-state_dict-是什么","href":"#1-state_dict-是什么","heading":3,"title":"1. state_dict 是什么？","children":[],"id":"1-state_dict-是什么"},{"key":"2-保存模型的两种方式","href":"#2-保存模型的两种方式","heading":3,"title":"2. 保存模型的两种方式","children":[{"key":"方式-1仅保存-state_dict推荐","href":"#方式-1仅保存-state_dict推荐","heading":4,"title":"方式 1：仅保存 state_dict（推荐）","children":[],"id":"方式-1仅保存-state_dict推荐"},{"key":"方式-2保存整个模型不推荐","href":"#方式-2保存整个模型不推荐","heading":4,"title":"方式 2：保存整个模型（不推荐）","children":[],"id":"方式-2保存整个模型不推荐"}],"id":"2-保存模型的两种方式"},{"key":"3-加载模型的对应方法","href":"#3-加载模型的对应方法","heading":3,"title":"3. 加载模型的对应方法","children":[{"key":"加载-state_dict需先构建模型结构","href":"#加载-state_dict需先构建模型结构","heading":4,"title":"加载 state_dict（需先构建模型结构）","children":[],"id":"加载-state_dict需先构建模型结构"},{"key":"加载整个模型直接使用","href":"#加载整个模型直接使用","heading":4,"title":"加载整个模型（直接使用）","children":[],"id":"加载整个模型直接使用"}],"id":"3-加载模型的对应方法"},{"key":"4-实际场景中的最佳实践","href":"#4-实际场景中的最佳实践","heading":3,"title":"4. 实际场景中的最佳实践","children":[],"id":"4-实际场景中的最佳实践"},{"key":"5-关键总结","href":"#5-关键总结","heading":3,"title":"5. 关键总结","children":[],"id":"5-关键总结"}],"id":"q-pytorch-中所谓的保存模型加载模型指的是保存加载什么是-state_dict-吗"},{"key":"q-model-的-config-又是什么东西是超参吗","href":"#q-model-的-config-又是什么东西是超参吗","heading":2,"title":"Q: model 的 config 又是什么东西？是超参吗？","children":[{"key":"1-什么是模型的-config","href":"#1-什么是模型的-config","heading":3,"title":"1. 什么是模型的 config？","children":[],"id":"1-什么是模型的-config"},{"key":"2-模型-config-与超参数的区别","href":"#2-模型-config-与超参数的区别","heading":3,"title":"2. 模型 config 与超参数的区别","children":[],"id":"2-模型-config-与超参数的区别"},{"key":"3-为什么需要单独管理-config","href":"#3-为什么需要单独管理-config","heading":3,"title":"3. 为什么需要单独管理 config？","children":[],"id":"3-为什么需要单独管理-config"},{"key":"4-config-与超参数的重叠情况","href":"#4-config-与超参数的重叠情况","heading":3,"title":"4. config 与超参数的重叠情况","children":[],"id":"4-config-与超参数的重叠情况"},{"key":"5-实际代码示例","href":"#5-实际代码示例","heading":3,"title":"5. 实际代码示例","children":[{"key":"定义模型时使用-config","href":"#定义模型时使用-config","heading":4,"title":"定义模型时使用 config：","children":[],"id":"定义模型时使用-config"},{"key":"保存与加载完整模型含结构和权重","href":"#保存与加载完整模型含结构和权重","heading":4,"title":"保存与加载完整模型（含结构和权重）：","children":[],"id":"保存与加载完整模型含结构和权重"}],"id":"5-实际代码示例"},{"key":"6-总结","href":"#6-总结","heading":3,"title":"6. 总结","children":[],"id":"6-总结"}],"id":"q-model-的-config-又是什么东西是超参吗"},{"key":"q-所谓权重就是指-state_dict-中的内容吗","href":"#q-所谓权重就是指-state_dict-中的内容吗","heading":2,"title":"Q: 所谓\"权重\"就是指 state_dict 中的内容吗？","children":[{"key":"1-权重是什么","href":"#1-权重是什么","heading":3,"title":"1. \"权重\"是什么？","children":[],"id":"1-权重是什么"},{"key":"2-state_dict-中的内容","href":"#2-state_dict-中的内容","heading":3,"title":"2. state_dict 中的内容","children":[],"id":"2-state_dict-中的内容"},{"key":"3-权重与-state_dict-的关系","href":"#3-权重与-state_dict-的关系","heading":3,"title":"3. \"权重\"与 state_dict 的关系","children":[],"id":"3-权重与-state_dict-的关系"},{"key":"4-为什么需要-state_dict-而不仅仅是权重","href":"#4-为什么需要-state_dict-而不仅仅是权重","heading":3,"title":"4. 为什么需要 state_dict 而不仅仅是权重？","children":[],"id":"4-为什么需要-state_dict-而不仅仅是权重"},{"key":"5-操作-state_dict-的常见场景","href":"#5-操作-state_dict-的常见场景","heading":3,"title":"5. 操作 state_dict 的常见场景","children":[{"key":"1-查看模型权重","href":"#1-查看模型权重","heading":4,"title":"(1) 查看模型权重","children":[],"id":"1-查看模型权重"},{"key":"2-选择性加载部分权重迁移学习","href":"#2-选择性加载部分权重迁移学习","heading":4,"title":"(2) 选择性加载部分权重（迁移学习）","children":[],"id":"2-选择性加载部分权重迁移学习"},{"key":"3-手动修改权重","href":"#3-手动修改权重","heading":4,"title":"(3) 手动修改权重","children":[],"id":"3-手动修改权重"}],"id":"5-操作-state_dict-的常见场景"},{"key":"6-总结-1","href":"#6-总结-1","heading":3,"title":"6. 总结","children":[],"id":"6-总结-1"}],"id":"q-所谓权重就是指-state_dict-中的内容吗"}],"wikiRefAliases":[],"richRefAliases":[]}}]},"__N_SSG":true}