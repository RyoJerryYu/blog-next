<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta property="og:image" content="https://ryojerryyu.github.io/blog-next/img/home-bg-kasumi-hanabi.jpg" data-next-head=""/><meta name="twitter:image" content="https://ryojerryyu.github.io/blog-next/img/home-bg-kasumi-hanabi.jpg" data-next-head=""/><meta property="og:url" content="https://blog.ryo-okami.xyz/articles/try-cursor-and-thinking" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@ryo_okami" data-next-head=""/><meta name="twitter:creator" content="@ryo_okami" data-next-head=""/><link rel="icon" href="/blog-next/favicon.ico" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1" data-next-head=""/><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests" data-next-head=""/><title data-next-head="">尝试 Cursor 的感想和一些思考 | Ryo&#x27;s Blog</title><meta property="og:title" content="尝试 Cursor 的感想和一些思考" data-next-head=""/><meta property="og:site_name" content="Ryo&#x27;s Blog" data-next-head=""/><meta name="twitter:title" content="尝试 Cursor 的感想和一些思考 | Ryo&#x27;s Blog" data-next-head=""/><meta name="description" content="试着用了一下 cursor ，感觉还不错。非技术也能半小时能做一款 App 可能是真的。但替代不了技术岗也是真的。
虽然 Cursor 基于 VSCode 二次开发，但可能为了做 AI 功能把 Editor Pooling 或者 File Watching 能力搞坏了，经常 Apply 了修改后 Explorer 和 Editor 里没有及时反馈。然后自动补全功能因为可以删内容导致手感跟 VSCode 里的 Copilot 比较不同，用起来比较 annoying 。
作为一个基于 VSCode 二次开发的编辑器，Cursor 在文件系统监控方面存在一些问题。当对文件进行修改后，Explorer 和 Editor 经常无法及时反映这些变化，这可能是因为为了实现 AI 功能而对原有的 Editor Pooling 或 File Watching 机制进行了修改导致的。" data-next-head=""/><meta property="og:description" content="试着用了一下 cursor ，感觉还不错。非技术也能半小时能做一款 App 可能是真的。但替代不了技术岗也是真的。
虽然 Cursor 基于 VSCode 二次开发，但可能为了做 AI 功能把 Editor Pooling 或者 File Watching 能力搞坏了，经常 Apply 了修改后 Explorer 和 Editor 里没有及时反馈。然后自动补全功能因为可以删内容导致手感跟 VSCode 里的 Copilot 比较不同，用起来比较 annoying 。
作为一个基于 VSCode 二次开发的编辑器，Cursor 在文件系统监控方面存在一些问题。当对文件进行修改后，Explorer 和 Editor 经常无法及时反映这些变化，这可能是因为为了实现 AI 功能而对原有的 Editor Pooling 或 File Watching 机制进行了修改导致的。" data-next-head=""/><meta name="twitter:description" content="试着用了一下 cursor ，感觉还不错。非技术也能半小时能做一款 App 可能是真的。但替代不了技术岗也是真的。
虽然 Cursor 基于 VSCode 二次开发，但可能为了做 AI 功能把 Editor Pooling 或者 File Watching 能力搞坏了，经常 Apply 了修改后 Explorer 和 Editor 里没有及时反馈。然后自动补全功能因为可以删内容导致手感跟 VSCode 里的 Copilot 比较不同，用起来比较 annoying 。
作为一个基于 VSCode 二次开发的编辑器，Cursor 在文件系统监控方面存在一些问题。当对文件进行修改后，Explorer 和 Editor 经常无法及时反映这些变化，这可能是因为为了实现 AI 功能而对原有的 Editor Pooling 或 File Watching 机制进行了修改导致的。" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="article:published_time" content="2024-11-16T15:42:00.000Z" data-next-head=""/><meta property="article:modified_time" content="2024-11-16T15:42:00.000Z" data-next-head=""/><meta property="article:tag" content="Cursor" data-next-head=""/><meta property="article:tag" content="杂谈" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="192x192" href="/android-chrome-192x192.png"/><link rel="manifest" href="/site.webmanifest"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/blog-next/_next/static/css/8db32621d94754ab.css" as="style"/><link rel="stylesheet" href="/blog-next/_next/static/css/8db32621d94754ab.css" data-n-g=""/><link rel="preload" href="/blog-next/_next/static/css/5bd958681a4509b5.css" as="style"/><link rel="stylesheet" href="/blog-next/_next/static/css/5bd958681a4509b5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/blog-next/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/blog-next/_next/static/chunks/webpack-8f367e2716300a44.js" defer=""></script><script src="/blog-next/_next/static/chunks/framework-898697981b1ca118.js" defer=""></script><script src="/blog-next/_next/static/chunks/main-a0a5fc4e603d1ed2.js" defer=""></script><script src="/blog-next/_next/static/chunks/pages/_app-861c1dc7f33f622c.js" defer=""></script><script src="/blog-next/_next/static/chunks/ee8b1517-1aa9307a3ec34e41.js" defer=""></script><script src="/blog-next/_next/static/chunks/4785-70b0f2db3df81cbd.js" defer=""></script><script src="/blog-next/_next/static/chunks/9965-95126bbb1fbfbef4.js" defer=""></script><script src="/blog-next/_next/static/chunks/39-0a3d8a77fd97d8ce.js" defer=""></script><script src="/blog-next/_next/static/chunks/7171-04c424bd9f009d33.js" defer=""></script><script src="/blog-next/_next/static/chunks/pages/articles/%5Bslug%5D-254a27b2b1020aab.js" defer=""></script><script src="/blog-next/_next/static/YbslwkcmPy29WMud75N6k/_buildManifest.js" defer=""></script><script src="/blog-next/_next/static/YbslwkcmPy29WMud75N6k/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="DefaultLayout_header__aepTD"><div class="DefaultLayout_icon__11sTk"><div class="DefaultLayout_textbox__H9FZG"><a class="DefaultLayout_textlink__EVwys" href="/blog-next">Ryo&#x27;s Blog</a></div></div><div class="DefaultLayout_navBar__gY4ra"><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/articles">Articles</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/ideas">Ideas</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/tags">Tags</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/clips">Clips</a></div></div><div class="DefaultLayout_headerRight__0Kj26"><div class=" flex flex-row gap-8 items-center justify-center"><a title="Twitter" href="https://twitter.com/ryo_okami"><svg class=" h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a title="GitHub" href="https://github.com/RyoJerryYu"><svg class=" h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a title="Pixiv" href="https://www.pixiv.net/users/9159893"><svg class=" h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4.935 0A4.924 4.924 0 0 0 0 4.935v14.13A4.924 4.924 0 0 0 4.935 24h14.13A4.924 4.924 0 0 0 24 19.065V4.935A4.924 4.924 0 0 0 19.065 0zm7.81 4.547c2.181 0 4.058.676 5.399 1.847a6.118 6.118 0 0 1 2.116 4.66c.005 1.854-.88 3.476-2.257 4.563-1.375 1.092-3.225 1.697-5.258 1.697-2.314 0-4.46-.842-4.46-.842v2.718c.397.116 1.048.365.635.779H5.79c-.41-.41.19-.65.644-.779V7.666c-1.053.81-1.593 1.51-1.868 2.031.32 1.02-.284.969-.284.969l-1.09-1.73s3.868-4.39 9.553-4.39zm-.19.971c-1.423-.003-3.184.473-4.27 1.244v8.646c.988.487 2.484.832 4.26.832h.01c1.596 0 2.98-.593 3.93-1.533.952-.948 1.486-2.183 1.492-3.683-.005-1.54-.504-2.864-1.42-3.86-.918-.992-2.274-1.645-4.002-1.646Z"></path></svg></a></div></div></header><div class="DefaultLayout_headerBg__FStmg"></div><div class="max-w-3xl mx-auto p-2"><div class="DefaultLayout_contentHeight__DabjQ"><article class="Post_post__acRqJ"><h1 class="Post_postTitle__N1NIA">尝试 Cursor 的感想和一些思考</h1><div class="Post_postDate__SQx7A"><time dateTime="2024-11-16T15:42:00.000Z">2024-11-16</time></div><div class="TagsBox_tagsBox__WzhAf mt-2"><a href="/blog-next/tags/cursor"><div class="TagsBox_tag__Rk32C">Cursor</div></a><a href="/blog-next/tags/%E6%9D%82%E8%B0%88"><div class="TagsBox_tag__Rk32C">杂谈</div></a></div><div class="post-body Post_postContent__mJ_Ju"><p>试着用了一下 cursor ，感觉还不错。非技术也能半小时能做一款 App 可能是真的。但替代不了技术岗也是真的。</p>
<h2 id="先说问题"><a href="#先说问题">先说问题</a></h2>
<p>虽然 Cursor 基于 VSCode 二次开发，但可能为了做 AI 功能把 Editor Pooling 或者 File Watching 能力搞坏了，经常 Apply 了修改后 Explorer 和 Editor 里没有及时反馈。然后自动补全功能因为可以删内容导致手感跟 VSCode 里的 Copilot 比较不同，用起来比较 annoying 。</p>
<h3 id="文件系统监控问题"><a href="#文件系统监控问题">文件系统监控问题</a></h3>
<p>作为一个基于 VSCode 二次开发的编辑器，Cursor 在文件系统监控方面存在一些问题。当对文件进行修改后，Explorer 和 Editor 经常无法及时反映这些变化，这可能是因为为了实现 AI 功能而对原有的 Editor Pooling 或 File Watching 机制进行了修改导致的。</p>
<blockquote>
<p>这简直太让人抓狂了！😫 你改了代码，编辑器却在那装傻充愣。就好像你发了消息，对方已读不回 💬。程序员最讨厌等待了，对吧？⏳</p>
</blockquote>
<h3 id="自动补全体验差异"><a href="#自动补全体验差异">自动补全体验差异</a></h3>
<p>Cursor 的自动补全功能与 VSCode 中的 GitHub Copilot 有明显的使用体验差异。由于 Cursor 的补全可以删除已有内容，这种行为方式与程序员习惯的编辑模式不太相符，使用起来感觉比较突兀和烦人。</p>
<blockquote>
<p>这个功能真的让人又爱又恨 💔！AI 小助手太热情了，动不动就想帮你重写代码。冷静点，老铁！我只是想要一点提示而已！🤪</p>
</blockquote>
<h3 id="性能影响"><a href="#性能影响">性能影响</a></h3>
<p>这些技术问题不仅影响了开发体验，还可能会降低编码效率。实时的文件系统反馈对于开发工作流程来说是非常重要的，而自动补全功能的差异也会影响到日常编码的流畅度。</p>
<blockquote>
<p>性能问题真是让人头大 🤯！写代码就应该是行云流水的感觉，现在却经常要等等等...等到我都能喝完一杯咖啡了 ☕️！</p>
</blockquote>
<h2 id="chat-anywhere"><a href="#chat-anywhere">Chat Anywhere</a></h2>
<p>不过 Chat Anywhere 这个做法应该是做对了。需要用 AI 代写的场景，很多时候并不是不会写而是懒得写，以前在 VSCode 里需要切到 Coplilot 的 Tab ，写 prompt 等回复，然后再将答案复制粘贴回去，多数情况下有这闲工夫还不如直接自己写🤣，在 Cursor 里可以直接原地调 AI 改写，真的巨舒服。</p>
<blockquote>
<p>这功能简直就是懒人福音啊！🎯 再也不用在各种窗口之间跳来跳去了！爽歪歪！🎊</p>
</blockquote>
<h2 id="智能代码分析"><a href="#智能代码分析">智能代码分析</a></h2>
<p>还有 Cursor 可以直接将整个项目 Indexing 掉，还能理解代码间的调用关系，一个 prompt 直接出调用关系图，再也不用挠爆头想怎么做 RAG 怎么给文章分块了，爽到。</p>
<img src="/blog-next/content/articles/2024-11-16-try-cursor-and-thinking/Pasted%20image%2020241116200220.png" alt="Pasted image 20241116200220.png" title="Pasted image 20241116200220.png"/>
<img src="/blog-next/content/articles/2024-11-16-try-cursor-and-thinking/Pasted%20image%2020241116200148.png" alt="Pasted image 20241116200148.png" title="Pasted image 20241116200148.png"/>
<h2 id="cursor-可能会比较有用的场景"><a href="#cursor-可能会比较有用的场景">Cursor 可能会比较有用的场景</a></h2>
<h3 id="智能注释生成"><a href="#智能注释生成">智能注释生成</a></h3>
<p>Cursor 在生成代码注释方面表现出色。它不仅能分析当前文件的代码，还能理解整个项目的上下文。通过分析 import 关系、函数调用链、接口实现和类型定义等多个维度，它能生成更加准确和有意义的注释。这对于维护大型项目或者需要快速理解他人代码的场景特别有帮助。</p>
<blockquote>
<p>终于不用为写注释抓耳挠腮了！🎉 AI 帮你分析完所有代码关系，三下五除二就能生成一份漂亮的注释！💡</p>
</blockquote>
<h3 id="文档和文章创作"><a href="#文档和文章创作">文档和文章创作</a></h3>
<p>在文档和文章创作方面，Cursor 的原地 AI 改写功能特别实用。当你需要写一篇长文，需要参考多个文档源，但又不需要特别严格的逻辑推导时，这个功能简直是神器。你可以：</p>
<ul>
<li>让 AI 帮你规划文章结构，生成合适的目录</li>
<li>根据已有内容快速扩充段落</li>
<li>实时调整文章语气和风格</li>
<li>参考相关文档自动补充内容</li>
</ul>
<blockquote>
<p>写文档再也不用对着空白发呆了！✍️ AI 小助手随时待命，帮你把想法变成优美的文字！📝</p>
</blockquote>
<h3 id="项目结构优化"><a href="#项目结构优化">项目结构优化</a></h3>
<p>在项目结构维护方面，Cursor 提供了一系列强大的功能：</p>
<ol>
<li>
<p><strong>可视化项目结构</strong>：一键生成项目依赖关系图，让项目结构一目了然。再也不用在复杂的目录结构中迷失方向。</p>
</li>
<li>
<p><strong>智能重构建议</strong>：基于项目分析，AI 可以提供项目结构优化建议，帮助你建立更清晰的代码组织方式。</p>
</li>
<li>
<p><strong>自动化工具生成</strong>：需要批量处理文件？Cursor 可以直接生成 Shell 命令或 Python 脚本，帮你完成繁琐的目录操作。</p>
</li>
</ol>
<blockquote>
<p>项目管理变得如此轻松！🚀 让 AI 帮你梳理项目结构，生成工具脚本，程序员的生产力简直起飞！✨</p>
</blockquote>
<h2 id="总结"><a href="#总结">总结</a></h2>
<blockquote>
<p>总的来说，Cursor 作为一款基于 AI 的代码编辑器有其独特的优势，尤其是 Chat Anywhere 和智能代码分析这样的创新功能确实提升了开发效率。但同时也存在一些技术问题，比如文件系统监控和自动补全体验等需要改进的地方。尽管如此，它展示了 AI 辅助编程的潜力，为未来编程工具的发展提供了新的思路。</p>
</blockquote>
<p>顺便一提，在引用块里的内容都是由 AI 生成的。</p></div><div class="w-96 text-gray-700 leading-none"><span class="!text-sm"><a class="!inline-block !p-0 !m-0 align-text-bottom" rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" loading="lazy" width="88" height="31" decoding="async" data-nimg="1" class="!m-0 h-4 w-auto pr-1" style="color:transparent;border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png"/></a>This work is licensed under a<!-- --> <a class="underline" rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.</span></div><div class="TagsBox_tagsBox__WzhAf mt-4"><a href="/blog-next/tags/cursor"><div class="TagsBox_tag__Rk32C">Cursor</div></a><a href="/blog-next/tags/%E6%9D%82%E8%B0%88"><div class="TagsBox_tag__Rk32C">杂谈</div></a></div><div class="mt-4 mb-4 flex justify-center"><div class="mr-0 ml-auto"><a href="/blog-next/articles/introduction-for-k8s-2">Kubernetes 入门 （2） -&gt;</a></div></div><hr class="mt-4"/></article><div class="w-full"><div></div><div class="text-center">Loading comments...</div></div></div></div><footer class="DefaultLayout_footer__n5339"><div class="max-w-3xl mx-auto p-2 w-full"><div class="flex flex-row justify-center items-center"><div class="DefaultLayout_footerLeft__j0yvY">© 2023 Ryo Jerry Yu. All rights reserved.</div><div class="DefaultLayout_footerRight___Dn67"><div class=" flex flex-row gap-8 items-center justify-center"><a title="Twitter" href="https://twitter.com/ryo_okami"><svg class="DefaultLayout_footerIcon__sgrmB h-8 w-8" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a title="GitHub" href="https://github.com/RyoJerryYu"><svg class="DefaultLayout_footerIcon__sgrmB h-8 w-8" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a title="Pixiv" href="https://www.pixiv.net/users/9159893"><svg class="DefaultLayout_footerIcon__sgrmB h-8 w-8" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4.935 0A4.924 4.924 0 0 0 0 4.935v14.13A4.924 4.924 0 0 0 4.935 24h14.13A4.924 4.924 0 0 0 24 19.065V4.935A4.924 4.924 0 0 0 19.065 0zm7.81 4.547c2.181 0 4.058.676 5.399 1.847a6.118 6.118 0 0 1 2.116 4.66c.005 1.854-.88 3.476-2.257 4.563-1.375 1.092-3.225 1.697-5.258 1.697-2.314 0-4.46-.842-4.46-.842v2.718c.397.116 1.048.365.635.779H5.79c-.41-.41.19-.65.644-.779V7.666c-1.053.81-1.593 1.51-1.868 2.031.32 1.02-.284.969-.284.969l-1.09-1.73s3.868-4.39 9.553-4.39zm-.19.971c-1.423-.003-3.184.473-4.27 1.244v8.646c.988.487 2.484.832 4.26.832h.01c1.596 0 2.98-.593 3.93-1.533.952-.948 1.486-2.183 1.492-3.683-.005-1.54-.504-2.864-1.42-3.86-.918-.992-2.274-1.645-4.002-1.646Z"></path></svg></a></div></div></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"try-cursor-and-thinking","tags":[{"tag":"Cursor","slug":"cursor","path":"/tags/cursor","postSlugs":[{"postType":"article","postSlug":"try-cursor-and-thinking","postPagePath":"/articles/try-cursor-and-thinking"}]},{"tag":"杂谈","slug":"杂谈","path":"/tags/杂谈","postSlugs":[{"postType":"article","postSlug":"hello-world","postPagePath":"/articles/hello-world"},{"postType":"article","postSlug":"try-cursor-and-thinking","postPagePath":"/articles/try-cursor-and-thinking"}]}],"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    h2: \"h2\",\n    h3: \"h3\",\n    img: \"img\",\n    li: \"li\",\n    ol: \"ol\",\n    p: \"p\",\n    strong: \"strong\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"试着用了一下 cursor ，感觉还不错。非技术也能半小时能做一款 App 可能是真的。但替代不了技术岗也是真的。\"\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"先说问题\",\n      children: _jsx(_components.a, {\n        href: \"#先说问题\",\n        children: \"先说问题\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"虽然 Cursor 基于 VSCode 二次开发，但可能为了做 AI 功能把 Editor Pooling 或者 File Watching 能力搞坏了，经常 Apply 了修改后 Explorer 和 Editor 里没有及时反馈。然后自动补全功能因为可以删内容导致手感跟 VSCode 里的 Copilot 比较不同，用起来比较 annoying 。\"\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"文件系统监控问题\",\n      children: _jsx(_components.a, {\n        href: \"#文件系统监控问题\",\n        children: \"文件系统监控问题\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"作为一个基于 VSCode 二次开发的编辑器，Cursor 在文件系统监控方面存在一些问题。当对文件进行修改后，Explorer 和 Editor 经常无法及时反映这些变化，这可能是因为为了实现 AI 功能而对原有的 Editor Pooling 或 File Watching 机制进行了修改导致的。\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"这简直太让人抓狂了！😫 你改了代码，编辑器却在那装傻充愣。就好像你发了消息，对方已读不回 💬。程序员最讨厌等待了，对吧？⏳\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"自动补全体验差异\",\n      children: _jsx(_components.a, {\n        href: \"#自动补全体验差异\",\n        children: \"自动补全体验差异\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Cursor 的自动补全功能与 VSCode 中的 GitHub Copilot 有明显的使用体验差异。由于 Cursor 的补全可以删除已有内容，这种行为方式与程序员习惯的编辑模式不太相符，使用起来感觉比较突兀和烦人。\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"这个功能真的让人又爱又恨 💔！AI 小助手太热情了，动不动就想帮你重写代码。冷静点，老铁！我只是想要一点提示而已！🤪\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"性能影响\",\n      children: _jsx(_components.a, {\n        href: \"#性能影响\",\n        children: \"性能影响\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"这些技术问题不仅影响了开发体验，还可能会降低编码效率。实时的文件系统反馈对于开发工作流程来说是非常重要的，而自动补全功能的差异也会影响到日常编码的流畅度。\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"性能问题真是让人头大 🤯！写代码就应该是行云流水的感觉，现在却经常要等等等...等到我都能喝完一杯咖啡了 ☕️！\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"chat-anywhere\",\n      children: _jsx(_components.a, {\n        href: \"#chat-anywhere\",\n        children: \"Chat Anywhere\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"不过 Chat Anywhere 这个做法应该是做对了。需要用 AI 代写的场景，很多时候并不是不会写而是懒得写，以前在 VSCode 里需要切到 Coplilot 的 Tab ，写 prompt 等回复，然后再将答案复制粘贴回去，多数情况下有这闲工夫还不如直接自己写🤣，在 Cursor 里可以直接原地调 AI 改写，真的巨舒服。\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"这功能简直就是懒人福音啊！🎯 再也不用在各种窗口之间跳来跳去了！爽歪歪！🎊\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"智能代码分析\",\n      children: _jsx(_components.a, {\n        href: \"#智能代码分析\",\n        children: \"智能代码分析\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"还有 Cursor 可以直接将整个项目 Indexing 掉，还能理解代码间的调用关系，一个 prompt 直接出调用关系图，再也不用挠爆头想怎么做 RAG 怎么给文章分块了，爽到。\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog-next/content/articles/2024-11-16-try-cursor-and-thinking/Pasted%20image%2020241116200220.png\",\n      alt: \"Pasted image 20241116200220.png\",\n      title: \"Pasted image 20241116200220.png\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/blog-next/content/articles/2024-11-16-try-cursor-and-thinking/Pasted%20image%2020241116200148.png\",\n      alt: \"Pasted image 20241116200148.png\",\n      title: \"Pasted image 20241116200148.png\"\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"cursor-可能会比较有用的场景\",\n      children: _jsx(_components.a, {\n        href: \"#cursor-可能会比较有用的场景\",\n        children: \"Cursor 可能会比较有用的场景\"\n      })\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"智能注释生成\",\n      children: _jsx(_components.a, {\n        href: \"#智能注释生成\",\n        children: \"智能注释生成\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Cursor 在生成代码注释方面表现出色。它不仅能分析当前文件的代码，还能理解整个项目的上下文。通过分析 import 关系、函数调用链、接口实现和类型定义等多个维度，它能生成更加准确和有意义的注释。这对于维护大型项目或者需要快速理解他人代码的场景特别有帮助。\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"终于不用为写注释抓耳挠腮了！🎉 AI 帮你分析完所有代码关系，三下五除二就能生成一份漂亮的注释！💡\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"文档和文章创作\",\n      children: _jsx(_components.a, {\n        href: \"#文档和文章创作\",\n        children: \"文档和文章创作\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"在文档和文章创作方面，Cursor 的原地 AI 改写功能特别实用。当你需要写一篇长文，需要参考多个文档源，但又不需要特别严格的逻辑推导时，这个功能简直是神器。你可以：\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"让 AI 帮你规划文章结构，生成合适的目录\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"根据已有内容快速扩充段落\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"实时调整文章语气和风格\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"参考相关文档自动补充内容\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"写文档再也不用对着空白发呆了！✍️ AI 小助手随时待命，帮你把想法变成优美的文字！📝\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"项目结构优化\",\n      children: _jsx(_components.a, {\n        href: \"#项目结构优化\",\n        children: \"项目结构优化\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"在项目结构维护方面，Cursor 提供了一系列强大的功能：\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"可视化项目结构\"\n          }), \"：一键生成项目依赖关系图，让项目结构一目了然。再也不用在复杂的目录结构中迷失方向。\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"智能重构建议\"\n          }), \"：基于项目分析，AI 可以提供项目结构优化建议，帮助你建立更清晰的代码组织方式。\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"自动化工具生成\"\n          }), \"：需要批量处理文件？Cursor 可以直接生成 Shell 命令或 Python 脚本，帮你完成繁琐的目录操作。\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"项目管理变得如此轻松！🚀 让 AI 帮你梳理项目结构，生成工具脚本，程序员的生产力简直起飞！✨\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"总结\",\n      children: _jsx(_components.a, {\n        href: \"#总结\",\n        children: \"总结\"\n      })\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"总的来说，Cursor 作为一款基于 AI 的代码编辑器有其独特的优势，尤其是 Chat Anywhere 和智能代码分析这样的创新功能确实提升了开发效率。但同时也存在一些技术问题，比如文件系统监控和自动补全体验等需要改进的地方。尽管如此，它展示了 AI 辅助编程的潜力，为未来编程工具的发展提供了新的思路。\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"顺便一提，在引用块里的内容都是由 AI 生成的。\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}},"meta":{"content":"\n试着用了一下 cursor ，感觉还不错。非技术也能半小时能做一款 App 可能是真的。但替代不了技术岗也是真的。\n\n## 先说问题\n\n虽然 Cursor 基于 VSCode 二次开发，但可能为了做 AI 功能把 Editor Pooling 或者 File Watching 能力搞坏了，经常 Apply 了修改后 Explorer 和 Editor 里没有及时反馈。然后自动补全功能因为可以删内容导致手感跟 VSCode 里的 Copilot 比较不同，用起来比较 annoying 。\n\n### 文件系统监控问题\n\n作为一个基于 VSCode 二次开发的编辑器，Cursor 在文件系统监控方面存在一些问题。当对文件进行修改后，Explorer 和 Editor 经常无法及时反映这些变化，这可能是因为为了实现 AI 功能而对原有的 Editor Pooling 或 File Watching 机制进行了修改导致的。\n\n\u003e 这简直太让人抓狂了！😫 你改了代码，编辑器却在那装傻充愣。就好像你发了消息，对方已读不回 💬。程序员最讨厌等待了，对吧？⏳\n\n### 自动补全体验差异\n\nCursor 的自动补全功能与 VSCode 中的 GitHub Copilot 有明显的使用体验差异。由于 Cursor 的补全可以删除已有内容，这种行为方式与程序员习惯的编辑模式不太相符，使用起来感觉比较突兀和烦人。\n\n\u003e 这个功能真的让人又爱又恨 💔！AI 小助手太热情了，动不动就想帮你重写代码。冷静点，老铁！我只是想要一点提示而已！🤪\n\n### 性能影响\n\n这些技术问题不仅影响了开发体验，还可能会降低编码效率。实时的文件系统反馈对于开发工作流程来说是非常重要的，而自动补全功能的差异也会影响到日常编码的流畅度。\n\n\u003e 性能问题真是让人头大 🤯！写代码就应该是行云流水的感觉，现在却经常要等等等...等到我都能喝完一杯咖啡了 ☕️！\n\n\n## Chat Anywhere\n\n不过 Chat Anywhere 这个做法应该是做对了。需要用 AI 代写的场景，很多时候并不是不会写而是懒得写，以前在 VSCode 里需要切到 Coplilot 的 Tab ，写 prompt 等回复，然后再将答案复制粘贴回去，多数情况下有这闲工夫还不如直接自己写🤣，在 Cursor 里可以直接原地调 AI 改写，真的巨舒服。\n\n\u003e 这功能简直就是懒人福音啊！🎯 再也不用在各种窗口之间跳来跳去了！爽歪歪！🎊\n\n## 智能代码分析\n\n还有 Cursor 可以直接将整个项目 Indexing 掉，还能理解代码间的调用关系，一个 prompt 直接出调用关系图，再也不用挠爆头想怎么做 RAG 怎么给文章分块了，爽到。\n\n![[Pasted image 20241116200220.png]]\n\n![[Pasted image 20241116200148.png]]\n\n## Cursor 可能会比较有用的场景\n\n### 智能注释生成\n\nCursor 在生成代码注释方面表现出色。它不仅能分析当前文件的代码，还能理解整个项目的上下文。通过分析 import 关系、函数调用链、接口实现和类型定义等多个维度，它能生成更加准确和有意义的注释。这对于维护大型项目或者需要快速理解他人代码的场景特别有帮助。\n\n\u003e 终于不用为写注释抓耳挠腮了！🎉 AI 帮你分析完所有代码关系，三下五除二就能生成一份漂亮的注释！💡\n\n### 文档和文章创作\n\n在文档和文章创作方面，Cursor 的原地 AI 改写功能特别实用。当你需要写一篇长文，需要参考多个文档源，但又不需要特别严格的逻辑推导时，这个功能简直是神器。你可以：\n\n- 让 AI 帮你规划文章结构，生成合适的目录\n- 根据已有内容快速扩充段落\n- 实时调整文章语气和风格\n- 参考相关文档自动补充内容\n\n\u003e 写文档再也不用对着空白发呆了！✍️ AI 小助手随时待命，帮你把想法变成优美的文字！📝\n\n### 项目结构优化\n\n在项目结构维护方面，Cursor 提供了一系列强大的功能：\n\n1. **可视化项目结构**：一键生成项目依赖关系图，让项目结构一目了然。再也不用在复杂的目录结构中迷失方向。\n\n2. **智能重构建议**：基于项目分析，AI 可以提供项目结构优化建议，帮助你建立更清晰的代码组织方式。\n\n3. **自动化工具生成**：需要批量处理文件？Cursor 可以直接生成 Shell 命令或 Python 脚本，帮你完成繁琐的目录操作。\n\n\u003e 项目管理变得如此轻松！🚀 让 AI 帮你梳理项目结构，生成工具脚本，程序员的生产力简直起飞！✨\n\n\n## 总结\n\n\u003e 总的来说，Cursor 作为一款基于 AI 的代码编辑器有其独特的优势，尤其是 Chat Anywhere 和智能代码分析这样的创新功能确实提升了开发效率。但同时也存在一些技术问题，比如文件系统监控和自动补全体验等需要改进的地方。尽管如此，它展示了 AI 辅助编程的潜力，为未来编程工具的发展提供了新的思路。\n\n顺便一提，在引用块里的内容都是由 AI 生成的。\n","title":"尝试 Cursor 的感想和一些思考","abstract":"试着用了一下 cursor ，感觉还不错。非技术也能半小时能做一款 App 可能是真的。但替代不了技术岗也是真的。\n虽然 Cursor 基于 VSCode 二次开发，但可能为了做 AI 功能把 Editor Pooling 或者 File Watching 能力搞坏了，经常 Apply 了修改后 Explorer 和 Editor 里没有及时反馈。然后自动补全功能因为可以删内容导致手感跟 VSCode 里的 Copilot 比较不同，用起来比较 annoying 。\n作为一个基于 VSCode 二次开发的编辑器，Cursor 在文件系统监控方面存在一些问题。当对文件进行修改后，Explorer 和 Editor 经常无法及时反映这些变化，这可能是因为为了实现 AI 功能而对原有的 Editor Pooling 或 File Watching 机制进行了修改导致的。","length":78,"created_at":"2024-11-16T15:42:00.000Z","updated_at":"2024-11-16T15:42:00.000Z","tags":["Cursor","杂谈"],"license":true},"prevNextInfo":{"prevInfo":null,"nextInfo":{"pathMapping":{"filePath":"public/content/articles/2022-08-20-introduction-for-k8s-2.md","pagePath":"/articles/introduction-for-k8s-2","slug":"introduction-for-k8s-2"},"meta":{"content":"\n我们之前说的都是用于部署 Pod 的资源，我们接下来介绍与创建 Pod 不相关的资源：储存与网络。\n\n# 储存\n\n其实我们之前已经接触过储存相关的内容了：在讲 Stateful Set 时我们提过 Stateful Set 创建出来的 Pod 都会有相互独立的储存；而讲 Daemon Set 时我们提到 K8s 推荐只在 Daemon Set 的 Pod 中访问宿主机磁盘。但独立的储存具体指什么？除了访问宿主机磁盘以外还有什么其他的储存？\n\n在 Docker 中，我们可以把宿主机磁盘上的一个路径作为一个 Volume 来给容器绑定，或者直接使用 Docker Engine 管理的 Volume 来提供持久化存储或是容器间共享文件。在 K8s 里面也沿用了 Volume 这个概念，可以通过 Mount 绑定到容器内的路径，并通过实现 CSI 的各种引擎来提供更多样的存储。\n\n\u003e CSI: Container Storage Interface ，容器储存接口标准，是 K8s 提出的一种规范。不管是哪种储存引擎，只要编写一个对应的插件实现 CSI ，都可以在 K8s 中使用。\n\n### K8s 中使用 Volume 与可用的 Volume 类型\n\n其实 K8s 中使用 Volume 的例子我们一开始就已经接触过了。还记得一开始介绍 Pod 时的 Nginx 例子吗？\n\n```yaml\nmetadata:\n  name: simple-webapp\nspec:\n  containers:\n    - name: main-application\n      image: nginx\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n    - name: sidecar-container\n      image: busybox\n      command: [\"sh\",\"-c\",\"while true; do cat /var/log/nginx/access.log; sleep 30; done\"]\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n  volumes:\n    - name: shared-logs\n      emptyDir: {}\n```\n\n这个 Pod 描述中声明了一个种类为 `emptyDir` 的，名为 `shared-logs` 的 Volume ，然后 Pod 中的两个容器都分别 Mount 了这个 Volume 。\n\nK8s 中默认提供了几种 Volume ，比如：\n\n- emptyDir ：一个简单的空目录，一般用于储存临时数据或是 Pod 的容器之间共享数据。\n- hostPath ：绑定到节点宿主机文件系统上的路径，一般在 Daemon Set 中使用。\n- gitRepo ：这种 Volume 其实相当于 emptyDir ，不过在 Pod 启动时会从 Git 仓库 clone 一份内容作为默认数据。\n- configMap 、 secret ：一般用于配置文件加载，需要与 configMap 、 secret 这两种资源一同使用。会将 configMap 、 secret 中对应的内容拷贝一份作为 Volume 绑到容器。（下一节中会展开讨论）\n- nfs 、 glusterfs 、 ……：可以通过各种网络存储协议直接挂载一个网络存储\n- (deprecated!) gcePersistentDisk 、 awsElasticBlockStore ……：可以调用各个云平台的 API ，创建一个块储存硬件挂载到宿主机上，再将那个硬件挂载到容器中。\n- persistentVolumeClaim ：持久卷声明，用于把实际储存方式抽象化，使得 Pod 不需要关心具体的储存类型。这种类型会在下面详细介绍。\n\n我们可以注意到， Volume 的声明是 Pod 的一个属性，而不是一种单独的资源。 Volume 是 Pod 的一部分，因此不同的 Pod 之间永远不可能共享同一个 Volume 。\n\n\u003e 但是 Volume 所指向的位置可以相同，比如 HostPath 类型的 Volume 就可以两个 Pod 可以绑定到宿主机上同一个路径，因此 Volume 里的数据还是能通过一定方式在 Pod 间共享。但当然 K8s 不推荐这么做。\n\n另外，由于 Volume 是 Pod 的一部分， Volume 的生命周期也是跟随 Pod 的，当一个 Pod 被销毁时， Volume 也会被销毁，因此最主要还是用于 Pod 内容器间的文件共享。如果需要持久化储存，需要使用 Persistent Volume 。\n\n\u003e Volume 会被销毁不代表 Volume 指向的内容会被销毁。比如 hostPath 、 NFS 等类型 Volume 中的内容就会继续保留在宿主机或是 NAS 上。下面提到的 Presistent Volume Claim 也是，拥有 `persistentVolumeClaim` 类型 Volume 的 Pod 被删除后对应的 PVC 不一定会被删除。\n\n### Presistent Volume 、 Presistent Volume Claim 、 Storage Class\n\n如果需要在 Pod 声明中直接指定 NFS 、 awsElasticBlockStore 之类的信息，就需要应用的开发人员对真实可用的储存结构有所理解，违背了 K8s 的理念。因此 K8s 就弄出了小标题中的三种资源来将储存抽象化。\n\n一个 Persistent Volume (PV) 对应云平台提供的一个块存储，或是 NAS 上的一个路径。可以简单地理解为 **PV 直接描述了一块可用的物理存储** 。因为 PV 直接对应到硬件，因此 PV 跟节点一样，是名称空间无关的。\n\n而一个 **Persistent Volume Claim (PVC) 则是描述了怎样去使用储存** ：使用多少空间、只读还是读写等。一个 PVC 被创建后会且只会对应到一个 PV 。 PVC 从属于一个名称空间，并能被该名称空间下的 Pod 指定为一个 Volume 。\n\nPV 与 PVC 这两种抽象是很必要的。试想一下用自己的物理机搭建一个 K8s 集群的场景。你会提前给物理机插上许多个储存硬件，这时你就需要用 PV 来描述这些硬件，之后才能在 K8s 里利用这些硬件的储存。而实际将应用部署到 K8s 中时，你才需要用 PVC 来描述 Pod 中需要怎么样的储存卷，然后 K8s 就会自动挑一个合适 PV 给这个 PVC 绑定上。这样实际部署应用的时候就不用再特意跑去机房给物理机插硬件了。\n\n但是现在都云原生时代了，各供应商都有提供 API 可以直接创建一个块储存，还要想办法提前准备 PV 实在是太蠢了。于是便需要 Storage Class 这种资源。\n\n使用 Storage Class 前需要先安装各种云供应商提供的插件（当然使用云服务提供的 K8s 的话一般已经准备好了），然后再创建一个 Storage Class 类型的资源（当然一般也已经准备好了）。下面是 AWS 上的 EKS 服务中默认自带的 Storage Class ：\n\n```yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\n  name: gp2\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  fsType: ext4\n  type: gp2\n# 当 PVC 被删除时会同时删除 PV\nreclaimPolicy: Delete\n# 只有当 PVC 被绑定为一个 Pod 的 Volume 时才会创建一个 PV\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n可以看到 EKS 自带的 gp2 提供了一些默认的选项，我们也可以类似地去定义自己的 Storage Class 。有了 gp2 这个 Storage Class ，我们创建一个 PVC 后 K8s 就会调用 AWS 的 API ，创建一个块储存接到我们的节点上，然后 K8s 再自动创建一个 PV 并绑定到 PVC 上。\n\n例如，我们部署 Kafka 时会创建一个这样的 PVC ：\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-kafka-0\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: gp2\n```\n\nK8s 就会自动为我们创建出一个对应的 PV ：\n\n```sh\n# `pvc-` 开头这个是 AWS 自动给我们起的名字。它虽然是 `pvc-` 开头，但他其实是一个 PV 。\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                STORAGECLASS   REASON   AGE\npvc-3614c15f-5697-4d66-a13c-6ddf7eb89998   10Gi       RWO            Delete           Bound    kafka/data-kafka-0   gp2                     152d\n```\n\n要是打开 AWS Console 还会发现， K8s 调用了 AWS 的 API ，自动为我们创建了一个 EBS 块储存并绑定到了我们对应的宿主机上。\n\n可以用下面这张图来表示 Pod 中的 Volume 、 PVC 、 PV 之间的关系：\n\n```mermaid\nflowchart TD\n\nsubgraph Pod[Pod: Kafka-0]\nsubgraph Container[Container: docker.io/bitnami/kafka:3.1.0]\nvm[VolumeMount: /bitnami/kafka]\nend\nvolume[(Volume: data)]\nvm --\u003e volume\nend\n\npvc[pvc: data-kafka-0]\npv[pv: pvc-3614c15f-5697-4d66-a13c-6ddf7eb89998]\nebs[ebs: AWS 为我们创建的块储存硬件]\n\nvolume --\u003e pvc\npvc --\u003e pv\npv --\u003e ebs\n```\n\n而 Storage Class 在上图中则负责读取我们提交的 PVC ，然后创建 PV 与 EBS 。\n\n### 再说回 Stateful Set\n\n之前我们提到 Stateful Set 时说到 Stateful Set 创建的 Pod 拥有固定的储存，到底是什么意思呢？跟 Deployment 的储存又有什么区别呢？\n\n我们先来看看，如果要给 Deployment 创建出来的 Pod 挂载 PVC 需要怎么做。下面是一个部署 Nginx 的 Deployment 清单，其中 html 目录下的静态文件存放在 NFS 里，通过 PVC 挂载到 Pod 中：\n\n```yaml\n# 这里省略了 Service 相关的内容\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-dpl-with-nfs-pvc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts: #挂载容器中的目录到 pvc nfs 中的目录\n        - name: www\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: www\n        persistentVolumeClaim: #指定pvc\n          claimName: nfs-pvc-for-nginx\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc-for-nginx\n  namespace: default\nspec:\n  storageclassname: \"\" # 指定使用现有 PV ，不使用 StorageClass 创建 PV\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\n# 这个例子中需要挂载 NFS 上的特定路径，所以手动定义了一个 PV\n# 一般情况下我们不会手动创建 PV，而是使用 StorageClass 自动创建\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv-for-nginx\nspec:\n  capacity: \n    storage: 1Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /nfs/sharefolder/nginx\n    server: 81.70.4.171\n```\n\n这份清单我们主要关注前两个资源，我们可以看到除了一个 Deployment 资源以外我们还单独定义了一个 PVC 资源。然后在 Deployment 的 Pod 模板中声明并绑定了这个 PVC 。\n\n可这样 apply 了之后会发生什么情况呢？因为我们只声明了一份 PVC ，当然我们只会拥有一个 PVC 资源。但这个 Deployment 的副本数是 3 ，因此我们会有 3 个相同的 Pod 去绑定同一个 PVC 。也就是最终会在 3 个容器里访问同一个 NFS 的同一个目录。如果我们在其中一个容器里对这个目录作修改，也会影响到另外两个容器。\n\n\u003e 注：这一现象不一定在任何情况下都适用。比如 AWS 的 EBS 卷只支持单个 AZ 内的绑定。如果 Pod 因为 Node Affinity 等设定被部署到了多个区，没法绑定同一个 EBS 卷，就会在 Scedule 的阶段报错。\n\n很多时候我们都不希望多个 Pod 绑定到同一 PVC 。比如我们部署一个 DB 集群的时候，如果好不容易部署出来的多个实例居然用的是同一份储存，就会显得很呆。 Stateful Set 就是为了解决这种情况，会为其管理下的每个 Pod 都部署一个专用的 PVC 。\n\n下面是给 Stateful Set 创建出来的 Pod 挂载 PVC 的一份清单：\n\n```yaml\n# 这里省略了 Service 相关的内容\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"nginx\"\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n```\n\n我们可以看到，部署 Stateful Set 时我们不能另外单独定义一份 PVC 了，只能作为 Stateful Set 定义的一部分，在 volumeClaimTemplates 字段中定义 PVC 的模板。这样一来， Stateful Set 会根据这个模板，为每个 Pod 创建一个对应的 PVC ，并作为 Pod 的 Volume 绑定上：\n\n```bash\n# Stateful Set 创建出来的 Pod ，名字都是按顺序的\n$ kubectl get pods -l app=nginx\nNAME      READY     STATUS    RESTARTS   AGE\nweb-0     1/1       Running   0          1m\nweb-1     1/1       Running   0          1m\n\n# Stateful Set 创建出来的 PVC ，名字与 Pod 的名字一一对应\n$ kubectl get pvc -l app=nginx\nNAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nwww-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s\nwww-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s\n```\n\n这样， Stateful Set 的多个 Pod 就会拥有自己的储存，不会相互打架了。另外，如果我们事先定义了 StorageClass ，还能根据 Stateful Set 的副本数动态配置 PV 。\n\n### ConfigMap 与 Secret 挂载作为特殊的卷\n\n有时候我们需要使用配置文件来配置应用（比如 Nginx 的配置文件），而且我们有时候会需要不重启 Pod 就热更新配置。如果用 PVC 来加载配置文件略微麻烦，这时候可以使用 Config Map 。\n\n下面是 K8s 官网上 Config Map 的一个例子：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-demo\ndata:\n  # 一个 Key 可以对应一个值\n  player_initial_lives: \"3\"\n  ui_properties_file_name: \"user-interface.properties\"\n\n  # 一个 Key 也可以对应一个文件的内容\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5    \n  user-interface.properties: |\n    color.good=purple\n    color.bad=yellow\n    allow.textmode=true    \n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-demo-pod\nspec:\n  containers:\n    - name: demo\n      image: alpine\n      command: [\"sleep\", \"3600\"]\n      env:\n        # ConfigMap 的 Key 可以作为环境变量引用\n        - name: PLAYER_INITIAL_LIVES\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo           # 从这个 Config Map 里\n              key: player_initial_lives # 拿到这个 key 的值\n        - name: UI_PROPERTIES_FILE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo\n              key: ui_properties_file_name\n      volumeMounts:\n      - name: config\n        mountPath: \"/config\"\n        readOnly: true\n  volumes:\n    # 定义 Pod 的 Volume ，种类为 configMap\n    - name: config\n      configMap:\n        name: game-demo # ConfigMap的名字\n        # 需要作为文件放入 Volume 的 Key\n        items:\n        - key: \"game.properties\"\n          path: \"game.properties\"\n        - key: \"user-interface.properties\"\n          path: \"user-interface.properties\"\n```\n\n我们可以看到 ConfigMap 里的 Key 可以作为文件或是环境变量加载到 Pod 中。另外，作为环境变量加载后其实还能作为命令行参数传入应用，实现各种配置方式。如果修改 Config map 的内容，也可以自动更新 Pod 中的文件。\n\n然而， Config Map 的热更新有一些不太灵活的地方：\n\n1. 作为环境变量加载的 Config Map 数据不会被热更新。想要更新这一部分数据需要重启 Pod。（当然，命令行参数也不能热更新）\n2. 由于 Kubelet 会先将 Config Map 内容加载到本地作为缓存，因此修改 Config Map 后新的内容不会第一时间加载到 Pod 中。而且在旧版本的 K8s 中， Config Map 被更新直到缓存被刷新的时间间隔还会很长，新版本的 K8s 这一部分有了优化，可以设定刷新时间，但会导致 API Server 的负担加重。（这其实是一个 Known Issue ，被诟病多年： https://github.com/kubernetes/kubernetes/issues/22368 ）\n\n除 Config Map 以外， K8s 还提供了一种叫 Secret 的资源，用法和 Config Map 几乎一样。对比 Config Map ，Secret 有以下几个特点：\n\n1. 在 Pod 里， Secret 只会被加载到内存中，而永远不会被写到磁盘上。\n2. 用 `kubectl get` 之类的命令显示的 Secret 内容会被用 base64 编码。（不过， well ，众所周知 base64 可不算是什么加密）\n3. 可以通过 K8s 的 Service Account 等 RBAC 相关的资源来控制 Secret 的访问权限。\n\n不过，由于 Secret 也是以明文的形式被存储在 K8s 的主节点中的，因此需要保证 K8s 主节点的安全。\n\n\u003e **Downward API 挂载作为特殊的卷**\n\u003e \n\u003e 还有另外一种叫 Downward API 的东西，可以作为 Volume 或是环境变量被加载到 Pod 中。有一些参数我们很难事先在 Manifest 中定义（ e.g. Deployment 生成的 Pod 的名字），因此可以通过 Downward API 来实现。\n\u003e \n\u003e ```yaml\n\u003e apiVersion: v1\n\u003e kind: Pod\n\u003e metadata:\n\u003e     name: test-volume-pod\n\u003e     namespace: kube-system\n\u003e     labels:\n\u003e         k8s-app: test-volume\n\u003e         node-env: test\n\u003e spec:\n\u003e     containers:\n\u003e     - name: test-volume-pod-container\n\u003e       image: busybox:latest\n\u003e       env:\n\u003e       - name: POD_NAME # 将 Pod 的名字作为环境变量 POD_NAME 加载到 Pod 中\n\u003e         valueFrom:\n\u003e           fieldRef:\n\u003e             fieldPath: metadata.name\n\u003e       command: [\"sh\", \"-c\"]\n\u003e       args:\n\u003e       - while true; do\n\u003e           cat /etc/podinfo/labels | echo;\n\u003e           env | sort | echo;\n\u003e           sleep 3600;\n\u003e         done;\n\u003e       volumeMounts:\n\u003e       - name: podinfo\n\u003e         mountPath: /etc/podinfo\n\u003e     volumes:\n\u003e     - name: podinfo\n\u003e       downwardAPI: # Downward API 类型的卷\n\u003e         items:\n\u003e         - path: \"labels\" # 将 Pod 的标签作为  labels 文件挂载到 Pod 中\n\u003e           fieldRef:\n\u003e             fieldPath: metadata.labels\n\u003e ```\n\n\n\n# 网络\n\n其实 Pod 只要部署好了，就会被分配到一个集群内部的 IP 地址，流量就可以通过 IP 地址来访问 Pod 了。然而通过可能会有很大问题： **Pod 随时会被杀死。** 虽然通过用 Deployment 等资源可以在挂掉后重新创建一个 Pod ，但那毕竟是不同的 Pod ， IP 已经改变。\n\n另外， Deployment 等资源的就是为了能更方便的做到多副本部署及任意缩容扩容而存在的。如果在 K8s 中访问 Pod 还需要小心翼翼地去找到 Pod 的 IP 地址，或是去寻找 Pod 是否部署了新副本， Deployment 等资源就几乎没有存在价值了。\n\n\u003e 其实 Pod 部署好后不止会被分配 IP 地址，还会被分配到一个类似 `\u003cpod-ip\u003e.\u003cnamespace\u003e.pod.cluster.local` 的 DNS 记录。例如一个位于 default 名字空间，IP 地址为 172.17.0.3 的 Pod ，对应 DNS 记录为 `172-17-0-3.default.pod.cluster.local` 。\n\n### Service\n\n在古代，人们是通过注册中心、服务发现、负载均衡等中间件来解决上面这些问题的，但这样很不云原生。于是 K8s 引入了 Service 这种资源，来实现简易的服务发现、 DNS 功能。\n\n下面是一个经典的例子，部署了一个 Service 和一个 Deployment：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth-service\n  labels:\n    app: auth\nspec:\n  type: ClusterIP\n  selector:\n    app: auth # 指向 Deployment 创建的 Pod\n  ports:\n  - port: 80 # Service 暴露的端口\n    targetPort: 8080 # Pod 的端口\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name:  auth\n  labels:\n    app: auth\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: auth\n  template:\n    metadata:\n      name: auth\n      labels:\n        app: auth\n    spec:\n      containers:\n      - name: auth\n        image: xxxxx.dkr.ecr.ap-northeast-1.amazonaws.com/auth:xxxxx\n        ports:\n        - containerPort: 8080\n```\n\n根据前面的知识我们知道，这份文件会部署 Deployment 会创建 2 个相同的 Pod 副本。另外还会部署一个名为 auth-service 的 Service 资源。这个 Service 暴露了一个 80 端口，并且指向那两个 Pod 的 8080 端口。\n\n而这份文件部署后， Service 资源就会在集群中注册一个 DNS A 记录（或 AAAA 记录），集群内其他 Pod （为了辨别我们叫它 Client ）就可以通过相同的 DNS 名称来访问 Deployment 部署的这 2 个 Pod ：\n\n```sh\ncurl http://auth-service.\u003cnamespace\u003e.svc.cluster.local:80\n# 或者省略掉后面的一大串\ncurl http://auth-service.\u003cnamespace\u003e:80\n# 如果 Client 和 Service 在同一个 Namespace 中，还可以：\ncurl http://auth-service:80\n```\n\n像这样 Client 通过 Service 来访问时，会随机访问到其中一个 Pod ，这样一来无论 Deployment 到底创建了多少个副本，只要副本的标签相同，就能通过同一个 DNS 名称来访问，还能自动实现一些简单的负载均衡。\n\n\u003e **为什么 DNS 名称可以简化？**\n\u003e \n\u003e Pod 被部署时， kubelet 会为每个 Pod 注入一个类似如下的 `/etc/resolv.conf` 文件：\n\u003e \n\u003e ```\n\u003e nameserver 10.32.0.10\n\u003e search \u003cnamespace\u003e.svc.cluster.local svc.cluster.local cluster.local\n\u003e options ndots:5\n\u003e ```\n\u003e \n\u003e Pod 中进行 DNS 查询时，默认会先读取这个文件，然后按照 `search` 选项中的内容展开 DNS 。例如，在 test 名称空间中的 Pod ，访问 data 时的查询可能被展开为 data.test.svc.cluster.local 。\n\u003e 更多关于 `/etc/resolv.conf` 文件的内容可参考 https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html\n\n### Service 的种类\n\n我们上面的例子中，可以看到 Service 资源有个字段 `type:ClusterIP` 。其实 Service 资源有以下几个种类：\n\n| 种类           | 作用                                                                                                                                                                                                                                                                                             |\n| :------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `ClusterIP`    | 这个类型的 Service 会在集群内创建一条 DNS A 记录并通过一定方法将流量代理到其指向的 Pod 上。这种 Service 不会暴露到集群外。这是最基础的 Service 种类。                                                                                                                                            |\n| `NodePort`     | 这种 Service 会在 ClusterIP 的基础上，在所有节点上各暴露一个端口，并把端口的流量也代理到指向的 Pod 上。可以通过这种方法从集群外访问集群内的资源。                                                                                                                                                |\n| `LoadBalancer` | 这种 Service 会在 ClusterIP 的基础上，在所有节点上各暴露一个端口，并在集群外创建一个负载均衡器来将外部流量路由到暴露的端口，再把流量代理到指向的 Pod 上。这种 Service 一般需要调用云服务提供的 API 或是额外安装的插件。如果什么插件都没安装的话，这种 Service 部署后会与 `NodePort` 的表现一样。 |\n| `ExternalName` | 这种 Service 不需要 selector 字段指定后端，而是用 externalName 字段指定一个外部 DNS 记录，然后将流量全部指向外部服务。如果打算将集群内的服务迁移到集群外、或是集群外迁移到集群内，这种类型的 Service 可以实现无缝迁移。                                                                          |\n\n### 虚拟 IP 与 Headless Service\n\n如果你在集群内尝试对 Service 对应的 DNS 记录进行域名解析，会发现返回来的 IP 地址与 Service 指向的任何一个 Pod 对应的 IP 地址都不相同。如果你还尝试了去 Ping 这个 IP 地址，会发现不能 Ping 通。为什么会这样呢？\n\n原来，每个 Service 被部署后， K8s 都会给他分配一个集群内部的 IP 地址，也就是 Cluster IP （这也是最基础的 Service 种类会起名叫 Cluster IP 的原因）。\n\n但是这个 Cluster IP 不会绑定任何的网卡，是一个虚拟 IP 。然后 K8s 中有一个叫 kube-proxy 的组件（这里叫他做组件，是因为 kube-proxy 与 Service 、 Deployment 等不一样，不是一种资源而是 K8s 的一部分）， kube-proxy 通过修改 iptables ，将虚拟 IP 的流量经过一定的负载均衡规则后代理到 Pod 上。\n\n![K8s 官网上的虚拟 IP 图](https://d33wubrfki0l68.cloudfront.net/27b2978647a8d7bdc2a96b213f0c0d3242ef9ce0/e8c9b/images/docs/services-iptables-overview.svg)\n\n\u003e **为什么不使用 DNS 轮询？**\n\u003e \n\u003e 为什么 K8s 不配置多条 DNS A 记录，然后通过轮询名称来解析？为什么需要搞出虚拟 IP 这么复杂的东西？这个问题 K8s 官网上也有特别提到原因：\n\u003e \n\u003e - DNS 实现的历史由来已久，它不遵守记录 TTL，并且在名称查找结果到期后对其进行缓存。\n\u003e - 有些应用程序仅执行一次 DNS 查找，并无限期地缓存结果。\n\u003e - 即使应用和库进行了适当的重新解析，DNS 记录上的 TTL 值低或为零也可能会给 DNS 带来高负载，从而使管理变得困难。\n\n有些时候（比如想使用自己的服务发现机制或是自己的负载均衡机制时）我们确实也会想越过虚拟 IP ，直接获取背后 Pod 的 IP 地址。这时候我们可以将 Service 的 `spec.clusterIP` 字段指定为 `None` ，这样 K8s 就不会给这个 Service 分配一个 Cluster IP 。这样的 Service 被称为 **Headless Service** 。\n\nHeadless Service 资源会创建一组 A 记录直接指向背后的 Pod ，可以通过 DNS 轮询等方式直接获得其中一个 Pod 的 IP 地址。另外更重要的一点， Headless Service 还会创建一组 SRV 记录，包含了指向各个 Pod 的 DNS 记录，可以通过 SRV 记录来发现所有 Pod 。\n\n我们可以在集群里用 nsloopup 或 dig 命令去验证一下：\n\n```sh\n# 在集群的 Pod 内部运行\n$ nslookup kafka-headless.kafka.svc.cluster.local\nServer:     10.96.0.10\nAddress:    10.96.0.10#53\n\nName:   kafka-headless.kafka.svc.cluster.local\nAddress: 172.17.0.6\nName:   kafka-headless.kafka.svc.cluster.local\nAddress: 172.17.0.5\nName:   kafka-headless.kafka.svc.cluster.local\nAddress: 172.17.0.4\n\n$ dig SRV kafka-headless.kafka.svc.cluster.local\n# .....\n;; ANSWER SECTION:\nkafka-headless.kafka.svc.cluster.local.      30      IN      SRV     0 20 9092 kafka-0.kafka-headless.kafka.svc.cluster.local.\nkafka-headless.kafka.svc.cluster.local.      30      IN      SRV     0 20 9092 kafka-1.kafka-headless.kafka.svc.cluster.local.\nkakfa-headless.kafka.svc.cluster.local.      30      IN      SRV     0 20 9092 kafka-2.kafka-headless.kafka.svc.cluster.local.\n\n;; ADDITIONAL SECTION:\nkafka-0.kafka-headless.kafka.svc.cluster.local. 30 IN A  172.17.0.6\nkafka-1.kafka-headless.kafka.svc.cluster.local. 30 IN A  172.17.0.5\nkafka-2.kafka-headless.kafka.svc.cluster.local. 30 IN A  172.17.0.4\n```\n\n\u003e 拥有 Cluster IP 的 Service 其实也有 SRV 记录。但这种情况的 SRV 记录中对应的 Target 仍为 Service 自己的 FQDN 。\n\n### 第三次回到 Stateful Set\n\n在上面 Headless Service 的例子中，我们看到，各个 Pod 对应的 DNS A 记录格式为 `\u003cpod_name\u003e.\u003csvc_name\u003e.\u003cnamespace\u003e.svc.cluster.local` 。不对啊，之前的小知识里不是说过 Pod 被分配的 DNS A 记录格式应该是 `172-17-0-3.default.pod.cluster.local` 的吗？\n\n其实 Headless Service 还有一个众所周知的隐藏功能。 Pod 这种资源本身的参数中有 `subdomain` 字段和 `hostname` 字段，如果设置了这两个字段，这个 Pod 就拥有了形如 `\u003chostname\u003e.\u003csubdomain\u003e.\u003cnamespace\u003e.svc.cluster.local` 的 FQDN （全限定域名）。如果这时刚好在同一名称空间下有与 `subdomain` 同名的 Headless Service ， DNS 就会用为这个 Pod 用它的 FQDN 来创建一条 DNS A 记录。\n\n比如 Pod1 在 `kafka` 名称空间中， `hostname` 为 `kafka-1` ， `subdomain` 为 `kafka-headless` ，那么 Pod1 的 FQDN 就是 `kafka-1.kafka-headless.kakfa.svc.cluster.local` 。而同样在 `kafka` 名称空间中，刚好又有一个 `kafka-headless` 的 Headless Service ，那么 DNS 就会创建一条 A 记录，就可以通过 `kafka-1.kafka-headless.kafka.svc.cluster.local` 来访问 Pod1 了。当然，由于 DNS 展开，也可以用 `kafka-1.kafka-headless.kafka` 甚至是 `kafka-1.kafka-headless` 来访问这个 Pod 。\n\n其实这些 Pod 是用 Stateful Set 来部署的，这一部分其实是 Stateful Set 相关的功能。之前我们说到 Stateful Set 有唯一稳定的网络标识。我们现在就来详细讲讲，这“唯一稳定的网络标识”到底是在指什么。\n\n我们来看一下这个 kafka Stateful Set 到底是怎么部署的：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-headless\nspec:\n  clusterIP: None # 这是一个 headless service\n  ports:\n  - name: tcp-client\n    port: 9092\n    protocol: TCP\n    targetPort: kafka-client\n  selector:\n    select-label: kafka-label\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\nspec:\n  replicas: 3\n  serviceName: kafka-headless # 注意到这里有 serviceName 字段\n  selector:\n    matchLabels:\n      select-label: kafka-label\n  template:\n    metadata:\n      labels:\n        select-label: kafka-label\n    spec:\n      containers:\n      - name: kafka\n        image: docker.io/bitnami/kafka:3.1.0-debian-10-r52\n        # 接下来 Pod 相关部分省略\n  # 下面 Volume 相关部分也省略\n```\n\n我们看到， Stateful Set 的定义中必须要用 `spec.serviceName` 字段指定一个 Headless Service 。 Stateful Set 创建 Pod 时，会自动给 Pod 指定 `hostname` 和 `subdomain` 字段。这样一来，每个 Pod 才有了唯一固定的 hostname ，唯一固定的 FQDN ，以及通过与 Headless Service 共同部署而获得唯一固定的 A 记录。（此外，其实当 Pod 因为版本升级等原因被重新创建时，相同序号的 Pod 还会被分配到相同固定的集群内 IP 。）\n\n\u003e **关于 Stateful Set 中 `serviceName` 字段的争议**\n\u003e \n\u003e Stateful Set 中的 serviceName 字段是必填字段。这个字段唯一的作用其实就是给 Pod 指定 subdomain 。其实这样会有一些问题：\n\u003e \n\u003e 1. Stateful Set 部署时不会检查是否真的存在这么一个 Headless Service 。如果 serviceName 乱填一个值，会导致虽然 Pod 的 `hostname` 和 `subdomain` 都指定了却没有创建 A 记录的情况。\n\u003e 2. 有时 Stateful Set 的 Pod 不需要接收流量，也不需要相互发现，这时候还强行需要指定一个 serviceName 显得有点多余。\n\u003e \n\u003e 在 GitHub 上有关于这个问题的 Issue ： https://github.com/kubernetes/kubernetes/issues/69608\n\n### 从集群外部访问\n\n在 K8s 集群里把应用部署好了，可是如何让集群外部的客户端访问我们集群中的应用呢？这可能是大家最关心的问题。\n\n不过有认真听的同学估计已经有这个问题的答案了。之前我们讲过 NodePort 和 LoadBalancer 这两种 Service 类型。\n\n其中 NodePort Service 只是简单地在节点机器上各开一个端口，而如何路由、如何负载均衡等则一概不管。\n\n而 LoadBalancer Service 则是在 NodePort 的基础上再加一个一个负载均衡器，然后把节点暴露的端口注册到这个负载均衡器上。这样一来，集群外部的客户端就可以通过同一个 IP 来访问集群中的应用。但是要使用 LoadBalancer Service ，一般需要先安装云供应商提供的 Controller ，或是安装其他第三方的 Controller （比如 Nginx Controller ）。\n\n在 Service 之外还另有一种资源类型叫 Ingress ，也可以用来实现集群外部访问集群内部应用的功能。 Ingress 其实也会在集群外创建一个负载均衡器，因此也需要预先安装云供应商的 Controller 。但 Ingress 与 Service 不同的是，它还会管理一定的路由逻辑，接收流量后可以根据路由来分配给不同的 Service 。\n\n| 类型                 | OSI 模型工作层数 | 依赖于云平台或其他插件 |\n| :------------------- | :--------------- | :--------------------- |\n| NodePort Service     | 第四层           | 否                     |\n| LoadBalancer Service | 第四层           | 是                     |\n| Ingress              | 第七层           | 是                     |\n\n特别再详细说一下 Ingress 这种资源。 Ingress 本身不会在集群内的 DNS 上创建记录，一般也不会主动去路由集群内的流量（除非你在集群内强行访问 Ingress 的负载均衡器…… 不过一般也没什么理由要这样做对吧）。但 Ingress 可以根据 HTTP 的 hostname 和 path 来路由流量，把流量分发到不同的 Service 上。 Ingress 也是 K8s 的原生资源里唯一能看到 OSI 第七层的资源。\n\n下面是 AWS 的 EKS 服务中部署的一个 Ingress 的例子（集群中已安装 AWS Load Balancer Controller ）：\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/backend-protocol-version: GRPC\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/healthcheck-path: /grpc.health.v1.Health/Check\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/success-codes: 0,12\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:xxxxxxxxxx:certificate/xxxxxxxxxx\n\n    external-dns.alpha.kubernetes.io/hostname: sample.example.com\n  \n  name: gateway-ingress\nspec:\n  rules:\n  - host: sample.example.com\n    http:\n      paths:\n      - path: /grpc.health.v1.Health\n        pathType: Prefix\n        backend:\n          service:\n            name: health-service\n            port:\n              number: 50051\n      - path: /proto.sample.v1.Sample\n        pathType: Prefix\n        backend:\n          service:\n            name: sample-service\n            port:\n              number: 50051\n```\n\n可以看到， Ingress 资源可以通过 `spec.rules` 字段中定义各条规则，通过 hostname 或是 path 等第七层的信息来进行路由。 Ingress 部署下去后， AWS Load Balancer Controller 会读取会根据的配置，并在云上创建一个 AWS Application Load Balancer （ALB），而 `spec.rules` 会应用到 ALB 上，由 ALB 来负责流量的路由。\n\n我们也会注意到，怎么 `metadata.annotations` 里有这么多奇奇怪怪的字段！ Ingress 本身的功能都是 AWS Load Balancer Controller 调用 AWS 的 API 创建 ALB 来实现的。但 AWS 的 ALB 能实现的功能可不止 Ingress 字段定义的这些，比如安装 TLS 证书、 health check 等 spec 字段中描述不下的功能，就只能是通过 annotation 的形式来定义了。\n\n\u003e 小彩蛋：可以看到例子中的 Ingress 资源 annotation 字段里还有一行 `external-dns.alpha.kubernetes.io/hostname: sample.example.com` 。其实这个 K8s 集群中还安装了 external-dns 这个应用，它可以根据 annotation 来在外部 DNS 上直接创建 DNS 记录！有了这个插件我们可不用再慢慢打开公共 DNS 管理页面，再小心翼翼地记下 IP 地址去添加 A 记录了。\n\n# 更高级的部署方式（一）\n\n一路说道这里， K8s 中最基础的资源大部分都已经介绍了。但是，这么多资源之间又需要相互配合，只部署一种资源基本没什么生产能力。\n\n比如只部署 Deployment 的话，我们确实是能在一组多副本的 Pod 里跑起可执行程序，但这组 Pod 却几乎没办法接受集群里其他 Pod 的流量（只能通过制定 Pod 的 IP 来访问，但 Pod 的 IP 是会变的）。因此一般来说一个 Deployment 都会搭配一个 Service 来使用。这还是最简单的一种搭配了。\n\n假若我们现在要在自己的 K8s 里安装一个别人提供的应用。当然由于 K8s 是基于容器的，只要别人提供了他应用的 yaml 清单，我们只用把清单用 `kubectl apply -f` 提交给 K8s ，然后让 K8s 把清单中的镜像拉下来就能跑了。可如果我们需要根据环境来改一些参数呢？\n\n如果别人提供的 yaml 文件比较简单还好说，改改对应的字段就好了。如果别人的应用比较复杂，那改 yaml 文件可就是一个大难题了。比如 AWS 的 Load Balancer Controller ，它的 yaml 清单文件可是多达 939 行！\n\n[[aws-elb-controller-lines.png]]\n\n在这种复杂的场景下，我们就需要一些更高级的部署方式了。\n\n### Helm\n\n首先来介绍的是 Helm 。 Helm 是一个包管理工具，可以类比一下 CentOS 中的 yum 工具。它可以把一组 K8s 资源发布成一个 Chart ，然后我们可以用 Helm 来安装这个 Chart ，并且可以通过参数设值来改变 Chart 中的部分资源。利用 Helm 安装 Chart 后还可以管理 Chart 的升级、回滚、卸载。\n\n使用别人提供的 Helm Chart 前，需要先 add 一下 Chart 的仓库，然后再安装仓库里提供的 Chart 。比如我们要安装 bitnami 提供的 Kafka Chart 时：\n\n```bash\n# 添加 https://charts.bitnami.com/bitnami 这个仓库，命名为 bitnami\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# 在 kafka 名称空间里安装 bitnami 仓库里的 kafka Chart ，并通过参数设置为 3 个副本，并同时安装一个 3 副本的 Zookeeper\nhelm install kafka -n kafka \\\n  --set replicaCount=3 \\\n  --set zookeeper.enabled=true \\\n  --set zookeeper.replicaCount=3 \\\n  bitnami/kafka\n```\n\n命令执行后， helm 就会根据参数与 Chart 的内容，在 K8s 里安装 StatefulSet 、 Service 、 ConfigMap 等一切所需要的资源。\n\n```sh\n$ k -n kafka get all,cm\nNAME                    READY   STATUS    RESTARTS      AGE\npod/kafka-0             1/1     Running   1             46d\npod/kafka-1             1/1     Running   3             46d\npod/kafka-2             1/1     Running   3             46d\npod/kafka-zookeeper-0   1/1     Running   0             46d\npod/kafka-zookeeper-1   1/1     Running   0             46d\npod/kafka-zookeeper-2   1/1     Running   0             46d\n\nNAME                               TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nservice/kafka                      ClusterIP      172.20.1.196     \u003cnone\u003e           9092/TCP                     164d\nservice/kafka-headless             ClusterIP      None             \u003cnone\u003e           9092/TCP,9093/TCP            164d\nservice/kafka-zookeeper            ClusterIP      172.20.227.236   \u003cnone\u003e           2181/TCP,2888/TCP,3888/TCP   164d\nservice/kafka-zookeeper-headless   ClusterIP      None             \u003cnone\u003e           2181/TCP,2888/TCP,3888/TCP   164d\n\nNAME                               READY   AGE\nstatefulset.apps/kafka             3/3     164d\nstatefulset.apps/kafka-zookeeper   3/3     164d\n\nNAME                                DATA   AGE\nconfigmap/kafka-scripts             2      164d\nconfigmap/kafka-zookeeper-scripts   2      164d\nconfigmap/kube-root-ca.crt          1      165d\n```\n\n甚至， Helm 可以通过模板生成的 Pod 环境变量，来预先设置好 Kafka 的配置，让他找得到 Zookeeper 服务：\n\n```yaml\napiVersion: v1\nkind: Pod\n# 略去无关信息\nspec:\n  containers:\n  - name: kafka\n    command:\n    - /scripts/setup.sh\n    env:\n    - name: KAFKA_CFG_ZOOKEEPER_CONNECT\n      value: kafka-zookeeper\n    # ...\n```\n\n通过设置 `KAFKA_CFG_ZOOKEEPER_CONNECT` 这个环境变量，指定了 Kafka Broker 可以通过访问 `kafka-zookeeper` 来找到 zookeeper 服务。（还记得 zookeeper 的 Service 名字是 `kafka-zookeeper` 吗？ zookeeper 与 kafka 部署在同一个名称空间里，因此可以直接通过 Service 名访问。）\n\n如果我们打开这个 helm chart 对应的[代码仓库](https://github.com/bitnami/charts/tree/master/bitnami/kafka)，会发现原来有一组 go template 文件，以及一个 `values.yaml` 文件和 `Chart.yaml` 文件：\n\n```sh\n.\n├── Chart.lock\n├── Chart.yaml\n├── README.md\n├── templates\n│   ├── NOTES.txt # 这里定义的是 helm 工具的命令行信息\n│   ├── _helpers.tpl # 这里面是一些定义好的 go template 代码块可以供其他模板使用\n│   ├── configmap.yaml\n│   ├── statefulset.yaml\n│   ├── svc-headless.yaml\n│   ├── svc.yaml\n│   └── # 以下省略若干模板文件\n└── values.yaml\n```\n\n- `Chart.yaml` 中定义了这个 Chart 的基本信息，包括名称、版本、描述、依赖等。\n- `values.yaml` 中定义了这个 Chart 的默认参数，包括各种资源的默认配置、副本数量、镜像版本等。其中的值都可以通过 `helm install` 命令的 `--set` 参数来覆盖。\n- `templates/` 文件夹下的都是 go template 的模板文件。\n\n`helm install` 就是通过用 `values.yaml` 中预定义的参数，渲染 `templates/` 文件夹下的 go template 文件，生成最终的 yaml 文件，然后再通过 kubectl apply -f 的方式，将 yaml 文件里的资源部署到 K8s 里。然后通过忘资源里注入一些特殊 annotation 的方式来记住自己部署了那些资源，进而提供 `update` 、 `uninstall` 等功能。\n\n关于更多 Helm 的内容，可以参考[官方文档](https://helm.sh/docs/)。\n\n### Kustomize\n\n另一个部署工具是 Kustomize 。之前提到 Config Map 时的例子中，将配置文件的内容直接写进了 yaml 清单的一个字段里：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-demo\ndata:\n  # 一个 Key 可以对应一个值\n  player_initial_lives: \"3\"\n  ui_properties_file_name: \"user-interface.properties\"\n\n  # 一个 Key 也可以对应一个文件的内容\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5    \n  user-interface.properties: |\n    color.good=purple\n    color.bad=yellow\n    allow.textmode=true    \n```\n\n其实这样很不好，先不说这样写没办法在 IDE 里用配置文件自己的语法检查，每行还需要一定的缩进，如果配置文件有好几百行，你甚至会忘了这一行到底是哪个配置文件！此时我们就会自然而然的想把每个配置文件以单独文件的形式保存。\n\nKustomize 就是这样一个工具，它可以帮助我们把每个配置文件以单独文件的形式保存，然后再通过一个 `kustomization.yaml` 文件，将这些配置文件组合起来，生成最终的 yaml 文件。\n\n```yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  # 其他资源也可以单独使用一个文件定义\n  - deployment.yaml\n\n# 用 configMapGenerator 从文件中生成 ConfigMap\nconfigMapGenerator:\n  - name: game-demo\n    literals:\n      - \"ui_properties_file_name=user-interface.properties\"\n      - \"player_initial_lives=3\"\n    # 从文件中读取内容\n    files:\n      - game.properties\n      - user-interface.properties\n# 有多个 configMap 时，可以通过统一的 generatorOptions 来设置一些通用的选项\ngeneratorOptions:\n  disableNameSuffixHash: true\n```\n\n然后两个配置文件的内容可以单独用文件定义，此时可以结合 IDE 的语法检查，以及代码补全功能，来编写配置文件。\n\n```properties\n# user-interface.properties\ncolor.good=purple\ncolor.bad=yellow\nallow.textmode=true    \n```\n\n然后将 `kustomization.yaml` 和其他所需的文件都放在同一个目录下：\n\n```bash\n.\n├── kustomization.yaml\n├── deployment.yaml\n├── game.properties\n└── user-interface.properties\n```\n\n然后就可以通过 `kubectl apply -k ./` 来将整个 kustomize 文件夹转换为 yaml 清单直接部署到 K8s 中。\n（没错，现在 Kustomize 已经成为 kubectl 中的内置功能！可以不用先 `kustomize build` 生成 yaml 文件再 `kubectl apply` 两步走了！）\n\n值得提醒的是，虽然 `kustomization.yaml` 有 `apiVersion` 和 `kind` 字段，长得很像一个资源清单，但其实 K8s 的 API server 并不认识他。 Kustomize 的工作原理其实是先根据 `kustomization.yaml` 生成 K8s 认识的 yaml 资源清单，然后再通过 `kubectl apply` 来部署。\n\n除了可以直接将 ConfigMap 与 Secret 中的文件字段内容用单独的文件定义外， Kustomize 还有其他比如为部署的资源添加统一的名称前缀、添加统一字段等功能。这些大家可以阅读 Kustomize 的[官方文档](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/)来了解。\n\n### 各种工具的优缺点\n\n我们目前已经知道有三种在 K8s 中部署资源的方式： `kubectl apply`、Helm 和 Kustomize 。\n\n其中 `kubectl apply` 的优缺点很明确，优点是最简单直接，缺点是会导致要么 yaml 清单过长，要么需要分多文件多次部署，使集群中产生中间状态。\n\n而 Helm 与 Kustomize 我们上面也分析过，其实都是基于 `kubectl apply` 的。 Helm 是通过 go template 先生成 yaml 文件再 `kubectl apply` ，而 Kustomize 是通过 `kustomization.yaml` 中的定义用自己的一套逻辑生成 yaml 文件，然后再 `kubectl apply` 。\n\nHelm 的优点是 Helm Chart 安装时可以直接使用别人 Helm 仓库中已经上传好的 Chart ，只需要设置参数就可以使用。这也是 Kustomize 的缺点：如果想要使用别人提供的 Kustomization 而只修改其中的一些配置，必须要先把放 `kustomization.yaml` 的整个文件夹下载下来才能做修改。\n\n而 Helm 的缺点也是明显的， Helm 依赖于往资源里注入特殊的 annotation 来管理 Chart 生成的资源，这可能会很难与集群中现有的一些系统（比如 Service Mesh 或是 GitOps 系统等）放一起管理。而 Kustomize 生成的 yaml 清单就是很干净的 K8s 资源，原先的 K8s 资源该是什么表现就是什么表现，与现有的系统兼容一般会比较好。\n\n而另外，由于 Helm 与 Kustomize 都是基于 `kubectl apply` 的，因此他们有共同的缺点，就是不能做 `kubectl apply` 不能做的事情。\n\n什么叫 `kubectl apply` 不能做的事情呢？比如说我们要在 K8s 中部署 Redis 集群。聪明的你可能就想到要用 Stateful Set 、 PVC 、 Headless Service 来一套组合拳。这确实可以部署一个多节点、有状态的 Redis Cluster 。可是如果我们要往 Redis Cluster 里加一个节点呢？\n\n你当然可以把 Stateful Set 中的 `Replicas` 字段加个 1 然后用 `kubectl apply` 部署，可是这实际上只能增加一个一个 Redis 实例 —— 然后什么都没发生。其他节点不认识这个新的节点，访问这个新节点也不能拿到正确的数据。要知道往 Redis Cluster 里加节点，是要先让集群发现这个新节点，然后还要迁移 slot 的！ `kubectl apply` 可不会做这些事。\n\n\u003e Well, 其实这些也是可以通过增加 initContainer 、修改镜像增加启动脚本等方式，实现用 `kubectl apply` 部署的。可是，这会让整个 Pod 资源变得很难理解，也不好维护。而且，如果不是因为做不到，谁会想去修改别人的镜像呢？\n\n我们接下来会介绍 K8s 的核心架构，来理解我们之前讲的这些资源到底是怎么工作的。最后会引出一组新的概念： Operator 与自定义资源（ Custom Resource Definition ，简称 CRD ）。通过 Operator 与 CRD ，我们可以做到 `kubectl apply` 所不能做到的事，包括 Redis Cluster 的扩容。\n\n\u003e DIO: `kubectl apply` 的能力是有限的……\n\u003e 越是部署复杂的应用，就越会发现 `kubectl apply` 的能力是有极限的……除非超越 `kubectl apply` 。\n\u003e \n\u003e JOJO: 你到底想说什么？\n\u003e \n\u003e DIO: 我不用 `kubectl apply` 了！ JOJO ！\n\u003e （其实还是要用的）\n\n","title":"Kubernetes 入门 （2）","abstract":"我们之前说的都是用于部署 Pod 的资源，我们接下来介绍与创建 Pod 不相关的资源：储存与网络。\n其实我们之前已经接触过储存相关的内容了：在讲 Stateful Set 时我们提过 Stateful Set 创建出来的 Pod 都会有相互独立的储存；而讲 Daemon Set 时我们提到 K8s 推荐只在 Daemon Set 的 Pod 中访问宿主机磁盘。但独立的储存具体指什么？除了访问宿主机磁盘以外还有什么其他的储存？\n在 Docker 中，我们可以把宿主机磁盘上的一个路径作为一个 Volume 来给容器绑定，或者直接使用 Docker Engine 管理的 Volume 来提供持久化存储或是容器间共享文件。在 K8s 里面也沿用了 Volume 这个概念，可以通过 Mount 绑定到容器内的路径，并通过实现 CSI 的各种引擎来提供更多样的存储。","length":875,"created_at":"2022-08-20T21:56:52.000Z","updated_at":"2022-08-20T14:02:18.000Z","tags":["Kubernetes","DevOps","Docker","Cloud Native"],"license":true}}}},"__N_SSG":true},"page":"/articles/[slug]","query":{"slug":"try-cursor-and-thinking"},"buildId":"YbslwkcmPy29WMud75N6k","assetPrefix":"/blog-next","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>