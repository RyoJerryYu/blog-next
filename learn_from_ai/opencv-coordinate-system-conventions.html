<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta property="og:image" content="https://ryojerryyu.github.io/blog-next/img/home-bg-kasumi-hanabi.jpg" data-next-head=""/><meta name="twitter:image" content="https://ryojerryyu.github.io/blog-next/img/home-bg-kasumi-hanabi.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta property="og:url" content="https://blog.ryo-okami.xyz/learn_from_ai/opencv-coordinate-system-conventions" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@ryo_okami" data-next-head=""/><meta name="twitter:creator" content="@ryo_okami" data-next-head=""/><link rel="icon" href="/blog-next/favicon.ico" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1" data-next-head=""/><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests" data-next-head=""/><title data-next-head="">OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别 | Ryo&#x27;s Blog</title><meta property="og:title" content="OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别" data-next-head=""/><meta property="og:site_name" content="Ryo&#x27;s Blog" data-next-head=""/><meta name="twitter:title" content="OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别 | Ryo&#x27;s Blog" data-next-head=""/><meta name="description" content="&gt; 📝 本文是我在学习 OpenCV 时遇到的一个问题。问题的解答和中文翻译均由 AI 生成。
Why sometimes opencv represent a point coordinate as (y,x) (e.g. the result of cv2.findContours) , and sometimes it represent as (x, y) (e.g. the arg `center` of cv2.circle)?
&gt; 为什么 OpenCV 有时用 (y,x) 表示坐标点（比如 cv2.findContours 的返回值），有时又用 (x,y) 表示（比如 cv2.circle 函数的 `center` 参数）？" data-next-head=""/><meta property="og:description" content="&gt; 📝 本文是我在学习 OpenCV 时遇到的一个问题。问题的解答和中文翻译均由 AI 生成。
Why sometimes opencv represent a point coordinate as (y,x) (e.g. the result of cv2.findContours) , and sometimes it represent as (x, y) (e.g. the arg `center` of cv2.circle)?
&gt; 为什么 OpenCV 有时用 (y,x) 表示坐标点（比如 cv2.findContours 的返回值），有时又用 (x,y) 表示（比如 cv2.circle 函数的 `center` 参数）？" data-next-head=""/><meta name="twitter:description" content="&gt; 📝 本文是我在学习 OpenCV 时遇到的一个问题。问题的解答和中文翻译均由 AI 生成。
Why sometimes opencv represent a point coordinate as (y,x) (e.g. the result of cv2.findContours) , and sometimes it represent as (x, y) (e.g. the arg `center` of cv2.circle)?
&gt; 为什么 OpenCV 有时用 (y,x) 表示坐标点（比如 cv2.findContours 的返回值），有时又用 (x,y) 表示（比如 cv2.circle 函数的 `center` 参数）？" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="192x192" href="/android-chrome-192x192.png"/><link rel="manifest" href="/site.webmanifest"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/blog-next/_next/static/css/8db32621d94754ab.css" as="style"/><link rel="stylesheet" href="/blog-next/_next/static/css/8db32621d94754ab.css" data-n-g=""/><link rel="preload" href="/blog-next/_next/static/css/5bd958681a4509b5.css" as="style"/><link rel="stylesheet" href="/blog-next/_next/static/css/5bd958681a4509b5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/blog-next/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/blog-next/_next/static/chunks/webpack-3597c977fede260d.js" defer=""></script><script src="/blog-next/_next/static/chunks/framework-898697981b1ca118.js" defer=""></script><script src="/blog-next/_next/static/chunks/main-a0a5fc4e603d1ed2.js" defer=""></script><script src="/blog-next/_next/static/chunks/pages/_app-861c1dc7f33f622c.js" defer=""></script><script src="/blog-next/_next/static/chunks/ee8b1517-1aa9307a3ec34e41.js" defer=""></script><script src="/blog-next/_next/static/chunks/4785-70b0f2db3df81cbd.js" defer=""></script><script src="/blog-next/_next/static/chunks/9965-95126bbb1fbfbef4.js" defer=""></script><script src="/blog-next/_next/static/chunks/39-0a3d8a77fd97d8ce.js" defer=""></script><script src="/blog-next/_next/static/chunks/7171-15e4933f5e756a14.js" defer=""></script><script src="/blog-next/_next/static/chunks/pages/learn_from_ai/%5Bslug%5D-609118750b7c6737.js" defer=""></script><script src="/blog-next/_next/static/w1ZimoTUC3z7UiVINTMsP/_buildManifest.js" defer=""></script><script src="/blog-next/_next/static/w1ZimoTUC3z7UiVINTMsP/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="DefaultLayout_header__aepTD"><div class="DefaultLayout_icon__11sTk"><div class="DefaultLayout_textbox__H9FZG"><a class="DefaultLayout_textlink__EVwys" href="/blog-next">Ryo&#x27;s Blog</a></div></div><div class="DefaultLayout_navBar__gY4ra"><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/articles">Articles</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/ideas">Ideas</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/learn_from_ai">Learn from AI</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/tags">Tags</a></div><div class="DefaultLayout_navBarItem__nhL6L"><a class="DefaultLayout_textlink__EVwys" href="/blog-next/clips">Clips</a></div></div><div class="DefaultLayout_headerRight__0Kj26"><div class=" flex flex-row gap-8 items-center justify-center"><a title="Twitter" href="https://twitter.com/ryo_okami"><svg class=" h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a title="GitHub" href="https://github.com/RyoJerryYu"><svg class=" h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a title="Pixiv" href="https://www.pixiv.net/users/9159893"><svg class=" h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4.935 0A4.924 4.924 0 0 0 0 4.935v14.13A4.924 4.924 0 0 0 4.935 24h14.13A4.924 4.924 0 0 0 24 19.065V4.935A4.924 4.924 0 0 0 19.065 0zm7.81 4.547c2.181 0 4.058.676 5.399 1.847a6.118 6.118 0 0 1 2.116 4.66c.005 1.854-.88 3.476-2.257 4.563-1.375 1.092-3.225 1.697-5.258 1.697-2.314 0-4.46-.842-4.46-.842v2.718c.397.116 1.048.365.635.779H5.79c-.41-.41.19-.65.644-.779V7.666c-1.053.81-1.593 1.51-1.868 2.031.32 1.02-.284.969-.284.969l-1.09-1.73s3.868-4.39 9.553-4.39zm-.19.971c-1.423-.003-3.184.473-4.27 1.244v8.646c.988.487 2.484.832 4.26.832h.01c1.596 0 2.98-.593 3.93-1.533.952-.948 1.486-2.183 1.492-3.683-.005-1.54-.504-2.864-1.42-3.86-.918-.992-2.274-1.645-4.002-1.646Z"></path></svg></a></div></div></header><div class="DefaultLayout_headerBg__FStmg"></div><div class="max-w-3xl mx-auto p-2"><div class="DefaultLayout_contentHeight__DabjQ"><article class="Post_post__acRqJ"><h1 class="Post_postTitle__N1NIA">OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别</h1><div class="Post_postDate__SQx7A"><time dateTime="2024-11-29T03:00:00.000Z">2024-11-29</time></div><div class="TagsBox_tagsBox__WzhAf mt-2"><a href="/blog-next/tags/opencv"><div class="TagsBox_tag__Rk32C">OpenCV</div></a><a href="/blog-next/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86"><div class="TagsBox_tag__Rk32C">图像处理</div></a><a href="/blog-next/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0"><div class="TagsBox_tag__Rk32C">学习笔记</div></a></div><div class="post-body Post_postContent__mJ_Ju"><blockquote>
<p>📝 本文是我在学习 OpenCV 时遇到的一个问题。问题的解答和中文翻译均由 AI 生成。</p>
</blockquote>
<h1 id="opencv-坐标表示理解-y-x-与-x-y-的区别"><a href="#opencv-坐标表示理解-y-x-与-x-y-的区别">OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别</a></h1>
<h2 id="q"><a href="#q">Q</a></h2>
<p>Why sometimes opencv represent a point coordinate as (y,x) (e.g. the result of cv2.findContours) , and sometimes it represent as (x, y) (e.g. the arg <code>center</code> of cv2.circle)?</p>
<blockquote>
<p>为什么 OpenCV 有时用 (y,x) 表示坐标点（比如 cv2.findContours 的返回值），有时又用 (x,y) 表示（比如 cv2.circle 函数的 <code>center</code> 参数）？</p>
</blockquote>
<h2 id="a"><a href="#a">A</a></h2>
<p>OpenCV uses different conventions for different functions based on their specific requirements and historical reasons:</p>
<blockquote>
<p>OpenCV 基于不同函数的具体需求和历史原因，使用了不同的坐标表示约定：</p>
</blockquote>
<ol>
<li><strong>(y, x) Convention</strong>: This is often used when dealing with image matrices directly. In image processing, images are typically represented as 2D arrays where the first dimension is the row (height, y-coordinate) and the second dimension is the column (width, x-coordinate). Functions like <code>cv2.findContours</code> return coordinates in this order to match the array indexing.</li>
</ol>
<blockquote>
<ol>
<li><strong>(y, x) 约定</strong>：这种表示方法通常用于直接处理图像矩阵时。在图像处理中，图像通常表示为二维数组，其中第一维是行（高度，y坐标），第二维是列（宽度，x坐标）。像 <code>cv2.findContours</code> 这样的函数返回这种顺序的坐标是为了匹配数组索引方式。</li>
</ol>
</blockquote>
<ol start="2">
<li><strong>(x, y) Convention</strong>: This is used in functions that deal with geometric shapes or points in a more traditional Cartesian coordinate system, where <code>x</code> is the horizontal axis and <code>y</code> is the vertical axis. Functions like <code>cv2.circle</code> use this convention to specify the center of the circle.</li>
</ol>
<blockquote>
<ol start="2">
<li><strong>(x, y) 约定</strong>：这种表示方法用于处理几何图形或在传统笛卡尔坐标系中的点，其中 <code>x</code> 是水平轴，<code>y</code> 是垂直轴。像 <code>cv2.circle</code> 这样的函数使用这种约定来指定圆心位置。</li>
</ol>
</blockquote>
<p>The inconsistency arises because OpenCV aims to balance between image matrix operations and geometric operations, each having their own conventional coordinate systems.</p>
<blockquote>
<p>这种不一致性的出现是因为 OpenCV 需要在图像矩阵运算和几何运算之间取得平衡，而这两种运算各自有其传统的坐标系统表示方法。</p>
</blockquote></div><div class="w-96 text-gray-700 leading-none"><span class="!text-sm"><a class="!inline-block !p-0 !m-0 align-text-bottom" rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" loading="lazy" width="88" height="31" decoding="async" data-nimg="1" class="!m-0 h-4 w-auto pr-1" style="color:transparent;border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png"/></a>This work is licensed under a<!-- --> <a class="underline" rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.</span></div><div class="TagsBox_tagsBox__WzhAf mt-4"><a href="/blog-next/tags/opencv"><div class="TagsBox_tag__Rk32C">OpenCV</div></a><a href="/blog-next/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86"><div class="TagsBox_tag__Rk32C">图像处理</div></a><a href="/blog-next/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0"><div class="TagsBox_tag__Rk32C">学习笔记</div></a></div><div class="mt-4 mb-4 flex justify-center"><div class="mr-0 ml-auto"><a href="/blog-next/learn_from_ai/deep-learning-model-formats">深度学习模型文件格式对比：pth、safetensor 与 onnx -&gt;</a></div></div><hr class="mt-4"/></article></div></div><footer class="DefaultLayout_footer__n5339"><div class="max-w-3xl mx-auto p-2 w-full"><div class="flex flex-row justify-center items-center"><div class="DefaultLayout_footerLeft__j0yvY">© 2023 Ryo Jerry Yu. All rights reserved.</div><div class="DefaultLayout_footerRight___Dn67"><div class=" flex flex-row gap-8 items-center justify-center"><a title="Twitter" href="https://twitter.com/ryo_okami"><svg class="DefaultLayout_footerIcon__sgrmB h-8 w-8" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a title="GitHub" href="https://github.com/RyoJerryYu"><svg class="DefaultLayout_footerIcon__sgrmB h-8 w-8" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a title="Pixiv" href="https://www.pixiv.net/users/9159893"><svg class="DefaultLayout_footerIcon__sgrmB h-8 w-8" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4.935 0A4.924 4.924 0 0 0 0 4.935v14.13A4.924 4.924 0 0 0 4.935 24h14.13A4.924 4.924 0 0 0 24 19.065V4.935A4.924 4.924 0 0 0 19.065 0zm7.81 4.547c2.181 0 4.058.676 5.399 1.847a6.118 6.118 0 0 1 2.116 4.66c.005 1.854-.88 3.476-2.257 4.563-1.375 1.092-3.225 1.697-5.258 1.697-2.314 0-4.46-.842-4.46-.842v2.718c.397.116 1.048.365.635.779H5.79c-.41-.41.19-.65.644-.779V7.666c-1.053.81-1.593 1.51-1.868 2.031.32 1.02-.284.969-.284.969l-1.09-1.73s3.868-4.39 9.553-4.39zm-.19.971c-1.423-.003-3.184.473-4.27 1.244v8.646c.988.487 2.484.832 4.26.832h.01c1.596 0 2.98-.593 3.93-1.533.952-.948 1.486-2.183 1.492-3.683-.005-1.54-.504-2.864-1.42-3.86-.918-.992-2.274-1.645-4.002-1.646Z"></path></svg></a></div></div></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"opencv-coordinate-system-conventions","tags":[{"tag":"OpenCV","slug":"opencv","path":"/tags/opencv","postSlugs":[{"postType":"learn_from_ai","postSlug":"opencv-coordinate-system-conventions","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"}]},{"tag":"图像处理","slug":"图像处理","path":"/tags/图像处理","postSlugs":[{"postType":"learn_from_ai","postSlug":"opencv-coordinate-system-conventions","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"}]},{"tag":"学习笔记","slug":"学习笔记","path":"/tags/学习笔记","postSlugs":[{"postType":"learn_from_ai","postSlug":"deep-learning-model-formats","postPagePath":"/learn_from_ai/deep-learning-model-formats"},{"postType":"learn_from_ai","postSlug":"opencv-coordinate-system-conventions","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"}]}],"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    h1: \"h1\",\n    h2: \"h2\",\n    li: \"li\",\n    ol: \"ol\",\n    p: \"p\",\n    strong: \"strong\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"📝 本文是我在学习 OpenCV 时遇到的一个问题。问题的解答和中文翻译均由 AI 生成。\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h1, {\n      id: \"opencv-坐标表示理解-y-x-与-x-y-的区别\",\n      children: _jsx(_components.a, {\n        href: \"#opencv-坐标表示理解-y-x-与-x-y-的区别\",\n        children: \"OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"q\",\n      children: _jsx(_components.a, {\n        href: \"#q\",\n        children: \"Q\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Why sometimes opencv represent a point coordinate as (y,x) (e.g. the result of cv2.findContours) , and sometimes it represent as (x, y) (e.g. the arg \", _jsx(_components.code, {\n        children: \"center\"\n      }), \" of cv2.circle)?\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsxs(_components.p, {\n        children: [\"为什么 OpenCV 有时用 (y,x) 表示坐标点（比如 cv2.findContours 的返回值），有时又用 (x,y) 表示（比如 cv2.circle 函数的 \", _jsx(_components.code, {\n          children: \"center\"\n        }), \" 参数）？\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"a\",\n      children: _jsx(_components.a, {\n        href: \"#a\",\n        children: \"A\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"OpenCV uses different conventions for different functions based on their specific requirements and historical reasons:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"OpenCV 基于不同函数的具体需求和历史原因，使用了不同的坐标表示约定：\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"(y, x) Convention\"\n        }), \": This is often used when dealing with image matrices directly. In image processing, images are typically represented as 2D arrays where the first dimension is the row (height, y-coordinate) and the second dimension is the column (width, x-coordinate). Functions like \", _jsx(_components.code, {\n          children: \"cv2.findContours\"\n        }), \" return coordinates in this order to match the array indexing.\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"(y, x) 约定\"\n          }), \"：这种表示方法通常用于直接处理图像矩阵时。在图像处理中，图像通常表示为二维数组，其中第一维是行（高度，y坐标），第二维是列（宽度，x坐标）。像 \", _jsx(_components.code, {\n            children: \"cv2.findContours\"\n          }), \" 这样的函数返回这种顺序的坐标是为了匹配数组索引方式。\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"(x, y) Convention\"\n        }), \": This is used in functions that deal with geometric shapes or points in a more traditional Cartesian coordinate system, where \", _jsx(_components.code, {\n          children: \"x\"\n        }), \" is the horizontal axis and \", _jsx(_components.code, {\n          children: \"y\"\n        }), \" is the vertical axis. Functions like \", _jsx(_components.code, {\n          children: \"cv2.circle\"\n        }), \" use this convention to specify the center of the circle.\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsxs(_components.ol, {\n        start: \"2\",\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"(x, y) 约定\"\n          }), \"：这种表示方法用于处理几何图形或在传统笛卡尔坐标系中的点，其中 \", _jsx(_components.code, {\n            children: \"x\"\n          }), \" 是水平轴，\", _jsx(_components.code, {\n            children: \"y\"\n          }), \" 是垂直轴。像 \", _jsx(_components.code, {\n            children: \"cv2.circle\"\n          }), \" 这样的函数使用这种约定来指定圆心位置。\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The inconsistency arises because OpenCV aims to balance between image matrix operations and geometric operations, each having their own conventional coordinate systems.\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"这种不一致性的出现是因为 OpenCV 需要在图像矩阵运算和几何运算之间取得平衡，而这两种运算各自有其传统的坐标系统表示方法。\"\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}},"meta":{"content":"\n\u003e 📝 本文是我在学习 OpenCV 时遇到的一个问题。问题的解答和中文翻译均由 AI 生成。\n\n# OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别\n\n## Q\nWhy sometimes opencv represent a point coordinate as (y,x) (e.g. the result of cv2.findContours) , and sometimes it represent as (x, y) (e.g. the arg `center` of cv2.circle)?\n\n\u003e 为什么 OpenCV 有时用 (y,x) 表示坐标点（比如 cv2.findContours 的返回值），有时又用 (x,y) 表示（比如 cv2.circle 函数的 `center` 参数）？\n\n## A\nOpenCV uses different conventions for different functions based on their specific requirements and historical reasons:\n\n\u003e OpenCV 基于不同函数的具体需求和历史原因，使用了不同的坐标表示约定：\n\n1. **(y, x) Convention**: This is often used when dealing with image matrices directly. In image processing, images are typically represented as 2D arrays where the first dimension is the row (height, y-coordinate) and the second dimension is the column (width, x-coordinate). Functions like `cv2.findContours` return coordinates in this order to match the array indexing.\n\n\u003e 1. **(y, x) 约定**：这种表示方法通常用于直接处理图像矩阵时。在图像处理中，图像通常表示为二维数组，其中第一维是行（高度，y坐标），第二维是列（宽度，x坐标）。像 `cv2.findContours` 这样的函数返回这种顺序的坐标是为了匹配数组索引方式。\n\n2. **(x, y) Convention**: This is used in functions that deal with geometric shapes or points in a more traditional Cartesian coordinate system, where `x` is the horizontal axis and `y` is the vertical axis. Functions like `cv2.circle` use this convention to specify the center of the circle.\n\n\u003e 2. **(x, y) 约定**：这种表示方法用于处理几何图形或在传统笛卡尔坐标系中的点，其中 `x` 是水平轴，`y` 是垂直轴。像 `cv2.circle` 这样的函数使用这种约定来指定圆心位置。\n\nThe inconsistency arises because OpenCV aims to balance between image matrix operations and geometric operations, each having their own conventional coordinate systems.\n\n\u003e 这种不一致性的出现是因为 OpenCV 需要在图像矩阵运算和几何运算之间取得平衡，而这两种运算各自有其传统的坐标系统表示方法。","title":"OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别","abstract":"\u003e 📝 本文是我在学习 OpenCV 时遇到的一个问题。问题的解答和中文翻译均由 AI 生成。\nWhy sometimes opencv represent a point coordinate as (y,x) (e.g. the result of cv2.findContours) , and sometimes it represent as (x, y) (e.g. the arg `center` of cv2.circle)?\n\u003e 为什么 OpenCV 有时用 (y,x) 表示坐标点（比如 cv2.findContours 的返回值），有时又用 (x,y) 表示（比如 cv2.circle 函数的 `center` 参数）？","length":26,"created_at":"2024-11-29T03:00:00.000Z","updated_at":"2024-11-29T03:00:00.000Z","tags":["OpenCV","图像处理","学习笔记"],"license":true},"prevNextInfo":{"prevInfo":null,"nextInfo":{"pathMapping":{"filePath":"public/content/learn_from_ai/2024-11-29-deep-learning-model-formats.md","pagePath":"/learn_from_ai/deep-learning-model-formats","slug":"deep-learning-model-formats"},"meta":{"content":"\n\u003e 本页面介绍了三种机器学习模型文件格式：.pth、.safetensor 和 .onnx。主要内容包括每种格式的定义、保存的内容、用途以及它们之间的区别和转换方法。\n\n# pytorch 保存的 .pth 文件里都保存了些什么东西？\n\nPyTorch中的`.pth`文件通常用于保存模型的权重（parameters）和训练过程中的优化器状态（optimizer state）。具体来说，一个`.pth`文件可能包含以下内容：\n1. **模型状态字典（Model state dictionary）**：\n   - 这是`.pth`文件中最主要的部分，它包含了模型的所有权重（即神经网络的参数），包括卷积层的滤波器、全连接层的权重和偏置等。\n   - 状态字典是一个简单的Python字典对象，其键是层的名字，值是相应的参数张量。\n2. **优化器状态字典（Optimizer state dictionary）**：\n   - 当保存模型时，通常也会保存优化器的状态，这样可以在模型训练中断后，从上次停止的地方继续训练。\n   - 优化器状态字典通常包含每个参数的梯度、动量（momentum）和其他与优化器相关的状态信息（例如Adam优化器的矩估计）。\n3. **其他可选信息**：\n   - 有时候，`.pth`文件可能还会包含其他辅助信息，比如训练的损失值、额外的元数据或者批处理大小等，但这些信息不是必须的。\n以下是一个简单的例子，展示如何在PyTorch中保存和加载`.pth`文件：\n```python\n# 保存模型和优化器状态\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    ...\n}, 'model.pth')\n# 加载模型和优化器状态\ncheckpoint = torch.load('model.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n...\n```\n在这个例子中，`model`是你要保存的模型对象，`optimizer`是训练该模型使用的优化器。通过`torch.save`函数，你可以将模型的状态字典和优化器的状态字典保存到文件中。之后，可以使用`torch.load`函数来恢复这些状态，以便继续训练或者进行推理。\n\n# .safetensor 文件中又保存了什么东西？\n\n`.safetensor` 文件格式是由 Hugging Face 的 Safety Checker 工具引入的，它是为了在模型推理时提供一个额外的安全层。`.safetensor` 文件中保存的内容主要包括：\n1. **模型权重**：\n   - 这是最核心的部分，与 PyTorch 的 `.pth` 文件类似，`.safetensor` 文件也保存了模型的权重，即神经网络中的参数。\n2. **元数据**：\n   - `.safetensor` 文件包含了一些元数据，这些数据描述了模型的架构、权重形状、数据类型等信息，这样可以在加载模型时进行验证和适配。\n3. **安全性信息**：\n   - `.safetensor` 文件可能会包含与模型安全性相关的信息，例如哪些层或参数可能需要特别的安全检查，以及可能的阈值设置等。\n具体来说，`.safetensor` 文件的内容结构可能如下：\n- `weights`: 包含模型权重的字典，通常是以张量的形式存储。\n- `metadata`: 包含模型的元数据，如版本号、架构信息等。\n- `config`: 包含模型配置信息，如模型类型、类名等。\n- `safety`: 包含模型安全性相关的信息。\n使用 `.safetensor` 文件的目的之一是为了确保在模型部署时能够进行有效的安全检查，特别是在处理生成型模型（如文本生成、图像生成等）时，可以防止生成不适当的内容。\n请注意，`.safetensor` 文件格式并不是 PyTorch 的标准保存格式，而是 Hugging Face 的特定实现。因此，它可能不如 `.pth` 文件那样被广泛使用。如果你在使用 Hugging Face 的模型和工具，可能会遇到 `.safetensor` 文件。在使用时，需要确保你的环境支持这种格式。\n\n# .onnx 文件中又保存了什么信息？\n\n`.onnx`（Open Neural Network Exchange）文件是一个开放格式，用于表示机器学习模型。它旨在提供一种标准的方式来表示深度学习模型，使得模型可以在不同的框架和工具之间进行转换和互操作。一个`.onnx`文件中通常包含以下信息：\n1. **模型图（Model Graph）**：\n   - 模型图定义了模型的计算流程，它由节点（nodes）和边（edges）组成。\n   - 节点代表计算操作，如卷积、矩阵乘法、激活函数等。\n   - 边代表节点之间的数据流，即张量（tensors）的流动。\n2. **初始值（Initializers）**：\n   - 初始值包含了模型的权重、偏置等参数的值。\n   - 这些参数是在模型图执行之前就已经确定的。\n3. **模型属性（Attributes）**：\n   - 每个节点可能有与之关联的属性，这些属性定义了节点的具体行为，例如卷积操作中的步长（stride）和填充（padding）。\n4. **输入和输出（Inputs and Outputs）**：\n   - 定义了模型的输入和输出张量的名称、数据类型和形状。\n   - 这有助于理解模型期望的输入格式以及它将产生的输出格式。\n5. **元数据（Metadata）**：\n   - 包括模型的名称、版本、作者、生产日期等信息。\n   - 这些信息有助于模型的追踪和管理。\n6. **优化信息（Optimization Information）**（可选）：\n   - 一些`.onnx`文件可能包含用于优化模型执行的额外信息，如融合操作或特定硬件上的优化指令。\n`.onnx`文件的结构大致如下：\n- `ir_version`: ONNX格式的版本号。\n- `opset_import`: 模型使用的操作集版本。\n- `producer_name`: 生成该ONNX模型的工具或库的名称。\n- `producer_version`: 生成该ONNX模型的工具或库的版本。\n- `graph`: 包含模型图的结构和属性。\n  - `node`: 计算节点列表。\n  - `input`: 输入列表。\n  - `output`: 输出列表。\n  - `initializer`: 初始化参数列表。\n  - `value_info`: 提供额外的张量信息。\n使用`.onnx`文件，可以轻松地将模型从PyTorch、TensorFlow、Caffe2等框架转换到ONNX格式，然后再部署到不同的推理引擎或硬件平台上，例如使用ONNX Runtime进行推理。这种格式提高了模型的便携性和互操作性。\n\n# 简述 .pth ， .safetensor ， .onnx 保存的信息中，有哪些是相同的？有哪些有区别？\n\n.pth, .safetensor, 和 .onnx 文件格式都用于保存机器学习模型的相关信息，但它们的设计目的、兼容性和包含的具体内容有所不同。以下是它们之间相同点和区别的概述：\n**相同点：**\n1. **模型参数/权重**：\n   - 所有三种格式都保存了模型的参数或权重，这些是训练过程中学习到的数值，对于模型的预测能力至关重要。\n2. **元数据**：\n   - 它们都可能包含一些元数据，如模型的版本、创建日期或其他描述性信息，尽管具体的元数据内容和格式可能有所不同。\n**区别：**\n1. **格式和兼容性**：\n   - `.pth` 是 PyTorch 的原生格式，专门用于保存和加载 PyTorch 模型的状态字典，包括模型权重和优化器状态。\n   - `.safetensor` 是 Hugging Face 的 Safety Checker 工具使用的格式，除了模型权重，还包含了安全性相关的信息。\n   - `.onnx` 是一个开放的、跨框架的格式，用于在不同的深度学习框架之间交换模型，不仅包含模型结构和参数，还定义了模型计算图。\n2. **包含内容**：\n   - `.pth` 文件通常包含模型的状态字典和优化器的状态字典，以及可选的其他训练信息（如损失值、迭代次数等）。\n   - `.safetensor` 文件包含模型权重、元数据以及与模型安全性相关的信息。\n   - `.onnx` 文件包含模型计算图、节点定义、边的连接、初始值（模型权重）、输入输出定义以及可选的优化信息。\n3. **使用场景**：\n   - `.pth` 文件主要用于 PyTorch 模型的训练和推理。\n   - `.safetensor` 文件用于在 Hugging Face 的生态系统中进行模型的安全检查。\n   - `.onnx` 文件用于模型在不同框架之间的转换和部署，特别是在需要跨平台或硬件优化的场景中。\n4. **开放性和互操作性**：\n   - `.onnx` 格式的设计重点在于开放性和互操作性，使其成为不同框架和工具之间模型交换的桥梁。\n   - `.pth` 和 `.safetensor` 更专注于特定框架或工具链内的使用。\n总结来说，这三种格式都用于保存模型权重，但它们在格式兼容性、包含的额外信息和使用场景上存在明显差异。`.onnx` 格式更加开放和通用，而 `.pth` 和 `.safetensor` 更专注于特定的框架或应用需求。\n\n# 简述 .pth ， .safetensor ， .onnx 保存的信息中有哪些是相同的？保存的信息中又有哪些有区别？\n\n.pth、.safetensor和.onnx文件格式虽然都是为了保存机器学习模型的相关信息，但它们在保存的内容上存在相同点和差异。\n**相同点：**\n1. **模型参数/权重**：这三种文件格式都会保存模型的参数或权重，这些权重是模型在训练过程中学习到的，对模型的预测能力至关重要。\n2. **元数据**：它们都可能包含一些元数据，例如模型的版本、创建日期或其他描述性信息，尽管具体的元数据和格式可能因文件格式而异。\n**区别：**\n1. **格式和兼容性**：\n   - .pth文件是PyTorch的原生格式，主要用于保存和加载PyTorch模型的状态字典，包括模型权重和优化器状态。\n   - .safetensor文件是Hugging Face的Safety Checker工具使用的格式，除了模型权重，还包括安全性相关的信息。\n   - .onnx文件是一种开放的、跨框架的格式，用于在不同深度学习框架之间交换模型，包含模型结构和参数以及模型计算图。\n2. **包含内容**：\n   - .pth文件通常包含模型的状态字典和优化器的状态字典，可能还包括其他训练信息，如损失值、迭代次数等。\n   - .safetensor文件包含模型权重、元数据以及与模型安全性相关的信息。\n   - .onnx文件包含模型计算图、节点定义、边的连接、初始值（模型权重）、输入输出定义以及可选的优化信息。\n3. **使用场景**：\n   - .pth文件主要用于PyTorch模型的训练和推理。\n   - .safetensor文件用于Hugging Face生态系统中的模型安全检查。\n   - .onnx文件用于模型在不同框架之间的转换和部署，特别是在需要跨平台或硬件优化的场景中。\n4. **开放性和互操作性**：\n   - .onnx格式重点在于开放性和互操作性，便于不同框架和工具之间模型交换。\n   - .pth和.safetensor文件更专注于特定框架或工具链内的使用。\n总结来说，这三种文件格式都保存模型权重，但在格式兼容性、包含的额外信息和使用场景上存在明显差异。.onnx格式更开放和通用，而.pth和.safetensor则更专注于特定框架或应用需求。\n\n# .pth ， .safetensor ， .onnx 三种格式，各自能否用于模型的 finetunning ？为什么？\n\n.pth、.safetensor 和 .onnx 三种格式在模型微调（finetuning）方面的适用性各有不同，以下是各自的适用性分析：\n**.pth 格式：**\n.pth 文件可以用于模型的微调。原因如下：\n- .pth 文件是 PyTorch 的原生格式，它保存了模型的状态字典，包括模型的权重和可选的优化器状态。\n- 在微调过程中，可以加载 .pth 文件中的模型权重，然后根据新的数据集对模型进行进一步的训练。\n- 如果 .pth 文件中包含了优化器状态，那么可以从中断点继续训练，这对于微调过程是有帮助的。\n**.safetensor 格式：**\n.safetensor 文件也可以用于模型的微调，但有一些限制：\n- .safetensor 文件主要是由 Hugging Face 的 Safety Checker 工具使用的，它保存了模型权重和与安全性相关的信息。\n- 虽然这个格式主要用于安全检查，但它仍然包含了模型权重，因此理论上可以用于微调。\n- 但是，由于 .safetensor 文件格式可能不如 .pth 那样被广泛支持，因此在微调时可能需要额外的步骤来转换格式或加载权重。\n**.onnx 格式：**\n.onnx 文件在微调方面的适用性较低，原因如下：\n- .onnx 文件是一个开放的、跨框架的格式，它不仅包含模型权重，还定义了模型计算图。\n- 虽然 .onnx 格式支持在不同的框架之间转换模型，但它不是为了微调而设计的。计算图是固定的，不易于修改。\n- 如果需要微调 .onnx 格式的模型，通常需要先将模型转换回原始框架（如 PyTorch 或 TensorFlow），然后再进行微调。\n- 因此，直接在 .onnx 格式上进行微调通常不切实际，除非有特定的工具或库支持这种操作。\n总结来说，.pth 格式最适合用于 PyTorch 模型的微调，因为它直接与 PyTorch 的训练和推理流程兼容。.safetensor 格式也可以用于微调，但可能需要额外的步骤来处理。而 .onnx 格式通常不直接用于微调，需要转换为其他框架格式后才能进行微调。\n\n# .pth ， .safetensor ， .onnx 三种格式，各自能否用于训练 LoRA ？为什么？\n\nLoRA（Low-Rank Adaptation）是一种用于微调大型预训练模型的技术，它通过引入低秩分解来减少微调所需的参数数量，从而减少存储和计算需求。以下是关于 .pth、.safetensor 和 .onnx 格式在训练 LoRA 时的适用性分析：\n**.pth 格式：**\n.pth 文件可以用于训练 LoRA。原因如下：\n- .pth 文件是 PyTorch 的原生格式，可以直接保存和加载模型的权重，包括 LoRA 引入的低秩矩阵。\n- 在训练 LoRA 时，通常需要在原始预训练模型的基础上添加额外的参数（即低秩矩阵），这些参数可以在 PyTorch 中通过自定义层或修改现有层来实现。\n- 由于 .pth 文件与 PyTorch 框架紧密集成，因此可以轻松地保存和加载 LoRA 的参数，并进行训练。\n**.safetensor 格式：**\n.safetensor 文件也可以用于训练 LoRA，但可能有一些限制：\n- .safetensor 文件是为了提高模型使用的安全性而设计的，它保存了模型权重和其他与安全性相关的信息。\n- 如果 LoRA 的参数被包含在 .safetensor 文件中，那么理论上可以用于训练。\n- 然而，由于 .safetensor 文件格式可能不如 .pth 那样被广泛支持，因此在训练 LoRA 时可能需要额外的步骤来处理文件格式。\n**.onnx 格式：**\n.onnx 文件在训练 LoRA 方面通常不适用。原因如下：\n- .onnx 文件定义了模型的计算图和权重，但它是一个静态的模型描述格式，不适合动态修改模型结构，如添加 LoRA 层。\n- LoRA 需要在预训练模型的特定层中插入低秩矩阵，这通常需要对模型结构进行修改，而 .onnx 格式不支持这种修改。\n- 如果需要使用 .onnx 文件中的模型来训练 LoRA，通常需要先将模型转换回 PyTorch 或其他支持动态图修改的框架，然后再进行训练。\n总结来说，.pth 格式最适合用于训练 LoRA，因为它与 PyTorch 框架紧密集成，支持动态图和自定义层的添加。.safetensor 格式在理论上也是可行的，但可能需要额外的处理步骤。而 .onnx 格式由于不支持动态修改模型结构，因此不适合直接用于训练 LoRA。\n\n# pth ， safetensor ， onnx ，三种格式如何两两转换？\n\n.pth、.safetensor 和 .onnx 文件格式之间的转换通常需要通过特定的工具和库来实现。以下是如何在这三种格式之间进行两两转换的概述：\n### .pth 到 .safetensor 的转换\n目前，并没有直接的标准工具来将 .pth 文件转换为 .safetensor 文件。但是，可以手动加载 .pth 文件中的权重，并将其保存为 .safetensor 格式。以下是一个示例步骤：\n1. 使用 PyTorch 加载 .pth 文件。\n2. 提取模型权重。\n3. 使用 Hugging Face 的 `transformers` 库或其他相关工具将权重保存为 .safetensor 文件。\n### .safetensor 到 .pth 的转换\n同样地，将 .safetensor 文件转换为 .pth 文件也没有直接的转换工具。以下是手动转换的步骤：\n1. 使用 Hugging Face 的 `transformers` 库或其他相关工具加载 .safetensor 文件。\n2. 提取模型权重。\n3. 使用 PyTorch 将权重保存为 .pth 文件。\n### .pth 到 .onnx 的转换\nPyTorch 提供了直接将模型转换为 .onnx 格式的工具：\n```python\nimport torch\nimport torch.onnx\n# 假设 model 是一个 PyTorch 模型\nmodel = ...  # 加载或定义模型\n# 假设输入是一个张量，其形状与模型输入兼容\ninput_sample = torch.randn(batch_size, channels, height, width)\n# 将模型转换为 ONNX 格式\ntorch.onnx.export(model,               # 模型\n                  input_sample,        # 模型输入（或一个包含输入的元组）\n                  \"model.onnx\",       # 输出文件名\n                  export_params=True,  # 是否导出参数\n                  opset_version=10,    # ONNX 算子集版本\n                  do_constant_folding=True,  # 是否执行常量折叠优化\n                  input_names=['input'],   # 输入名\n                  output_names=['output'], # 输出名\n                  dynamic_axes={'input': {0: 'batch_size'},  # 动态轴\n                                'output': {0: 'batch_size'}})\n```\n### .onnx 到 .pth 的转换\n将 .onnx 文件转换为 .pth 文件通常不直接进行，因为 .onnx 文件包含了模型的结构信息，而 .pth 文件只包含权重。如果需要，可以重新创建 PyTorch 模型结构，然后加载 .onnx 文件中的权重：\n1. 定义与 .onnx 文件对应的 PyTorch 模型结构。\n2. 使用 `onnx.load` 加载 .onnx 文件。\n3. 提取权重并将其加载到 PyTorch 模型中。\n### .safetensor 到 .onnx 的转换\n目前没有直接的方法将 .safetensor 文件转换为 .onnx 文件。如果需要，可以先将 .safetensor 文件转换为 .pth 文件，然后再按照上述方法将 .pth 文件转换为 .onnx 文件。\n### .onnx 到 .safetensor 的转换\n同样地，将 .onnx 文件转换为 .safetensor 文件也没有直接的转换工具。如果需要，可以先将 .onnx 文件转换为 .pth 文件，然后再将 .pth 文件转换为 .safetensor 文件。\n在进行这些转换时，需要注意的是，转换过程中可能会丢失一些特定的信息，如优化器状态或模型配置，因此在转换后可能需要额外的步骤来确保模型的完整性和正确性。\n\n# 我有一个从网络上下载的 .pth 文件 `001.pth` 。给我 Python 代码，将其转换为 onnx 格式。\n\n要将一个 `.pth` 文件转换为 `.onnx` 格式，你需要首先加载 `.pth` 文件中的模型，然后使用 PyTorch 的 `torch.onnx.export` 函数来进行转换。以下是一个 Python 代码示例，展示了如何进行这一转换过程。假设你已经有了对应的 PyTorch 模型定义，并且 `.pth` 文件中包含的是该模型的权重。\n请注意，以下代码假设你的模型是一个标准的 PyTorch 模型，并且你知道模型的输入尺��。你需要根据你的模型实际情况来调整输入尺寸和模型结构。\n```python\nimport torch\nimport torch.onnx\n# 假设模型结构和输入尺寸已知\n# 这里是示例模型结构和输入尺寸，你需要根据你的模型进行修改\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        # 定义模型结构\n        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.relu = torch.nn.ReLU()\n        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        # ... 添加更多层 ...\n    def forward(self, x):\n        # 定义前向传播\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        # ... 应用更多层 ...\n        return x\n# 创建模型实例\nmodel = MyModel()\n# 加载.pth文件中的权重\nmodel.load_state_dict(torch.load('001.pth'))\n# 设置模型为评估模式（如果适用）\nmodel.eval()\n# 创建一个符合模型输入尺寸的示例输入\n# 假设输入是单张图片，通道数为3，高和宽分别为224和224\n# 你需要根据你的模型调整输入尺寸\ninput_sample = torch.randn(1, 3, 224, 224)\n# 指定ONNX模型的输出文件名\noutput_file = 'model.onnx'\n# 导出模型为ONNX格式\ntorch.onnx.export(model,               # 模型\n                  input_sample,        # 模型输入（或一个包含输入的元组）\n                  output_file,         # 输出文件名\n                  export_params=True,  # 是否导出参数\n                  opset_version=10,    # ONNX 算子集版本\n                  do_constant_folding=True,  # 是否执行常量折叠优化\n                  input_names=['input'],   # 输入名\n                  output_names=['output'], # 输出名\n                  dynamic_axes={'input': {0: 'batch_size'},  # 动态轴\n                                'output': {0: 'batch_size'}})\n```\n在运行这段代码之前，请确保：\n- 你的环境中已经安装了 PyTorch 和 onnx。\n- 你已经根据你的模型结构正确地定义了 `MyModel` 类。\n- 你已经根据你的模型输入尺寸正确地设置了 `input_sample`。\n运行上述代码后，你应该会在当前目录下得到一个名为 `model.onnx` 的文件，这就是转换后的 ONNX 格式的模型。\n\n# 我没有对应的 PyTorch 模型定义。我要如何获得？\n\n如果你没有模型的 PyTorch 定义，但你有一个 `.pth` 文件，你可以尝试以下几种方法来获取或重建模型的结构：\n### 1. 使用 PyTorch 的 `torch.jit.load`\n如果你的 `.pth` 文件是一个 JIT traced 的模型，你可以使用 `torch.jit.load` 来加载它，这样就不需要原始的模型定义。\n```python\nimport torch\n# 加载JIT traced模型\ntraced_model = torch.jit.load('001.pth')\n# 使用模型（例如进行推理）\n# input_sample = ...  # 创建一个符合模型输入尺寸的示例输入\n# output = traced_model(input_sample)\n```\n### 2. 查找模型的文档或代码\n如果模型是从某个知名的代码库或论文中获得的，你可以尝试查找原始的模型定义代码。通常，论文的附录或代码库的 README 文件会包含模型的定义。\n### 3. 使用第三方库\n有些第三方库可能提供了预定义的模型结构，你可以尝试使用这些库来加载模型。\n例如，如果你有一个在 torchvision 中定义的模型，你可以这样做：\n```python\nimport torch\nimport torchvision.models as models\n# 假设你知道模型是 torchvision 中的某个模型\nmodel = models.resnet50(pretrained=False)\n# 加载权重\nmodel.load_state_dict(torch.load('001.pth'))\n```\n### 4. 反序列化状态字典\n如果你只是丢失了模型定义，但你有状态字典，你可以尝试反序列化状态字典来重建模型结构。\n```python\nimport torch\n# 加载状态字典\nstate_dict = torch.load('001.pth')\n# 假设你知道模型结构，可以创建一个空的模型实例，然后加载状态字典\n# model = ...  # 创建模型实例\n# model.load_state_dict(state_dict)\n```\n### 5. 使用模型架构搜索工具\n有一些工具和库可以帮助你根据权重文件推断出模型结构。例如，`netron` 是一个可视化神经网络工具，可以显示 `.pth` 文件中的模型结构。\n```bash\n# 安装 netron\npip install netron\n# 使用 netron 可视化模型\nnetron start 001.pth\n```\n通过这些方法，你应该能够找到或重建模型的结构。一旦你有了模型定义，你可以使用前面提供的代码示例来将 `.pth` 文件转换为 `.onnx` 格式。如果你无法确定模型结构，你可能需要联系模型的原始作者或查找相关的文档来获取模型定义。\n\n\n","title":"深度学习模型文件格式对比：pth、safetensor 与 onnx","abstract":"\u003e 本页面介绍了三种机器学习模型文件格式：.pth、.safetensor 和 .onnx。主要内容包括每种格式的定义、保存的内容、用途以及它们之间的区别和转换方法。\nPyTorch中的`.pth`文件通常用于保存模型的权重（parameters）和训练过程中的优化器状态（optimizer state）。具体来说，一个`.pth`文件可能包含以下内容：\n1. **模型状态字典（Model state dictionary）**：","length":322,"created_at":"2024-11-29T02:00:00.000Z","updated_at":"2024-11-29T02:00:00.000Z","tags":["PyTorch","ONNX","深度学习","模型部署","学习笔记"],"license":true}}}},"__N_SSG":true},"page":"/learn_from_ai/[slug]","query":{"slug":"opencv-coordinate-system-conventions"},"buildId":"w1ZimoTUC3z7UiVINTMsP","assetPrefix":"/blog-next","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>