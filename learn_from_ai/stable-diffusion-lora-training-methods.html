<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1" data-next-head=""/><meta property="og:image" content="https://ryojerryyu.github.io/blog-next/img/home-bg-kasumi-hanabi.jpg" data-next-head=""/><meta name="twitter:image" content="https://ryojerryyu.github.io/blog-next/img/home-bg-kasumi-hanabi.jpg" data-next-head=""/><meta property="og:url" content="https://blog.ryo-okami.xyz/learn_from_ai/stable-diffusion-lora-training-methods" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@ryo_okami" data-next-head=""/><meta name="twitter:creator" content="@ryo_okami" data-next-head=""/><link rel="icon" href="/blog-next/favicon.ico" data-next-head=""/><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests" data-next-head=""/><title data-next-head="">PyTorch 实现 Stable Diffusion LoRA 训练脚本：从数据预处理到模型修改与训练循环 | Ryo&#x27;s Blog</title><meta property="og:title" content="PyTorch 实现 Stable Diffusion LoRA 训练脚本：从数据预处理到模型修改与训练循环" data-next-head=""/><meta property="og:site_name" content="Ryo&#x27;s Blog" data-next-head=""/><meta name="twitter:title" content="PyTorch 实现 Stable Diffusion LoRA 训练脚本：从数据预处理到模型修改与训练循环 | Ryo&#x27;s Blog" data-next-head=""/><meta name="description" content="&gt; 本文详细介绍了如何使用 PyTorch 从零开始编写 Stable Diffusion LoRA 训练脚本，包括数据预处理、模型修改、训练循环、参数保存与加载等关键步骤。特别强调了 LoRA 层的手动实现和在 UNet 的 Cross-Attention 层注入 LoRA 的原因，以及在其他层应用 LoRA 的可能性和注意事项。此外，还提供了代码示例和参数效率的讨论，帮助读者深入理解 LoRA 在 Stable Diffusion 微调中的应用。
&gt; [!reasoning]-
&gt;" data-next-head=""/><meta property="og:description" content="&gt; 本文详细介绍了如何使用 PyTorch 从零开始编写 Stable Diffusion LoRA 训练脚本，包括数据预处理、模型修改、训练循环、参数保存与加载等关键步骤。特别强调了 LoRA 层的手动实现和在 UNet 的 Cross-Attention 层注入 LoRA 的原因，以及在其他层应用 LoRA 的可能性和注意事项。此外，还提供了代码示例和参数效率的讨论，帮助读者深入理解 LoRA 在 Stable Diffusion 微调中的应用。
&gt; [!reasoning]-
&gt;" data-next-head=""/><meta name="twitter:description" content="&gt; 本文详细介绍了如何使用 PyTorch 从零开始编写 Stable Diffusion LoRA 训练脚本，包括数据预处理、模型修改、训练循环、参数保存与加载等关键步骤。特别强调了 LoRA 层的手动实现和在 UNet 的 Cross-Attention 层注入 LoRA 的原因，以及在其他层应用 LoRA 的可能性和注意事项。此外，还提供了代码示例和参数效率的讨论，帮助读者深入理解 LoRA 在 Stable Diffusion 微调中的应用。
&gt; [!reasoning]-
&gt;" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="article:published_time" content="2025-03-29T02:00:00.000Z" data-next-head=""/><meta property="article:modified_time" content="2025-03-29T02:00:00.000Z" data-next-head=""/><meta property="article:tag" content="深度学习" data-next-head=""/><meta property="article:tag" content="Stable Diffusion" data-next-head=""/><meta property="article:tag" content="LoRA" data-next-head=""/><meta property="article:tag" content="参数高效微调" data-next-head=""/><meta property="article:tag" content="模型训练" data-next-head=""/><meta property="article:tag" content="学习笔记" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="192x192" href="/android-chrome-192x192.png"/><link rel="manifest" href="/site.webmanifest"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/blog-next/_next/static/css/8bc897d2815c155b.css" as="style"/><link rel="preload" href="/blog-next/_next/static/css/1fe12698870890cf.css" as="style"/><link rel="stylesheet" href="/blog-next/_next/static/css/8bc897d2815c155b.css" data-n-g=""/><link rel="stylesheet" href="/blog-next/_next/static/css/1fe12698870890cf.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/blog-next/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/blog-next/_next/static/chunks/webpack-0838eb7d41561e24.js" defer=""></script><script src="/blog-next/_next/static/chunks/framework-d33e0fe36067854f.js" defer=""></script><script src="/blog-next/_next/static/chunks/main-dea36a7529be985a.js" defer=""></script><script src="/blog-next/_next/static/chunks/pages/_app-0fb7c4bae3c90fb5.js" defer=""></script><script src="/blog-next/_next/static/chunks/6d2b60a9-09feccc7018ac291.js" defer=""></script><script src="/blog-next/_next/static/chunks/52d06cd5-4b5b522181081dc4.js" defer=""></script><script src="/blog-next/_next/static/chunks/5342-0e842ae0467f6356.js" defer=""></script><script src="/blog-next/_next/static/chunks/4587-c57308f349adb436.js" defer=""></script><script src="/blog-next/_next/static/chunks/878-eed54f05f61d9289.js" defer=""></script><script src="/blog-next/_next/static/chunks/8315-ca0cea8fcd39047d.js" defer=""></script><script src="/blog-next/_next/static/chunks/9474-5c91fbd5a8098aa7.js" defer=""></script><script src="/blog-next/_next/static/chunks/pages/learn_from_ai/%5Bslug%5D-60c5be3e9c114dc0.js" defer=""></script><script src="/blog-next/_next/static/1A7lYf5o3SCViJReqMN9a/_buildManifest.js" defer=""></script><script src="/blog-next/_next/static/1A7lYf5o3SCViJReqMN9a/_ssgManifest.js" defer=""></script></head><body><div id="__next"><style data-emotion="css czlpqi">.css-czlpqi{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;box-sizing:border-box;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;position:fixed;z-index:1100;top:0;left:auto;right:0;--AppBar-background:#1976d2;--AppBar-color:#fff;background-color:var(--AppBar-background);color:var(--AppBar-color);background-color:rgba(15, 23, 42, 0.75);}@media print{.css-czlpqi{position:absolute;}}</style><style data-emotion="css 1cmpeoq">.css-1cmpeoq{background-color:#fff;color:rgba(0, 0, 0, 0.87);-webkit-transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:box-shadow 300ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;box-shadow:var(--Paper-shadow);background-image:var(--Paper-overlay);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:100%;box-sizing:border-box;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;position:fixed;z-index:1100;top:0;left:auto;right:0;--AppBar-background:#1976d2;--AppBar-color:#fff;background-color:var(--AppBar-background);color:var(--AppBar-color);background-color:rgba(15, 23, 42, 0.75);}@media print{.css-1cmpeoq{position:absolute;}}</style><header class="MuiPaper-root MuiPaper-elevation MuiPaper-elevation4 MuiAppBar-root MuiAppBar-colorPrimary MuiAppBar-positionFixed mui-fixed css-1cmpeoq" style="--Paper-shadow:0px 2px 4px -1px rgba(0,0,0,0.2),0px 4px 5px 0px rgba(0,0,0,0.14),0px 1px 10px 0px rgba(0,0,0,0.12)"><style data-emotion="css awgou1">.css-awgou1{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:16px;padding-right:16px;min-height:56px;}@media (min-width:600px){.css-awgou1{padding-left:24px;padding-right:24px;}}@media (min-width:0px){@media (orientation: landscape){.css-awgou1{min-height:48px;}}}@media (min-width:600px){.css-awgou1{min-height:64px;}}</style><div class="MuiToolbar-root MuiToolbar-gutters MuiToolbar-regular css-awgou1"><style data-emotion="css 1guk29">@media (min-width:0px){.css-1guk29{display:none;}}@media (min-width:900px){.css-1guk29{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><div class="ml-2 w-24 mr-4 MuiBox-root css-1guk29"><a class="DefaultLayout_textlink__W55gl" href="/blog-next">Ryo&#x27;s Blog</a></div><style data-emotion="css 1m04nb5">@media (min-width:0px){.css-1m04nb5{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}@media (min-width:900px){.css-1m04nb5{display:none;}}</style><div class="ml-2 mr-4 MuiBox-root css-1m04nb5"><a title="Ryo&#x27;s Blog" href="/blog-next"><style data-emotion="css q7mezt">.css-q7mezt{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;fill:currentColor;font-size:1.5rem;}</style><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium h-6 w-6 text-gray-300 hover:text-white css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M10 20v-6h4v6h5v-8h3L12 3 2 12h3v8z"></path></svg></a></div><style data-emotion="css nznm6s">.css-nznm6s{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}</style><div class="MuiBox-root css-nznm6s"><div class="DefaultLayoutMenu bg-transparent min-w-full"><ul class="rc-menu-overflow rc-menu rc-menu-root rc-menu-horizontal" role="menu" tabindex="0" data-menu-list="true"><li class="rc-menu-overflow-item rc-menu-item" style="opacity:1;order:0" role="menuitem" tabindex="-1"><a class="DefaultLayout_textlink__W55gl" href="/blog-next/articles">Articles</a></li><li class="rc-menu-overflow-item rc-menu-item" style="opacity:1;order:1" role="menuitem" tabindex="-1"><a class="DefaultLayout_textlink__W55gl" href="/blog-next/learn_from_ai">Learn from AI</a></li><li class="rc-menu-overflow-item rc-menu-item" style="opacity:1;order:2" role="menuitem" tabindex="-1"><a class="DefaultLayout_textlink__W55gl" href="/blog-next/tags">Tags</a></li><li class="rc-menu-overflow-item rc-menu-submenu rc-menu-submenu-horizontal" style="opacity:1;order:3" role="none"><div role="menuitem" class="rc-menu-submenu-title" tabindex="-1" aria-expanded="false" aria-haspopup="true"><span class="DefaultLayout_textlink__W55gl">More</span><i class="rc-menu-submenu-arrow"></i></div></li><li class="rc-menu-overflow-item rc-menu-overflow-item-rest rc-menu-submenu rc-menu-submenu-horizontal" style="opacity:0;height:0;overflow-y:hidden;order:9007199254740991;pointer-events:none;position:absolute" aria-hidden="true" role="none"><div role="menuitem" class="rc-menu-submenu-title" tabindex="-1" title="..." aria-expanded="false" aria-haspopup="true">...<i class="rc-menu-submenu-arrow"></i></div></li></ul><div style="display:none" aria-hidden="true"></div></div></div><style data-emotion="css k008qs">.css-k008qs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><div class="MuiBox-root css-k008qs"><a title="Twitter" href="https://twitter.com/ryo_okami"><svg class="h-6 w-6 fill-gray-300 hover:fill-white mx-1 sm:mx-2" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a title="GitHub" href="https://github.com/RyoJerryYu"><svg class="h-6 w-6 fill-gray-300 hover:fill-white mx-1 sm:mx-2" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a title="Pixiv" href="https://www.pixiv.net/users/9159893"><svg class="h-6 w-6 fill-gray-300 hover:fill-white mx-1 sm:mx-2" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4.935 0A4.924 4.924 0 0 0 0 4.935v14.13A4.924 4.924 0 0 0 4.935 24h14.13A4.924 4.924 0 0 0 24 19.065V4.935A4.924 4.924 0 0 0 19.065 0zm7.81 4.547c2.181 0 4.058.676 5.399 1.847a6.118 6.118 0 0 1 2.116 4.66c.005 1.854-.88 3.476-2.257 4.563-1.375 1.092-3.225 1.697-5.258 1.697-2.314 0-4.46-.842-4.46-.842v2.718c.397.116 1.048.365.635.779H5.79c-.41-.41.19-.65.644-.779V7.666c-1.053.81-1.593 1.51-1.868 2.031.32 1.02-.284.969-.284.969l-1.09-1.73s3.868-4.39 9.553-4.39zm-.19.971c-1.423-.003-3.184.473-4.27 1.244v8.646c.988.487 2.484.832 4.26.832h.01c1.596 0 2.98-.593 3.93-1.533.952-.948 1.486-2.183 1.492-3.683-.005-1.54-.504-2.864-1.42-3.86-.918-.992-2.274-1.645-4.002-1.646Z"></path></svg></a></div></div></header><style data-emotion="css awgou1">.css-awgou1{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:16px;padding-right:16px;min-height:56px;}@media (min-width:600px){.css-awgou1{padding-left:24px;padding-right:24px;}}@media (min-width:0px){@media (orientation: landscape){.css-awgou1{min-height:48px;}}}@media (min-width:600px){.css-awgou1{min-height:64px;}}</style><div class="MuiToolbar-root MuiToolbar-gutters MuiToolbar-regular css-awgou1"></div><style data-emotion="css vktxal">.css-vktxal{--Grid-columns:12;--Grid-columnSpacing:0px;--Grid-rowSpacing:0px;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;min-width:0;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:var(--Grid-rowSpacing) var(--Grid-columnSpacing);width:100%;max-width:80rem;margin-left:auto;margin-right:auto;padding:0.5rem;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}.css-vktxal >*{--Grid-parent-columns:12;}.css-vktxal >*{--Grid-parent-columnSpacing:0px;}.css-vktxal >*{--Grid-parent-rowSpacing:0px;}</style><div class="MuiGrid-root MuiGrid-container MuiGrid-direction-xs-row css-vktxal"><style data-emotion="css 9gdssj">.css-9gdssj{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 0 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 0) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));min-width:0;box-sizing:border-box;}@media (min-width:900px){.css-9gdssj{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 0 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 0) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}@media (min-width:1200px){.css-9gdssj{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 2 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 2) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}</style><div class="MuiGrid-root MuiGrid-direction-xs-row MuiGrid-grid-xs-0 MuiGrid-grid-md-0 MuiGrid-grid-lg-2 css-9gdssj"></div><style data-emotion="css 9h67uz">.css-9h67uz{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 12 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 12) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));min-width:0;box-sizing:border-box;}@media (min-width:900px){.css-9h67uz{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 9 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 9) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}@media (min-width:1200px){.css-9h67uz{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 8 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 8) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}</style><div class="MuiGrid-root MuiGrid-direction-xs-row MuiGrid-grid-xs-12 MuiGrid-grid-md-9 MuiGrid-grid-lg-8 css-9h67uz"><div class="DefaultLayout_contentHeight__RDRZE"><article class="Post_post__acRqJ"><h1 class="Post_postTitle__N1NIA">PyTorch 实现 Stable Diffusion LoRA 训练脚本：从数据预处理到模型修改与训练循环</h1><div class="Post_postDate__SQx7A"><time dateTime="2025-03-29T02:00:00.000Z">2025-03-29</time></div><div class="TagsBox_tagsBox__WzhAf mt-2"><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">#<!-- -->深度学习</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/stable-diffusion">#<!-- -->Stable Diffusion</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/lora">#<!-- -->LoRA</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83">#<!-- -->参数高效微调</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">#<!-- -->模型训练</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0">#<!-- -->学习笔记</a></div><div class="post-body Post_postContent__mJ_Ju"><blockquote>
<p>本文详细介绍了如何使用 PyTorch 从零开始编写 Stable Diffusion LoRA 训练脚本，包括数据预处理、模型修改、训练循环、参数保存与加载等关键步骤。特别强调了 LoRA 层的手动实现和在 UNet 的 Cross-Attention 层注入 LoRA 的原因，以及在其他层应用 LoRA 的可能性和注意事项。此外，还提供了代码示例和参数效率的讨论，帮助读者深入理解 LoRA 在 Stable Diffusion 微调中的应用。</p>
</blockquote>
<h2 id="q-训练一个-stable-diffusion-的-lora-模型要有哪些步骤"><a href="#q-训练一个-stable-diffusion-的-lora-模型要有哪些步骤">Q: 训练一个 Stable Diffusion 的 LoRA 模型，要有哪些步骤？</a></h2>
<div class="rounded-md p-2 my-4 bg-sky-100"><div class="text-base font-bold text-sky-500 cursor-pointer"><style data-emotion="css q7mezt">.css-q7mezt{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;fill:currentColor;font-size:1.5rem;}</style><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="m15.82 7.22-1 .4c-.21-.16-.43-.29-.67-.39L14 6.17c-.02-.1-.1-.17-.2-.17h-1.6c-.1 0-.18.07-.19.17l-.15 1.06c-.24.1-.47.23-.67.39l-1-.4c-.09-.03-.2 0-.24.09l-.8 1.38c-.05.09-.03.2.05.26l.85.66c-.03.12-.05.26-.05.39q0 .195.03.39l-.84.66c-.08.06-.1.17-.05.25l.8 1.39c.05.09.15.12.25.09l.99-.4c.21.16.43.29.68.39l.14 1.06c.02.1.1.17.2.17h1.6c.1 0 .18-.07.2-.17l.15-1.06c.24-.1.47-.23.67-.39l.99.4c.09.04.2 0 .24-.09l.8-1.39c.05-.09.03-.19-.05-.25l-.83-.66q.03-.195.03-.39 0-.21-.03-.39l.85-.66c.08-.06.1-.17.05-.26l-.8-1.38c-.05-.09-.16-.12-.25-.09M13 11.43c-.79 0-1.43-.64-1.43-1.43s.64-1.43 1.43-1.43 1.43.64 1.43 1.43-.64 1.43-1.43 1.43"></path><path d="M19.94 9.06c-.43-3.27-3.23-5.86-6.53-6.05C13.27 3 13.14 3 13 3 9.47 3 6.57 5.61 6.08 9l-1.93 3.48c-.41.66.07 1.52.85 1.52h1v2c0 1.1.9 2 2 2h1v3h7v-4.68c2.62-1.25 4.35-4.08 3.94-7.26m-5.05 5.57-.89.42V19h-3v-3H8v-4H6.7l1.33-2.33C8.21 7.06 10.35 5 13 5c2.76 0 5 2.24 5 5 0 2.09-1.29 3.88-3.11 4.63"></path></svg> <!-- -->Reasoning<!-- --> <svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M8.59 16.59 13.17 12 8.59 7.41 10 6l6 6-6 6z"></path></svg></div></div>
<p>训练 Stable Diffusion 的 LoRA（Low-Rank Adaptation）模型是一种高效的微调方法，可以在保留原模型大部分参数的同时，通过低秩矩阵调整模型行为。以下是详细的步骤指南：</p>
<hr/>
<h3 id="1-数据准备"><a href="#1-数据准备"><strong>1. 数据准备</strong></a></h3>
<ul>
<li><strong>收集数据集</strong>:<!-- -->
<ul>
<li>需要与目标风格/主题相关的高质量图片（建议至少 <strong>100-200 张</strong>，数量越多效果越好）。</li>
<li>图片格式通常为 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>.jpg</span></span></code></span> 或 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>.png</span></span></code></span>，分辨率建议与 Stable Diffusion 的基础模型对齐（如 512x512）。</li>
</ul>
</li>
<li><strong>标注数据</strong>:<!-- -->
<ul>
<li>每张图片需搭配文本描述（<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>.txt</span></span></code></span> 文件），描述需清晰简洁（如 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>&quot;a photo of a [keyword], [style], [details]&quot;</span></span></code></span>）。</li>
<li>若训练特定对象（如人物、画风），确保描述中<strong>统一使用触发词</strong>（如 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>&quot;my_cat_v1&quot;</span></span></code></span>）。</li>
</ul>
</li>
</ul>
<hr/>
<h3 id="2-环境配置"><a href="#2-环境配置"><strong>2. 环境配置</strong></a></h3>
<ul>
<li>
<p><strong>安装依赖库</strong>:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme="plastic"><code data-language="bash" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#5F6672;font-style:italic"># 基础环境</span></span>
<span data-line=""><span style="color:#B57EDC">pip</span><span style="color:#98C379"> install</span><span style="color:#98C379"> torch</span><span style="color:#98C379"> torchvision</span><span style="color:#56B6C2"> --extra-index-url</span><span style="color:#98C379"> https://download.pytorch.org/whl/cu117</span></span>
<span data-line=""><span style="color:#B57EDC">pip</span><span style="color:#98C379"> install</span><span style="color:#98C379"> diffusers</span><span style="color:#98C379"> transformers</span><span style="color:#98C379"> accelerate</span><span style="color:#98C379"> peft</span></span>
<span data-line=""><span style="color:#B57EDC">pip</span><span style="color:#98C379"> install</span><span style="color:#98C379"> datasets</span><span style="color:#98C379"> huggingface_hub</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 可选：xformers 优化（A卡需调整）</span></span>
<span data-line=""><span style="color:#B57EDC">pip</span><span style="color:#98C379"> install</span><span style="color:#98C379"> xformers</span></span></code></pre></figure>
</li>
<li>
<p><strong>硬件要求</strong>:</p>
<ul>
<li>GPU 显存至少 <strong>8GB</strong>（如 RTX 3060/4090），24GB 可支持更大批次。</li>
<li>使用 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>accelerate</span></span></code></span> 库配置分布式训练（可选）。</li>
</ul>
</li>
</ul>
<hr/>
<h3 id="3-模型准备"><a href="#3-模型准备"><strong>3. 模型准备</strong></a></h3>
<ul>
<li>
<p><strong>选择基础模型</strong>:</p>
<ul>
<li>常用模型：<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>runwayml/stable-diffusion-v1-5</span></span></code></span>、<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>stabilityai/stable-diffusion-2-1</span></span></code></span>。</li>
<li>从 Hugging Face 下载并缓存模型：<!-- -->
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E06C75">from</span><span style="color:#A9B2C3"> diffusers </span><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> StableDiffusionPipeline</span></span>
<span data-line=""><span style="color:#A9B2C3">pipeline </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> StableDiffusionPipeline.from_pretrained(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">runwayml/stable-diffusion-v1-5</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span></code></pre></figure>
</li>
</ul>
</li>
<li>
<p><strong>配置 LoRA 参数</strong>:</p>
<ul>
<li><strong>Rank（秩）</strong>: 控制低秩矩阵的维度（典型值：4-64，越大拟合能力越强，但可能过拟合）。</li>
<li>在训练脚本中设置 LoRA 层（通常作用于 Cross-Attention 模块）：<!-- -->
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E06C75">from</span><span style="color:#A9B2C3"> peft </span><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> LoraConfig</span></span>
<span data-line=""><span style="color:#A9B2C3">lora_config </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> LoraConfig(</span></span>
<span data-line=""><span style="color:#C6CCD7">    r</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">8</span><span style="color:#A9B2C3">,                </span><span style="color:#5F6672;font-style:italic"># Rank 值</span></span>
<span data-line=""><span style="color:#C6CCD7">    target_modules</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">[</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">to_k</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">, </span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">to_q</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">, </span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">to_v</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">, </span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">to_out.0</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">],  </span><span style="color:#5F6672;font-style:italic"># 注入的模块</span></span>
<span data-line=""><span style="color:#C6CCD7">    init_lora_weights</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">gaussian</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">,</span></span>
<span data-line=""><span style="color:#A9B2C3">)</span></span></code></pre></figure>
</li>
</ul>
</li>
</ul>
<hr/>
<h3 id="4-训练配置"><a href="#4-训练配置"><strong>4. 训练配置</strong></a></h3>
<ul>
<li>
<p><strong>超参数设置</strong>:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="yaml" data-theme="plastic"><code data-language="yaml" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E5C07B">batch_size</span><span style="color:#A9B2C3">: </span><span style="color:#98C379">1-4（根据显存调整）</span></span>
<span data-line=""><span style="color:#E5C07B">learning_rate</span><span style="color:#A9B2C3">: </span><span style="color:#98C379">1e-5 到 1e-4（常用 1e-4）</span></span>
<span data-line=""><span style="color:#E5C07B">num_epochs</span><span style="color:#A9B2C3">: </span><span style="color:#98C379">50-200（观察 loss 调整）</span></span>
<span data-line=""><span style="color:#E5C07B">optimizer</span><span style="color:#A9B2C3">: </span><span style="color:#98C379">AdamW（带权重衰减）</span></span>
<span data-line=""><span style="color:#E5C07B">mixed_precision</span><span style="color:#A9B2C3">: </span><span style="color:#98C379">fp16（节省显存）</span></span>
<span data-line=""><span style="color:#E5C07B">gradient_checkpointing</span><span style="color:#A9B2C3">: </span><span style="color:#98C379">True（进一步省显存）</span></span></code></pre></figure>
</li>
<li>
<p><strong>训练脚本</strong>:</p>
<ul>
<li>使用 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>diffusers</span></span></code></span> 的 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>train_text_to_image.py</span></span></code></span> 或自定义训练循环。</li>
<li>关键代码片段：<!-- -->
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E06C75">from</span><span style="color:#A9B2C3"> diffusers </span><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> DDPMScheduler</span></span>
<span data-line=""><span style="color:#A9B2C3">scheduler </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> DDPMScheduler.from_pretrained(model_id, </span><span style="color:#C6CCD7">subfolder</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">scheduler</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#A9B2C3">trainer </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> StableDiffusionTrainer(</span></span>
<span data-line=""><span style="color:#C6CCD7">    model</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">pipeline.unet,</span></span>
<span data-line=""><span style="color:#C6CCD7">    args</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">training_args,</span></span>
<span data-line=""><span style="color:#C6CCD7">    data_collator</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">collate_fn,</span></span>
<span data-line=""><span style="color:#C6CCD7">    train_dataset</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">dataset,</span></span>
<span data-line=""><span style="color:#C6CCD7">    scheduler</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">scheduler,</span></span>
<span data-line=""><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#A9B2C3">trainer.add_callback(LoRACallback(lora_config))  </span><span style="color:#5F6672;font-style:italic"># 注入 LoRA</span></span></code></pre></figure>
</li>
</ul>
</li>
</ul>
<hr/>
<h3 id="5-训练过程"><a href="#5-训练过程"><strong>5. 训练过程</strong></a></h3>
<ul>
<li>
<p><strong>监控与调试</strong>:</p>
<ul>
<li>观察 Loss 曲线（理想情况应稳步下降后趋于平稳）。</li>
<li>定期生成测试图片（如每 100 步）检查过拟合/欠拟合。</li>
<li>调整策略：<!-- -->
<ul>
<li>Loss 不降：提高学习率、增加 Rank 值。</li>
<li>Loss 震荡：降低学习率、增大批次大小。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>保存检查点</strong>:</p>
<ul>
<li>每隔一定步数保存中间模型（如 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>./output/checkpoint-500</span></span></code></span>）。</li>
<li>最终模型保存为 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>.safetensors</span></span></code></span> 或合并到原模型。</li>
</ul>
</li>
</ul>
<hr/>
<h3 id="6-推理与部署"><a href="#6-推理与部署"><strong>6. 推理与部署</strong></a></h3>
<ul>
<li>
<p><strong>加载 LoRA 权重</strong>:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E06C75">from</span><span style="color:#A9B2C3"> diffusers </span><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> StableDiffusionPipeline</span></span>
<span data-line=""><span style="color:#A9B2C3">pipeline </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> StableDiffusionPipeline.from_pretrained(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">runwayml/stable-diffusion-v1-5</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#A9B2C3">pipeline.unet.load_attn_procs(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">path/to/lora_weights.safetensors</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span></code></pre></figure>
</li>
<li>
<p><strong>生成图片</strong>:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#A9B2C3">prompt </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> &quot;</span><span style="color:#98C379">A photo of my_cat_v1 sitting on a sofa, best quality</span><span style="color:#A9B2C3">&quot;</span></span>
<span data-line=""><span style="color:#A9B2C3">image </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> pipeline(prompt, </span><span style="color:#C6CCD7">num_inference_steps</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">50</span><span style="color:#A9B2C3">).images[</span><span style="color:#56B6C2">0</span><span style="color:#A9B2C3">]</span></span>
<span data-line=""><span style="color:#A9B2C3">image.save(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">output.png</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span></code></pre></figure>
</li>
</ul>
<hr/>
<h3 id="常见问题"><a href="#常见问题"><strong>常见问题</strong></a></h3>
<ul>
<li>
<p><strong>Q</strong>: 需要多少数据？</p>
<ul>
<li><strong>A</strong>: 简单风格/物体约 100 张，复杂概念需 500+ 张。</li>
</ul>
</li>
<li>
<p><strong>Q</strong>: 训练时间多久？</p>
<ul>
<li><strong>A</strong>: 单 GPU 约 1-6 小时（取决于数据量和参数）。</li>
</ul>
</li>
<li>
<p><strong>Q</strong>: 能否用 Colab 免费版？</p>
<ul>
<li><strong>A</strong>: 可以，但需启用 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>fp16</span></span></code></span> 和梯度检查点，批次设为 1。</li>
</ul>
</li>
</ul>
<hr/>
<p>通过以上步骤，你可以高效地训练出针对特定任务的 LoRA 模型，显著降低资源消耗同时保持生成质量。</p>
<div class="rounded-md p-2 my-4 bg-sky-100"><div class="text-base font-bold text-sky-500"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M11 7h2v2h-2zm0 4h2v6h-2zm1-9C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2m0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8"></path></svg> <!-- -->关于数据量的疑问<!-- --> </div><div class="m-2"><div class="rounded-md p-2 my-4 bg-fuchsia-100"><div class="text-base font-bold text-fuchsia-500"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M11 18h2v-2h-2zm1-16C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2m0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8m0-14c-2.21 0-4 1.79-4 4h2c0-1.1.9-2 2-2s2 .9 2 2c0 2-3 1.75-3 5h2c0-2.25 3-2.5 3-5 0-2.21-1.79-4-4-4"></path></svg> <!-- -->Query<!-- --> </div><div class="m-2"><p>你确定训练一个 LoRA 需要 100 多张图吗？</p></div></div><p>Reasoning 过程省略</p><div class="rounded-md p-2 my-4 bg-indigo-100"><div class="text-base font-bold text-indigo-500"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M20 9V7c0-1.1-.9-2-2-2h-3c0-1.66-1.34-3-3-3S9 3.34 9 5H6c-1.1 0-2 .9-2 2v2c-1.66 0-3 1.34-3 3s1.34 3 3 3v4c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2v-4c1.66 0 3-1.34 3-3s-1.34-3-3-3m-2 10H6V7h12zm-9-6c-.83 0-1.5-.67-1.5-1.5S8.17 10 9 10s1.5.67 1.5 1.5S9.83 13 9 13m7.5-1.5c0 .83-.67 1.5-1.5 1.5s-1.5-.67-1.5-1.5.67-1.5 1.5-1.5 1.5.67 1.5 1.5M8 15h8v2H8z"></path></svg> <!-- -->Ai<!-- --> </div><div class="m-2"><p>训练 LoRA 模型所需的图片数量并非固定，具体取决于训练目标、数据质量和参数设置。根据多个来源的建议，最低可接受数量为 10-20 张高质量图片，而复杂场景或高精度需求可能需要 50 张以上。
...</p></div></div></div></div>
<h2 id="q-我有-python-与-pytorch-基础需要自己写一个-lora-训练脚本应如何写"><a href="#q-我有-python-与-pytorch-基础需要自己写一个-lora-训练脚本应如何写">Q: 我有 Python 与 PyTorch 基础。需要自己写一个 LoRA 训练脚本，应如何写？</a></h2>
<div class="rounded-md p-2 my-4 bg-sky-100"><div class="text-base font-bold text-sky-500 cursor-pointer"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="m15.82 7.22-1 .4c-.21-.16-.43-.29-.67-.39L14 6.17c-.02-.1-.1-.17-.2-.17h-1.6c-.1 0-.18.07-.19.17l-.15 1.06c-.24.1-.47.23-.67.39l-1-.4c-.09-.03-.2 0-.24.09l-.8 1.38c-.05.09-.03.2.05.26l.85.66c-.03.12-.05.26-.05.39q0 .195.03.39l-.84.66c-.08.06-.1.17-.05.25l.8 1.39c.05.09.15.12.25.09l.99-.4c.21.16.43.29.68.39l.14 1.06c.02.1.1.17.2.17h1.6c.1 0 .18-.07.2-.17l.15-1.06c.24-.1.47-.23.67-.39l.99.4c.09.04.2 0 .24-.09l.8-1.39c.05-.09.03-.19-.05-.25l-.83-.66q.03-.195.03-.39 0-.21-.03-.39l.85-.66c.08-.06.1-.17.05-.26l-.8-1.38c-.05-.09-.16-.12-.25-.09M13 11.43c-.79 0-1.43-.64-1.43-1.43s.64-1.43 1.43-1.43 1.43.64 1.43 1.43-.64 1.43-1.43 1.43"></path><path d="M19.94 9.06c-.43-3.27-3.23-5.86-6.53-6.05C13.27 3 13.14 3 13 3 9.47 3 6.57 5.61 6.08 9l-1.93 3.48c-.41.66.07 1.52.85 1.52h1v2c0 1.1.9 2 2 2h1v3h7v-4.68c2.62-1.25 4.35-4.08 3.94-7.26m-5.05 5.57-.89.42V19h-3v-3H8v-4H6.7l1.33-2.33C8.21 7.06 10.35 5 13 5c2.76 0 5 2.24 5 5 0 2.09-1.29 3.88-3.11 4.63"></path></svg> <!-- -->Reasoning<!-- --> <svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M8.59 16.59 13.17 12 8.59 7.41 10 6l6 6-6 6z"></path></svg></div></div>
<p>以下是基于 PyTorch 实现 Stable Diffusion LoRA 训练脚本的完整指南，从底层原理到代码实现逐步解析。假设你已准备好数据集（图片+文本描述），我们将以 <strong>手动实现 LoRA 层</strong> 为核心，不依赖 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>peft</span></span></code></span> 库。</p>
<hr/>
<h3 id="1-lora-层的原理与实现"><a href="#1-lora-层的原理与实现">1. <strong>LoRA 层的原理与实现</strong></a></h3>
<p>LoRA 的核心思想：在原始权重矩阵旁增加一个低秩矩阵，只训练这部分参数。</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> torch</span></span>
<span data-line=""><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> torch.nn </span><span style="color:#E06C75">as</span><span style="color:#A9B2C3"> nn</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#61AFEF">class</span><span style="color:#E5C07B"> LoRALayer</span><span style="color:#A9B2C3">(</span><span style="color:#D19A66">nn</span><span style="color:#A9B2C3">.</span><span style="color:#D19A66">Module</span><span style="color:#A9B2C3">):</span></span>
<span data-line=""><span style="color:#61AFEF">    def</span><span style="color:#B57EDC"> __init__</span><span style="color:#A9B2C3">(</span><span style="color:#C6CCD7">self</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">original_layer</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">rank</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">8</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">alpha</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">16</span><span style="color:#A9B2C3">):</span></span>
<span data-line=""><span style="color:#E5C07B">        super</span><span style="color:#A9B2C3">().</span><span style="color:#B57EDC">__init__</span><span style="color:#A9B2C3">()</span></span>
<span data-line=""><span style="color:#E06C75">        self</span><span style="color:#A9B2C3">.original </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> original_layer  </span><span style="color:#5F6672;font-style:italic"># 原始线性层 (e.g., nn.Linear)</span></span>
<span data-line=""><span style="color:#E06C75">        self</span><span style="color:#A9B2C3">.original.requires_grad_(</span><span style="color:#56B6C2">False</span><span style="color:#A9B2C3">)  </span><span style="color:#5F6672;font-style:italic"># 冻结原始参数</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#A9B2C3">        in_dim </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> original_layer.in_features</span></span>
<span data-line=""><span style="color:#A9B2C3">        out_dim </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> original_layer.out_features</span></span>
<span data-line=""><span style="color:#E06C75">        self</span><span style="color:#A9B2C3">.rank </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> rank</span></span>
<span data-line=""><span style="color:#E06C75">        self</span><span style="color:#A9B2C3">.alpha </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> alpha</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic">        # 初始化低秩矩阵 A 和 B</span></span>
<span data-line=""><span style="color:#E06C75">        self</span><span style="color:#A9B2C3">.A </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> nn.Parameter(torch.randn(in_dim, rank))</span></span>
<span data-line=""><span style="color:#E06C75">        self</span><span style="color:#A9B2C3">.B </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> nn.Parameter(torch.zeros(rank, out_dim))</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#61AFEF">    def</span><span style="color:#B57EDC"> forward</span><span style="color:#A9B2C3">(</span><span style="color:#C6CCD7">self</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">x</span><span style="color:#A9B2C3">):</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">        # 原始输出 + 低秩增量</span></span>
<span data-line=""><span style="color:#A9B2C3">        orig_out </span><span style="color:#E06C75">=</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.original(x)</span></span>
<span data-line=""><span style="color:#A9B2C3">        lora_out </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> x </span><span style="color:#E06C75">@</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.A </span><span style="color:#E06C75">@</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.B  </span><span style="color:#5F6672;font-style:italic"># [batch, out_dim]</span></span>
<span data-line=""><span style="color:#E06C75">        return</span><span style="color:#A9B2C3"> orig_out </span><span style="color:#E06C75">+</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.alpha </span><span style="color:#E06C75">*</span><span style="color:#A9B2C3"> lora_out</span></span></code></pre></figure>
<hr/>
<h3 id="2-修改-stable-diffusion-的-unet"><a href="#2-修改-stable-diffusion-的-unet">2. <strong>修改 Stable Diffusion 的 UNet</strong></a></h3>
<p>需将 LoRA 注入到 UNet 的 <strong>Cross-Attention 层</strong>（关键步骤）：</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E06C75">from</span><span style="color:#A9B2C3"> diffusers </span><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> UNet2DConditionModel</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#61AFEF">def</span><span style="color:#B57EDC"> inject_lora</span><span style="color:#A9B2C3">(</span><span style="color:#C6CCD7">unet</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">rank</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">8</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">alpha</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">16</span><span style="color:#A9B2C3">):</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">    # 遍历 UNet 的所有注意力层</span></span>
<span data-line=""><span style="color:#E06C75">    for</span><span style="color:#A9B2C3"> name, module </span><span style="color:#E06C75">in</span><span style="color:#A9B2C3"> unet.named_modules():</span></span>
<span data-line=""><span style="color:#E06C75">        if</span><span style="color:#A9B2C3"> &quot;</span><span style="color:#98C379">attn2</span><span style="color:#A9B2C3">&quot;</span><span style="color:#E06C75"> in</span><span style="color:#A9B2C3"> name </span><span style="color:#E06C75">and</span><span style="color:#A9B2C3"> &quot;</span><span style="color:#98C379">to_</span><span style="color:#A9B2C3">&quot;</span><span style="color:#E06C75"> in</span><span style="color:#A9B2C3"> name:  </span><span style="color:#5F6672;font-style:italic"># 定位 Cross-Attention 的 Q/K/V/Out 层</span></span>
<span data-line=""><span style="color:#E06C75">            if</span><span style="color:#B57EDC"> isinstance</span><span style="color:#A9B2C3">(module, nn.Linear):</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">                # 替换原始 Linear 层为 LoRA 层</span></span>
<span data-line=""><span style="color:#A9B2C3">                lora_layer </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> LoRALayer(module, </span><span style="color:#C6CCD7">rank</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">rank, </span><span style="color:#C6CCD7">alpha</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">alpha)</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">                # 通过字符串路径替换模块 (需要解析层级结构)</span></span>
<span data-line=""><span style="color:#A9B2C3">                parent </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> unet</span></span>
<span data-line=""><span style="color:#A9B2C3">                parts </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> name.split(</span><span style="color:#A9B2C3">&#x27;</span><span style="color:#98C379">.</span><span style="color:#A9B2C3">&#x27;</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#E06C75">                for</span><span style="color:#A9B2C3"> part </span><span style="color:#E06C75">in</span><span style="color:#A9B2C3"> parts[:</span><span style="color:#E06C75">-</span><span style="color:#56B6C2">1</span><span style="color:#A9B2C3">]:</span></span>
<span data-line=""><span style="color:#A9B2C3">                    parent </span><span style="color:#E06C75">=</span><span style="color:#B57EDC"> getattr</span><span style="color:#A9B2C3">(parent, part)</span></span>
<span data-line=""><span style="color:#B57EDC">                setattr</span><span style="color:#A9B2C3">(parent, parts[</span><span style="color:#E06C75">-</span><span style="color:#56B6C2">1</span><span style="color:#A9B2C3">], lora_layer)</span></span>
<span data-line=""><span style="color:#E06C75">    return</span><span style="color:#A9B2C3"> unet</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 加载原始 UNet 并注入 LoRA</span></span>
<span data-line=""><span style="color:#A9B2C3">unet </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> UNet2DConditionModel.from_pretrained(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">runwayml/stable-diffusion-v1-5</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">subfolder</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">unet</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#A9B2C3">unet </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> inject_lora(unet, </span><span style="color:#C6CCD7">rank</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">8</span><span style="color:#A9B2C3">)</span></span></code></pre></figure>
<hr/>
<h3 id="3-数据预处理"><a href="#3-数据预处理">3. <strong>数据预处理</strong></a></h3>
<p>将图片和文本转换为模型输入：</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E06C75">from</span><span style="color:#A9B2C3"> torchvision </span><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> transforms</span></span>
<span data-line=""><span style="color:#E06C75">from</span><span style="color:#A9B2C3"> transformers </span><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> CLIPTokenizer, CLIPTextModel</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 加载 Stable Diffusion 的文本编码器</span></span>
<span data-line=""><span style="color:#A9B2C3">tokenizer </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> CLIPTokenizer.from_pretrained(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">openai/clip-vit-large-patch14</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#A9B2C3">text_encoder </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> CLIPTextModel.from_pretrained(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">openai/clip-vit-large-patch14</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 图像预处理（转换为潜在空间）</span></span>
<span data-line=""><span style="color:#A9B2C3">vae </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> AutoencoderKL.from_pretrained(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">runwayml/stable-diffusion-v1-5</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">subfolder</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">vae</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#61AFEF">def</span><span style="color:#B57EDC"> preprocess_image</span><span style="color:#A9B2C3">(</span><span style="color:#C6CCD7">image_path</span><span style="color:#A9B2C3">):</span></span>
<span data-line=""><span style="color:#A9B2C3">    image </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> Image.open(image_path).convert(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">RGB</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#A9B2C3">    transform </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> transforms.Compose([</span></span>
<span data-line=""><span style="color:#A9B2C3">        transforms.Resize(</span><span style="color:#56B6C2">512</span><span style="color:#A9B2C3">),</span></span>
<span data-line=""><span style="color:#A9B2C3">        transforms.CenterCrop(</span><span style="color:#56B6C2">512</span><span style="color:#A9B2C3">),</span></span>
<span data-line=""><span style="color:#A9B2C3">        transforms.ToTensor(),</span></span>
<span data-line=""><span style="color:#A9B2C3">    ])</span></span>
<span data-line=""><span style="color:#A9B2C3">    image </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> transform(image).unsqueeze(</span><span style="color:#56B6C2">0</span><span style="color:#A9B2C3">)  </span><span style="color:#5F6672;font-style:italic"># [1, 3, 512, 512]</span></span>
<span data-line=""><span style="color:#E06C75">    with</span><span style="color:#A9B2C3"> torch.no_grad():</span></span>
<span data-line=""><span style="color:#A9B2C3">        latents </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> vae.encode(image).latent_dist.sample() </span><span style="color:#E06C75">*</span><span style="color:#56B6C2"> 0.18215</span><span style="color:#5F6672;font-style:italic">  # [1, 4, 64, 64]</span></span>
<span data-line=""><span style="color:#E06C75">    return</span><span style="color:#A9B2C3"> latents</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 文本编码</span></span>
<span data-line=""><span style="color:#61AFEF">def</span><span style="color:#B57EDC"> encode_text</span><span style="color:#A9B2C3">(</span><span style="color:#C6CCD7">text</span><span style="color:#A9B2C3">):</span></span>
<span data-line=""><span style="color:#A9B2C3">    inputs </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> tokenizer(text, </span><span style="color:#C6CCD7">return_tensors</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">pt</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">padding</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">max_length</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">max_length</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">77</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">truncation</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">True</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#E06C75">    with</span><span style="color:#A9B2C3"> torch.no_grad():</span></span>
<span data-line=""><span style="color:#A9B2C3">        text_embeddings </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> text_encoder(inputs.input_ids)[</span><span style="color:#56B6C2">0</span><span style="color:#A9B2C3">]  </span><span style="color:#5F6672;font-style:italic"># [1, 77, 768]</span></span>
<span data-line=""><span style="color:#E06C75">    return</span><span style="color:#A9B2C3"> text_embeddings</span></span></code></pre></figure>
<hr/>
<h3 id="4-训练循环"><a href="#4-训练循环">4. <strong>训练循环</strong></a></h3>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> torch.optim </span><span style="color:#E06C75">as</span><span style="color:#A9B2C3"> optim</span></span>
<span data-line=""><span style="color:#E06C75">from</span><span style="color:#A9B2C3"> diffusers </span><span style="color:#E06C75">import</span><span style="color:#A9B2C3"> DDPMScheduler</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 初始化噪声调度器</span></span>
<span data-line=""><span style="color:#A9B2C3">noise_scheduler </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> DDPMScheduler(</span></span>
<span data-line=""><span style="color:#C6CCD7">    num_train_timesteps</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">1000</span><span style="color:#A9B2C3">,</span></span>
<span data-line=""><span style="color:#C6CCD7">    beta_start</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">0.0001</span><span style="color:#A9B2C3">,</span></span>
<span data-line=""><span style="color:#C6CCD7">    beta_end</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">0.02</span><span style="color:#A9B2C3">,</span></span>
<span data-line=""><span style="color:#C6CCD7">    beta_schedule</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">linear</span><span style="color:#A9B2C3">&quot;</span></span>
<span data-line=""><span style="color:#A9B2C3">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 优化器（仅训练 LoRA 参数）</span></span>
<span data-line=""><span style="color:#A9B2C3">optimizer </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> optim.AdamW(</span></span>
<span data-line=""><span style="color:#B57EDC">    filter</span><span style="color:#A9B2C3">(</span><span style="color:#61AFEF">lambda</span><span style="color:#C6CCD7"> p</span><span style="color:#A9B2C3">: p.requires_grad, unet.parameters()),  </span><span style="color:#5F6672;font-style:italic"># 只选择可训练参数</span></span>
<span data-line=""><span style="color:#C6CCD7">    lr</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">1e-4</span><span style="color:#A9B2C3">,</span></span>
<span data-line=""><span style="color:#C6CCD7">    weight_decay</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">1e-4</span></span>
<span data-line=""><span style="color:#A9B2C3">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 训练循环</span></span>
<span data-line=""><span style="color:#E06C75">for</span><span style="color:#A9B2C3"> epoch </span><span style="color:#E06C75">in</span><span style="color:#B57EDC"> range</span><span style="color:#A9B2C3">(num_epochs):</span></span>
<span data-line=""><span style="color:#E06C75">    for</span><span style="color:#A9B2C3"> batch </span><span style="color:#E06C75">in</span><span style="color:#A9B2C3"> dataloader:</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">        # 加载数据</span></span>
<span data-line=""><span style="color:#A9B2C3">        images, texts </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> batch</span></span>
<span data-line=""><span style="color:#A9B2C3">        latents </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> preprocess_image(images)</span></span>
<span data-line=""><span style="color:#A9B2C3">        text_embeds </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> encode_text(texts)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic">        # 添加噪声</span></span>
<span data-line=""><span style="color:#A9B2C3">        noise </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> torch.randn_like(latents)</span></span>
<span data-line=""><span style="color:#A9B2C3">        timesteps </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> torch.randint(</span><span style="color:#56B6C2">0</span><span style="color:#A9B2C3">, noise_scheduler.num_train_timesteps, (latents.shape[</span><span style="color:#56B6C2">0</span><span style="color:#A9B2C3">],))</span></span>
<span data-line=""><span style="color:#A9B2C3">        noisy_latents </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> noise_scheduler.add_noise(latents, noise, timesteps)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic">        # 前向传播</span></span>
<span data-line=""><span style="color:#A9B2C3">        noise_pred </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> unet(noisy_latents, timesteps, text_embeds).sample</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic">        # 计算损失</span></span>
<span data-line=""><span style="color:#A9B2C3">        loss </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> nn.functional.mse_loss(noise_pred, noise)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic">        # 反向传播</span></span>
<span data-line=""><span style="color:#A9B2C3">        optimizer.zero_grad()</span></span>
<span data-line=""><span style="color:#A9B2C3">        loss.backward()</span></span>
<span data-line=""><span style="color:#A9B2C3">        optimizer.step()</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#B57EDC">    print</span><span style="color:#A9B2C3">(</span><span style="color:#61AFEF">f</span><span style="color:#98C379">&quot;Epoch </span><span style="color:#56B6C2">{</span><span style="color:#A9B2C3">epoch</span><span style="color:#56B6C2">}</span><span style="color:#98C379">, Loss: </span><span style="color:#56B6C2">{</span><span style="color:#A9B2C3">loss.item()</span><span style="color:#56B6C2">}</span><span style="color:#98C379">&quot;</span><span style="color:#A9B2C3">)</span></span></code></pre></figure>
<hr/>
<h3 id="5-保存-lora-权重"><a href="#5-保存-lora-权重">5. <strong>保存 LoRA 权重</strong></a></h3>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#61AFEF">def</span><span style="color:#B57EDC"> save_lora_weights</span><span style="color:#A9B2C3">(</span><span style="color:#C6CCD7">unet</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">save_path</span><span style="color:#A9B2C3">):</span></span>
<span data-line=""><span style="color:#A9B2C3">    lora_state_dict </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> {}</span></span>
<span data-line=""><span style="color:#E06C75">    for</span><span style="color:#A9B2C3"> name, module </span><span style="color:#E06C75">in</span><span style="color:#A9B2C3"> unet.named_modules():</span></span>
<span data-line=""><span style="color:#E06C75">        if</span><span style="color:#B57EDC"> isinstance</span><span style="color:#A9B2C3">(module, LoRALayer):</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">            # 保存 A 和 B 矩阵</span></span>
<span data-line=""><span style="color:#A9B2C3">            lora_state_dict[</span><span style="color:#61AFEF">f</span><span style="color:#98C379">&quot;</span><span style="color:#56B6C2">{</span><span style="color:#A9B2C3">name</span><span style="color:#56B6C2">}</span><span style="color:#98C379">.A&quot;</span><span style="color:#A9B2C3">] </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> module.A</span></span>
<span data-line=""><span style="color:#A9B2C3">            lora_state_dict[</span><span style="color:#61AFEF">f</span><span style="color:#98C379">&quot;</span><span style="color:#56B6C2">{</span><span style="color:#A9B2C3">name</span><span style="color:#56B6C2">}</span><span style="color:#98C379">.B&quot;</span><span style="color:#A9B2C3">] </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> module.B</span></span>
<span data-line=""><span style="color:#A9B2C3">    torch.save(lora_state_dict, save_path)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#A9B2C3">save_lora_weights(unet, </span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">lora_weights.pth</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span></code></pre></figure>
<hr/>
<h3 id="6-推理时加载-lora"><a href="#6-推理时加载-lora">6. <strong>推理时加载 LoRA</strong></a></h3>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#5F6672;font-style:italic"># 加载原始模型</span></span>
<span data-line=""><span style="color:#A9B2C3">pipeline </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> StableDiffusionPipeline.from_pretrained(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">runwayml/stable-diffusion-v1-5</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 注入 LoRA</span></span>
<span data-line=""><span style="color:#A9B2C3">inject_lora(pipeline.unet, </span><span style="color:#C6CCD7">rank</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">8</span><span style="color:#A9B2C3">)</span></span>
<span data-line=""><span style="color:#A9B2C3">pipeline.unet.load_state_dict(torch.load(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">lora_weights.pth</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">), </span><span style="color:#C6CCD7">strict</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">False</span><span style="color:#A9B2C3">)  </span><span style="color:#5F6672;font-style:italic"># 仅加载 LoRA 参数</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 生成图片</span></span>
<span data-line=""><span style="color:#A9B2C3">image </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> pipeline(</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">A photo of [your trigger word]</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">).images[</span><span style="color:#56B6C2">0</span><span style="color:#A9B2C3">]</span></span></code></pre></figure>
<hr/>
<h3 id="关键注意事项"><a href="#关键注意事项"><strong>关键注意事项</strong></a></h3>
<ol>
<li><strong>参数冻结</strong>：确保原始模型的参数被冻结，只训练 LoRA 的 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>A</span></span></code></span> 和 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>B</span></span></code></span> 矩阵。</li>
<li><strong>Rank 选择</strong>：通常 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>rank=8</span></span></code></span> 足够，复杂任务可尝试 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>rank=16-64</span></span></code></span>。</li>
<li><strong>混合精度训练</strong>：添加 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>with torch.autocast(&quot;cuda&quot;)</span></span></code></span> 加速训练并减少显存。</li>
<li><strong>梯度检查点</strong>：对 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>unet.enable_gradient_checkpointing()</span></span></code></span> 以减少显存占用。</li>
<li><strong>学习率调整</strong>：LoRA 学习率通常比全模型微调大（例如 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>1e-4</span></span></code></span> vs <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>1e-5</span></span></code></span>）。</li>
</ol>
<p>通过此脚本，你可以完全控制 LoRA 的实现细节，适用于定制化需求或研究场景。若追求快速开发，仍推荐使用 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>peft</span></span></code></span> 库的封装。</p>
<h2 id="q-你说需将-lora-注入到-unet-的-cross-attention-层-lora-只能加到-crossattention-层吗为什么"><a href="#q-你说需将-lora-注入到-unet-的-cross-attention-层-lora-只能加到-crossattention-层吗为什么">Q: 你说“需将 LoRA 注入到 UNet 的 Cross-Attention 层”， LoRA 只能加到 CrossAttention 层吗？为什么？</a></h2>
<div class="rounded-md p-2 my-4 bg-sky-100"><div class="text-base font-bold text-sky-500 cursor-pointer"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="m15.82 7.22-1 .4c-.21-.16-.43-.29-.67-.39L14 6.17c-.02-.1-.1-.17-.2-.17h-1.6c-.1 0-.18.07-.19.17l-.15 1.06c-.24.1-.47.23-.67.39l-1-.4c-.09-.03-.2 0-.24.09l-.8 1.38c-.05.09-.03.2.05.26l.85.66c-.03.12-.05.26-.05.39q0 .195.03.39l-.84.66c-.08.06-.1.17-.05.25l.8 1.39c.05.09.15.12.25.09l.99-.4c.21.16.43.29.68.39l.14 1.06c.02.1.1.17.2.17h1.6c.1 0 .18-.07.2-.17l.15-1.06c.24-.1.47-.23.67-.39l.99.4c.09.04.2 0 .24-.09l.8-1.39c.05-.09.03-.19-.05-.25l-.83-.66q.03-.195.03-.39 0-.21-.03-.39l.85-.66c.08-.06.1-.17.05-.26l-.8-1.38c-.05-.09-.16-.12-.25-.09M13 11.43c-.79 0-1.43-.64-1.43-1.43s.64-1.43 1.43-1.43 1.43.64 1.43 1.43-.64 1.43-1.43 1.43"></path><path d="M19.94 9.06c-.43-3.27-3.23-5.86-6.53-6.05C13.27 3 13.14 3 13 3 9.47 3 6.57 5.61 6.08 9l-1.93 3.48c-.41.66.07 1.52.85 1.52h1v2c0 1.1.9 2 2 2h1v3h7v-4.68c2.62-1.25 4.35-4.08 3.94-7.26m-5.05 5.57-.89.42V19h-3v-3H8v-4H6.7l1.33-2.33C8.21 7.06 10.35 5 13 5c2.76 0 5 2.24 5 5 0 2.09-1.29 3.88-3.11 4.63"></path></svg> <!-- -->Reasoning<!-- --> <svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M8.59 16.59 13.17 12 8.59 7.41 10 6l6 6-6 6z"></path></svg></div></div>
<p>你的问题非常关键！<strong>LoRA 并不只能加到 Cross-Attention 层</strong>，但在 Stable Diffusion 微调中，<strong>Cross-Attention 层是最关键的目标</strong>。以下是深度解析：</p>
<hr/>
<h3 id="1-理论层面lora-可以加在哪里"><a href="#1-理论层面lora-可以加在哪里"><strong>1. 理论层面：LoRA 可以加在哪里？</strong></a></h3>
<p>从数学原理上，LoRA 是一种通用的参数高效微调方法，<strong>可应用于任何线性层</strong>（如 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>nn.Linear</span></span></code></span> 或等效的矩阵运算）。例如：</p>
<ul>
<li><strong>Self-Attention 中的 Q/K/V 层</strong>（处理图像自身特征）</li>
<li><strong>Feed-Forward Network (FFN) 的中间层</strong></li>
<li><strong>甚至 VAE 或 Text Encoder 的某些层</strong>（但较少见）</li>
</ul>
<p>但实际应用中，<strong>选择注入位置需权衡效果与效率</strong>。</p>
<hr/>
<h3 id="2-为什么-stable-diffusion-优先选择-cross-attention-层"><a href="#2-为什么-stable-diffusion-优先选择-cross-attention-层"><strong>2. 为什么 Stable Diffusion 优先选择 Cross-Attention 层？</strong></a></h3>
<h4 id="1-文本-图像对齐的核心枢纽"><a href="#1-文本-图像对齐的核心枢纽"><strong>(1) 文本-图像对齐的核心枢纽</strong></a></h4>
<ul>
<li><strong>Cross-Attention 层</strong>是 UNet 中<strong>文本条件（Text Embedding）与图像特征交互的唯一通道</strong>。</li>
<li>修改这些层能直接控制生成内容如何响应文本提示（Prompt），例如：<!-- -->
<ul>
<li>学习新概念（如特定物体 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>my_cat_v1</span></span></code></span>）</li>
<li>调整风格（如油画风 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>van gogh style</span></span></code></span>）</li>
</ul>
</li>
</ul>
<h4 id="2-参数效率最大化"><a href="#2-参数效率最大化"><strong>(2) 参数效率最大化</strong></a></h4>
<ul>
<li>Stable Diffusion 的 UNet 参数量巨大（约 860M），但 <strong>Cross-Attention 层仅占约 10% 的参数量</strong>。</li>
<li>仅微调这部分，即可用 <strong>0.1%~1% 的参数量达到接近全参数微调的效果</strong>，性价比极高。</li>
</ul>
<h4 id="3-实践经验验证"><a href="#3-实践经验验证"><strong>(3) 实践经验验证</strong></a></h4>
<ul>
<li>大量实验表明，<strong>调整 Cross-Attention 层对生成结果影响最显著</strong>。</li>
<li>若同时微调 Self-Attention 或 FFN 层，可能引入噪声（如破坏预训练模型的构图能力）。</li>
</ul>
<hr/>
<h3 id="3-其他可能的目标层"><a href="#3-其他可能的目标层"><strong>3. 其他可能的目标层</strong></a></h3>
<h4 id="1-self-attention-层"><a href="#1-self-attention-层"><strong>(1) Self-Attention 层</strong></a></h4>
<ul>
<li><strong>作用</strong>：控制图像内部结构（如物体形状、布局）。</li>
<li><strong>适用场景</strong>：需要修改生成图像的构图规则（如强制对称性）。</li>
<li><strong>风险</strong>：容易导致模型遗忘预训练知识（如失去多样性）。</li>
</ul>
<h4 id="2-ffn-层"><a href="#2-ffn-层"><strong>(2) FFN 层</strong></a></h4>
<ul>
<li><strong>作用</strong>：非线性特征变换。</li>
<li><strong>适用场景</strong>：调整颜色、纹理等低级特征。</li>
<li><strong>缺点</strong>：参数量大，效率较低。</li>
</ul>
<h4 id="3-组合策略"><a href="#3-组合策略"><strong>(3) 组合策略</strong></a></h4>
<ul>
<li>同时微调 <strong>Cross-Attention + 部分 Self-Attention</strong>，可平衡控制力与稳定性，但需更多显存和训练数据。</li>
</ul>
<hr/>
<h3 id="4-代码示例注入到-self-attention"><a href="#4-代码示例注入到-self-attention"><strong>4. 代码示例：注入到 Self-Attention</strong></a></h3>
<p>只需修改 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>inject_lora</span></span></code></span> 函数中的目标模块筛选条件：</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="plastic"><code data-language="python" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#61AFEF">def</span><span style="color:#B57EDC"> inject_lora</span><span style="color:#A9B2C3">(</span><span style="color:#C6CCD7">unet</span><span style="color:#A9B2C3">, </span><span style="color:#C6CCD7">rank</span><span style="color:#E06C75">=</span><span style="color:#56B6C2">8</span><span style="color:#A9B2C3">):</span></span>
<span data-line=""><span style="color:#E06C75">    for</span><span style="color:#A9B2C3"> name, module </span><span style="color:#E06C75">in</span><span style="color:#A9B2C3"> unet.named_modules():</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">        # 定位 Self-Attention 的 Q/K/V 层（&quot;attn1&quot; 表示 Self-Attention）</span></span>
<span data-line=""><span style="color:#E06C75">        if</span><span style="color:#A9B2C3"> &quot;</span><span style="color:#98C379">attn1</span><span style="color:#A9B2C3">&quot;</span><span style="color:#E06C75"> in</span><span style="color:#A9B2C3"> name </span><span style="color:#E06C75">and</span><span style="color:#A9B2C3"> &quot;</span><span style="color:#98C379">to_</span><span style="color:#A9B2C3">&quot;</span><span style="color:#E06C75"> in</span><span style="color:#A9B2C3"> name:</span></span>
<span data-line=""><span style="color:#E06C75">            if</span><span style="color:#B57EDC"> isinstance</span><span style="color:#A9B2C3">(module, nn.Linear):</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">                # 替换为 LoRALayer</span></span>
<span data-line=""><span style="color:#56B6C2">                ...</span></span>
<span data-line=""><span style="color:#E06C75">    return</span><span style="color:#A9B2C3"> unet</span></span></code></pre></figure>
<hr/>
<h3 id="5-如何选择注入位置"><a href="#5-如何选择注入位置"><strong>5. 如何选择注入位置？</strong></a></h3>
<table><thead><tr><th><strong>目标层类型</strong></th><th><strong>参数量占比</strong></th><th><strong>影响范围</strong></th><th><strong>推荐场景</strong></th></tr></thead><tbody><tr><td><strong>Cross-Attention</strong></td><td>~10%</td><td>文本控制、概念学习</td><td>学习新物体/风格（主流选择）</td></tr><tr><td><strong>Self-Attention</strong></td><td>~20%</td><td>图像结构、布局</td><td>特殊构图需求</td></tr><tr><td><strong>FFN</strong></td><td>~70%</td><td>颜色、纹理</td><td>低级特征微调</td></tr></tbody></table>
<hr/>
<h3 id="6-终极建议"><a href="#6-终极建议"><strong>6. 终极建议</strong></a></h3>
<ul>
<li><strong>优先 Cross-Attention</strong>：95% 的场景只需调整这里。</li>
<li><strong>谨慎添加其他层</strong>：仅在数据充足且任务特殊时尝试（如医学图像生成需精确形状控制）。</li>
<li><strong>监控过拟合</strong>：添加更多可训练参数会增加过拟合风险，需配合数据增强/早停法。</li>
</ul>
<div class="rounded-md p-2 my-4 bg-violet-100"><div class="text-base font-bold text-violet-500"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M7 20h4c0 1.1-.9 2-2 2s-2-.9-2-2m-2-1h8v-2H5zm11.5-9.5c0 3.82-2.66 5.86-3.77 6.5H5.27c-1.11-.64-3.77-2.68-3.77-6.5C1.5 5.36 4.86 2 9 2s7.5 3.36 7.5 7.5m-2 0C14.5 6.47 12.03 4 9 4S3.5 6.47 3.5 9.5c0 2.47 1.49 3.89 2.35 4.5h6.3c.86-.61 2.35-2.03 2.35-4.5m6.87-2.13L20 8l1.37.63L22 10l.63-1.37L24 8l-1.37-.63L22 6zM19 6l.94-2.06L22 3l-2.06-.94L19 0l-.94 2.06L16 3l2.06.94z"></path></svg> <!-- -->亮曰：<!-- --> </div><div class="m-2"><p><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>UNet2DConditionModel</span></span></code></span> 的整个流程中， Cross-Attention 主要作用在下采样、中间块、上采样三个步骤中。在 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>__init__</span></span></code></span> 参数中传入每个步骤中用的 Module ，其中用 <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>CrossAttnDownBlock2D</span></span></code></span> 等 Module <span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>self.has_cross_attention</span></span></code></span> 为 true ，能处理 cross_attention_mask 等参数。其中的 attentions 层即为交叉注意力层。推测，是通过 Attentions 层对下采样层入参做处理，并列的 ResNet 应该是为了提升注意力层的训练性能。</p></div></div>
<h2 id="unet-代码中的-cross-attension-层"><a href="#unet-代码中的-cross-attension-层">UNet 代码中的 Cross-Attension 层</a></h2>
<blockquote>
<p>以下为在 Cursor 中有 diffusers 代码上下文时与 claude-3.7-sonnet 的对话内容。</p>
</blockquote>
<div class="rounded-md p-2 my-4 bg-fuchsia-100"><div class="text-base font-bold text-fuchsia-500"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M11 18h2v-2h-2zm1-16C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2m0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8m0-14c-2.21 0-4 1.79-4 4h2c0-1.1.9-2 2-2s2 .9 2 2c0 2-3 1.75-3 5h2c0-2.25 3-2.5 3-5 0-2.21-1.79-4-4-4"></path></svg> <!-- -->Query<!-- --> </div><div class="m-2"><p>UNet 中 Cross-Attension 是做什么的？具体到代码里如何作用？ @unet_2d_condition.py</p></div></div>
<div class="rounded-md p-2 my-4 bg-sky-100"><div class="text-base font-bold text-sky-500 cursor-pointer"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="m15.82 7.22-1 .4c-.21-.16-.43-.29-.67-.39L14 6.17c-.02-.1-.1-.17-.2-.17h-1.6c-.1 0-.18.07-.19.17l-.15 1.06c-.24.1-.47.23-.67.39l-1-.4c-.09-.03-.2 0-.24.09l-.8 1.38c-.05.09-.03.2.05.26l.85.66c-.03.12-.05.26-.05.39q0 .195.03.39l-.84.66c-.08.06-.1.17-.05.25l.8 1.39c.05.09.15.12.25.09l.99-.4c.21.16.43.29.68.39l.14 1.06c.02.1.1.17.2.17h1.6c.1 0 .18-.07.2-.17l.15-1.06c.24-.1.47-.23.67-.39l.99.4c.09.04.2 0 .24-.09l.8-1.39c.05-.09.03-.19-.05-.25l-.83-.66q.03-.195.03-.39 0-.21-.03-.39l.85-.66c.08-.06.1-.17.05-.26l-.8-1.38c-.05-.09-.16-.12-.25-.09M13 11.43c-.79 0-1.43-.64-1.43-1.43s.64-1.43 1.43-1.43 1.43.64 1.43 1.43-.64 1.43-1.43 1.43"></path><path d="M19.94 9.06c-.43-3.27-3.23-5.86-6.53-6.05C13.27 3 13.14 3 13 3 9.47 3 6.57 5.61 6.08 9l-1.93 3.48c-.41.66.07 1.52.85 1.52h1v2c0 1.1.9 2 2 2h1v3h7v-4.68c2.62-1.25 4.35-4.08 3.94-7.26m-5.05 5.57-.89.42V19h-3v-3H8v-4H6.7l1.33-2.33C8.21 7.06 10.35 5 13 5c2.76 0 5 2.24 5 5 0 2.09-1.29 3.88-3.11 4.63"></path></svg> <!-- -->Reasoning<!-- --> <svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M8.59 16.59 13.17 12 8.59 7.41 10 6l6 6-6 6z"></path></svg></div></div>
<p>Cross-Attention 在 UNet 中主要用于将条件信息（如文本特征）引入到图像生成过程中。从代码来看，主要体现在以下几个方面：</p>
<ol>
<li>
<p><strong>数据流向</strong>：</p>
<ul>
<li>UNet 接收图像特征(<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>sample</span></span></code></span>)和条件编码(<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_hidden_states</span></span></code></span>)</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_hidden_states</span></span></code></span>通常是文本编码器输出的特征</li>
<li>条件信息通过 Cross-Attention 在 UNet 的不同层级与图像特征交互</li>
</ul>
</li>
<li>
<p><strong>代码实现</strong>：</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="py" data-theme="plastic"><code data-language="py" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#E06C75">if</span><span style="color:#B57EDC"> hasattr</span><span style="color:#A9B2C3">(downsample_block, </span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">has_cross_attention</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">) </span><span style="color:#E06C75">and</span><span style="color:#A9B2C3"> downsample_block.has_cross_attention:</span></span>
<span data-line=""><span style="color:#A9B2C3">    sample, res_samples </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> downsample_block(</span></span>
<span data-line=""><span style="color:#C6CCD7">        hidden_states</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">sample,</span></span>
<span data-line=""><span style="color:#C6CCD7">        temb</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">emb,</span></span>
<span data-line=""><span style="color:#C6CCD7">        encoder_hidden_states</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">encoder_hidden_states,</span></span>
<span data-line=""><span style="color:#C6CCD7">        attention_mask</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">attention_mask,</span></span>
<span data-line=""><span style="color:#C6CCD7">        cross_attention_kwargs</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">cross_attention_kwargs,</span></span>
<span data-line=""><span style="color:#C6CCD7">        encoder_attention_mask</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">encoder_attention_mask,</span></span>
<span data-line=""><span style="color:#E06C75">        **</span><span style="color:#A9B2C3">additional_residuals,</span></span>
<span data-line=""><span style="color:#A9B2C3">    )</span></span></code></pre></figure>
</li>
<li>
<p><strong>核心组件</strong>：</p>
<ul>
<li>在下采样、中间块和上采样阶段都可能使用 Cross-Attention</li>
<li>通过<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>has_cross_attention</span></span></code></span>属性判断块是否需要使用 Cross-Attention</li>
<li>当使用 Cross-Attention 时，<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_hidden_states</span></span></code></span>会作为额外输入传递</li>
</ul>
</li>
<li>
<p><strong>参数控制</strong>：</p>
<ul>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>only_cross_attention</span></span></code></span>参数控制是否只使用 Cross-Attention 而不使用 Self-Attention</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>cross_attention_dim</span></span></code></span>指定了条件编码的维度大小</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>transformer_layers_per_block</span></span></code></span>控制每个块中 Transformer 层的数量</li>
</ul>
</li>
</ol>
<p>Cross-Attention 允许模型在保持图像空间结构的同时，根据文本或其他条件调整生成内容，实现了条件生成的核心机制。</p>
<div class="rounded-md p-2 my-4 bg-fuchsia-100"><div class="text-base font-bold text-fuchsia-500"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M11 18h2v-2h-2zm1-16C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2m0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8m0-14c-2.21 0-4 1.79-4 4h2c0-1.1.9-2 2-2s2 .9 2 2c0 2-3 1.75-3 5h2c0-2.25 3-2.5 3-5 0-2.21-1.79-4-4-4"></path></svg> <!-- -->Query<!-- --> </div><div class="m-2"><p>TransformerBlock 具体是在@transformer_2d.py 里实现吗？如何体现 Self-Attention 与 Cross-Attention ？</p></div></div>
<div class="rounded-md p-2 my-4 bg-sky-100"><div class="text-base font-bold text-sky-500 cursor-pointer"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="m15.82 7.22-1 .4c-.21-.16-.43-.29-.67-.39L14 6.17c-.02-.1-.1-.17-.2-.17h-1.6c-.1 0-.18.07-.19.17l-.15 1.06c-.24.1-.47.23-.67.39l-1-.4c-.09-.03-.2 0-.24.09l-.8 1.38c-.05.09-.03.2.05.26l.85.66c-.03.12-.05.26-.05.39q0 .195.03.39l-.84.66c-.08.06-.1.17-.05.25l.8 1.39c.05.09.15.12.25.09l.99-.4c.21.16.43.29.68.39l.14 1.06c.02.1.1.17.2.17h1.6c.1 0 .18-.07.2-.17l.15-1.06c.24-.1.47-.23.67-.39l.99.4c.09.04.2 0 .24-.09l.8-1.39c.05-.09.03-.19-.05-.25l-.83-.66q.03-.195.03-.39 0-.21-.03-.39l.85-.66c.08-.06.1-.17.05-.26l-.8-1.38c-.05-.09-.16-.12-.25-.09M13 11.43c-.79 0-1.43-.64-1.43-1.43s.64-1.43 1.43-1.43 1.43.64 1.43 1.43-.64 1.43-1.43 1.43"></path><path d="M19.94 9.06c-.43-3.27-3.23-5.86-6.53-6.05C13.27 3 13.14 3 13 3 9.47 3 6.57 5.61 6.08 9l-1.93 3.48c-.41.66.07 1.52.85 1.52h1v2c0 1.1.9 2 2 2h1v3h7v-4.68c2.62-1.25 4.35-4.08 3.94-7.26m-5.05 5.57-.89.42V19h-3v-3H8v-4H6.7l1.33-2.33C8.21 7.06 10.35 5 13 5c2.76 0 5 2.24 5 5 0 2.09-1.29 3.88-3.11 4.63"></path></svg> <!-- -->Reasoning<!-- --> <svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M8.59 16.59 13.17 12 8.59 7.41 10 6l6 6-6 6z"></path></svg></div></div>
<p>TransformerBlock 的核心实现是在<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>attention.py</span></span></code></span>中的<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>BasicTransformerBlock</span></span></code></span>类中，而不是直接在<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>transformer_2d.py</span></span></code></span>中。在<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>transformer_2d.py</span></span></code></span>中只是调用了这个类来构建 Transformer 模型。</p>
<p>从<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>transformer_2d.py</span></span></code></span>代码中可以看到 Self-Attention 和 Cross-Attention 的几个关键特点：</p>
<ol>
<li>
<p><strong>实例化 transformer 块时</strong>:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="156:169:src/diffusers/models/transformers/transformer_2d.py" data-theme="plastic"><code data-language="156:169:src/diffusers/models/transformers/transformer_2d.py" data-theme="plastic" style="display:grid"><span data-line=""><span>BasicTransformerBlock(</span></span>
<span data-line=""><span>    self.inner_dim,</span></span>
<span data-line=""><span>    self.config.num_attention_heads,</span></span>
<span data-line=""><span>    self.config.attention_head_dim,</span></span>
<span data-line=""><span>    dropout=self.config.dropout,</span></span>
<span data-line=""><span>    cross_attention_dim=self.config.cross_attention_dim,</span></span>
<span data-line=""><span>    activation_fn=self.config.activation_fn,</span></span>
<span data-line=""><span>    num_embeds_ada_norm=self.config.num_embeds_ada_norm,</span></span>
<span data-line=""><span>    attention_bias=self.config.attention_bias,</span></span>
<span data-line=""><span>    only_cross_attention=self.config.only_cross_attention,</span></span>
<span data-line=""><span>    double_self_attention=self.config.double_self_attention,</span></span>
<span data-line=""><span>    ...</span></span>
<span data-line=""><span>)</span></span></code></pre></figure>
</li>
<li>
<p><strong>调用时的数据流</strong>:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="386:396:src/diffusers/models/transformers/transformer_2d.py" data-theme="plastic"><code data-language="386:396:src/diffusers/models/transformers/transformer_2d.py" data-theme="plastic" style="display:grid"><span data-line=""><span>hidden_states = block(</span></span>
<span data-line=""><span>    hidden_states,</span></span>
<span data-line=""><span>    attention_mask=attention_mask,</span></span>
<span data-line=""><span>    encoder_hidden_states=encoder_hidden_states,</span></span>
<span data-line=""><span>    encoder_attention_mask=encoder_attention_mask,</span></span>
<span data-line=""><span>    timestep=timestep,</span></span>
<span data-line=""><span>    cross_attention_kwargs=cross_attention_kwargs,</span></span>
<span data-line=""><span>    class_labels=class_labels,</span></span>
<span data-line=""><span>)</span></span></code></pre></figure>
</li>
</ol>
<p>核心区别：</p>
<ul>
<li><strong>Self-Attention</strong>: 使用<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>hidden_states</span></span></code></span>自身进行注意力计算，<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>attention_mask</span></span></code></span>作为掩码</li>
<li><strong>Cross-Attention</strong>: 使用<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>hidden_states</span></span></code></span>作为 query，<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_hidden_states</span></span></code></span>作为 key 和 value，<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_attention_mask</span></span></code></span>作为掩码</li>
</ul>
<p>配置选项：</p>
<ul>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>only_cross_attention</span></span></code></span>: 如果为 True，只使用 Cross-Attention 而跳过 Self-Attention</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>double_self_attention</span></span></code></span>: 如果为 True，使用两次 Self-Attention</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>cross_attention_dim</span></span></code></span>: 指定 Cross-Attention 的维度大小</li>
</ul>
<p>在实际运行时，BasicTransformerBlock 内部会先进行 Self-Attention 操作，然后进行 Cross-Attention 操作（如果提供了<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_hidden_states</span></span></code></span>）。这样就实现了 UNet 中文本条件对图像生成的影响。</p>
<div class="rounded-md p-2 my-4 bg-fuchsia-100"><div class="text-base font-bold text-fuchsia-500"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M11 18h2v-2h-2zm1-16C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2m0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8m0-14c-2.21 0-4 1.79-4 4h2c0-1.1.9-2 2-2s2 .9 2 2c0 2-3 1.75-3 5h2c0-2.25 3-2.5 3-5 0-2.21-1.79-4-4-4"></path></svg> <!-- -->Query<!-- --> </div><div class="m-2"><p>@attention.py 中 BasicTransformerBlock 如何体现 Self-Attention 与 Cross-Attention ?</p></div></div>
<div class="rounded-md p-2 my-4 bg-sky-100"><div class="text-base font-bold text-sky-500"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="m15.82 7.22-1 .4c-.21-.16-.43-.29-.67-.39L14 6.17c-.02-.1-.1-.17-.2-.17h-1.6c-.1 0-.18.07-.19.17l-.15 1.06c-.24.1-.47.23-.67.39l-1-.4c-.09-.03-.2 0-.24.09l-.8 1.38c-.05.09-.03.2.05.26l.85.66c-.03.12-.05.26-.05.39q0 .195.03.39l-.84.66c-.08.06-.1.17-.05.25l.8 1.39c.05.09.15.12.25.09l.99-.4c.21.16.43.29.68.39l.14 1.06c.02.1.1.17.2.17h1.6c.1 0 .18-.07.2-.17l.15-1.06c.24-.1.47-.23.67-.39l.99.4c.09.04.2 0 .24-.09l.8-1.39c.05-.09.03-.19-.05-.25l-.83-.66q.03-.195.03-.39 0-.21-.03-.39l.85-.66c.08-.06.1-.17.05-.26l-.8-1.38c-.05-.09-.16-.12-.25-.09M13 11.43c-.79 0-1.43-.64-1.43-1.43s.64-1.43 1.43-1.43 1.43.64 1.43 1.43-.64 1.43-1.43 1.43"></path><path d="M19.94 9.06c-.43-3.27-3.23-5.86-6.53-6.05C13.27 3 13.14 3 13 3 9.47 3 6.57 5.61 6.08 9l-1.93 3.48c-.41.66.07 1.52.85 1.52h1v2c0 1.1.9 2 2 2h1v3h7v-4.68c2.62-1.25 4.35-4.08 3.94-7.26m-5.05 5.57-.89.42V19h-3v-3H8v-4H6.7l1.33-2.33C8.21 7.06 10.35 5 13 5c2.76 0 5 2.24 5 5 0 2.09-1.29 3.88-3.11 4.63"></path></svg> <!-- -->Reasoning<!-- --> </div><div class="m-2"><p>让我看看 attention.py 中的 BasicTransformerBlock 类，特别是它如何实现 Self-Attention 和 Cross-Attention 的部分。</p><p>从代码中，我可以看到 BasicTransformerBlock 类包含了以下几个关键组件：</p><p>初始化函数中，它设置了多个 Attention 模块和标准化层：</p><p>self.norm1 和 self.attn1 用于 Self-Attention</p><p>self.norm2 和 self.attn2 用于 Cross-Attention（如果需要的话）</p><p>self.norm3 和 self.ff 用于前馈网络</p><p>在 forward 函数中，处理流程如下：</p><p>首先对输入进行 normalization</p><p>然后应用 Self-Attention（self.attn1）</p><p>如果有 cross_attention_dim 或 double_self_attention，则应用 Cross-Attention（self.attn2）</p><p>最后是 Feed-Forward 网络</p><p>特别需要注意的是，对于 Self-Attention 和 Cross-Attention 的区别：</p><p>Self-Attention：使用 self.attn1，只使用 hidden_states 作为输入，在 only_cross_attention 为 True 时可能使用 encoder_hidden_states。</p><p>Cross-Attention：使用 self.attn2，hidden_states 作为 query，encoder_hidden_states 作为 key 和 value。</p><p>在 forward 方法中，我们可以清楚看到这一点：</p><p>对于 Self-Attention，attn_output = self.attn1(norm_hidden_states, ...)</p><p>对于 Cross-Attention，attn_output = self.attn2(norm_hidden_states, encoder_hidden_states=encoder_hidden_states, ...)</p><p>我会重点关注 forward 方法中实现 Self-Attention 和 Cross-Attention 的关键部分。</p></div></div>
<p>在<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>attention.py</span></span></code></span>中，<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>BasicTransformerBlock</span></span></code></span>类通过以下方式实现了 Self-Attention 和 Cross-Attention：</p>
<h3 id="1-架构设计"><a href="#1-架构设计">1. 架构设计</a></h3>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="py" data-theme="plastic"><code data-language="py" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#5F6672;font-style:italic"># 1. Self-Attn</span></span>
<span data-line=""><span style="color:#E06C75">self</span><span style="color:#A9B2C3">.attn1 </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> Attention(</span></span>
<span data-line=""><span style="color:#C6CCD7">    query_dim</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">dim,</span></span>
<span data-line=""><span style="color:#C6CCD7">    heads</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">num_attention_heads,</span></span>
<span data-line=""><span style="color:#C6CCD7">    dim_head</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">attention_head_dim,</span></span>
<span data-line=""><span style="color:#C6CCD7">    dropout</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">dropout,</span></span>
<span data-line=""><span style="color:#C6CCD7">    bias</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">attention_bias,</span></span>
<span data-line=""><span style="color:#C6CCD7">    cross_attention_dim</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">cross_attention_dim </span><span style="color:#E06C75">if</span><span style="color:#A9B2C3"> only_cross_attention </span><span style="color:#E06C75">else</span><span style="color:#56B6C2"> None</span><span style="color:#A9B2C3">,</span></span>
<span data-line=""><span style="color:#C6CCD7">    upcast_attention</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">upcast_attention,</span></span>
<span data-line=""><span style="color:#C6CCD7">    out_bias</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">attention_out_bias,</span></span>
<span data-line=""><span style="color:#A9B2C3">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 2. Cross-Attn</span></span>
<span data-line=""><span style="color:#E06C75">if</span><span style="color:#A9B2C3"> cross_attention_dim </span><span style="color:#E06C75">is</span><span style="color:#E06C75"> not</span><span style="color:#56B6C2"> None</span><span style="color:#E06C75"> or</span><span style="color:#A9B2C3"> double_self_attention:</span></span>
<span data-line=""><span style="color:#E06C75">    self</span><span style="color:#A9B2C3">.norm2 </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> nn.LayerNorm(dim, norm_eps, norm_elementwise_affine)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#E06C75">    self</span><span style="color:#A9B2C3">.attn2 </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> Attention(</span></span>
<span data-line=""><span style="color:#C6CCD7">        query_dim</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">dim,</span></span>
<span data-line=""><span style="color:#C6CCD7">        cross_attention_dim</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">cross_attention_dim </span><span style="color:#E06C75">if</span><span style="color:#E06C75"> not</span><span style="color:#A9B2C3"> double_self_attention </span><span style="color:#E06C75">else</span><span style="color:#56B6C2"> None</span><span style="color:#A9B2C3">,</span></span>
<span data-line=""><span style="color:#C6CCD7">        heads</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">num_attention_heads,</span></span>
<span data-line=""><span style="color:#C6CCD7">        dim_head</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">attention_head_dim,</span></span>
<span data-line=""><span style="color:#C6CCD7">        dropout</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">dropout,</span></span>
<span data-line=""><span style="color:#C6CCD7">        bias</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">attention_bias,</span></span>
<span data-line=""><span style="color:#C6CCD7">        upcast_attention</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">upcast_attention,</span></span>
<span data-line=""><span style="color:#C6CCD7">        out_bias</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">attention_out_bias,</span></span>
<span data-line=""><span style="color:#A9B2C3">    )</span></span></code></pre></figure>
<h3 id="2-前向传播过程"><a href="#2-前向传播过程">2. 前向传播过程</a></h3>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="py" data-theme="plastic"><code data-language="py" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#5F6672;font-style:italic"># 1. Self-Attention</span></span>
<span data-line=""><span style="color:#A9B2C3">norm_hidden_states </span><span style="color:#E06C75">=</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm1(hidden_states)</span></span>
<span data-line=""><span style="color:#A9B2C3">attn_output </span><span style="color:#E06C75">=</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.attn1(</span></span>
<span data-line=""><span style="color:#A9B2C3">    norm_hidden_states,</span></span>
<span data-line=""><span style="color:#C6CCD7">    encoder_hidden_states</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">encoder_hidden_states </span><span style="color:#E06C75">if</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.only_cross_attention </span><span style="color:#E06C75">else</span><span style="color:#56B6C2"> None</span><span style="color:#A9B2C3">,</span></span>
<span data-line=""><span style="color:#C6CCD7">    attention_mask</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">attention_mask,</span></span>
<span data-line=""><span style="color:#E06C75">    **</span><span style="color:#A9B2C3">cross_attention_kwargs,</span></span>
<span data-line=""><span style="color:#A9B2C3">)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># ... (处理输出)</span></span>
<span data-line=""><span style="color:#A9B2C3">hidden_states </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> attn_output </span><span style="color:#E06C75">+</span><span style="color:#A9B2C3"> hidden_states</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#5F6672;font-style:italic"># 2. Cross-Attention</span></span>
<span data-line=""><span style="color:#E06C75">if</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.attn2 </span><span style="color:#E06C75">is</span><span style="color:#E06C75"> not</span><span style="color:#56B6C2"> None</span><span style="color:#A9B2C3">:</span></span>
<span data-line=""><span style="color:#A9B2C3">    norm_hidden_states </span><span style="color:#E06C75">=</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm2(hidden_states)</span></span>
<span data-line=""><span style="color:#A9B2C3">    attn_output </span><span style="color:#E06C75">=</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.attn2(</span></span>
<span data-line=""><span style="color:#A9B2C3">        norm_hidden_states,</span></span>
<span data-line=""><span style="color:#C6CCD7">        encoder_hidden_states</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">encoder_hidden_states,</span></span>
<span data-line=""><span style="color:#C6CCD7">        attention_mask</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">encoder_attention_mask,</span></span>
<span data-line=""><span style="color:#E06C75">        **</span><span style="color:#A9B2C3">cross_attention_kwargs,</span></span>
<span data-line=""><span style="color:#A9B2C3">    )</span></span>
<span data-line=""><span style="color:#A9B2C3">    hidden_states </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> attn_output </span><span style="color:#E06C75">+</span><span style="color:#A9B2C3"> hidden_states</span></span></code></pre></figure>
<h3 id="3-关键区别"><a href="#3-关键区别">3. 关键区别</a></h3>
<ul>
<li>
<p><strong>Self-Attention (attn1)</strong>：</p>
<ul>
<li>默认情况下只使用<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>hidden_states</span></span></code></span>作为输入（query, key, value 都来自同一源）</li>
<li>只有当<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>only_cross_attention=True</span></span></code></span>时才会用<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_hidden_states</span></span></code></span></li>
<li>使用<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>attention_mask</span></span></code></span>作为掩码</li>
</ul>
</li>
<li>
<p><strong>Cross-Attention (attn2)</strong>：</p>
<ul>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>norm_hidden_states</span></span></code></span>作为 query</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_hidden_states</span></span></code></span>作为 key 和 value</li>
<li>使用<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_attention_mask</span></span></code></span>作为掩码</li>
<li>当<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>encoder_hidden_states=None</span></span></code></span>时会退化为 Self-Attention</li>
</ul>
</li>
</ul>
<h3 id="4-配置控制"><a href="#4-配置控制">4. 配置控制</a></h3>
<ul>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>only_cross_attention</span></span></code></span>：是否在 attn1 中使用 cross-attention</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>double_self_attention</span></span></code></span>：是否将 attn2 配置为另一个 self-attention 层</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>cross_attention_dim</span></span></code></span>：指定条件编码的维度</li>
</ul>
<p>这种设计允许 UNet 模型在不同层级灵活组合 Self-Attention 和 Cross-Attention，实现对图像特征的内部处理和对条件信息的整合。</p>
<div class="rounded-md p-2 my-4 bg-fuchsia-100"><div class="text-base font-bold text-fuchsia-500"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M11 18h2v-2h-2zm1-16C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2m0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8m0-14c-2.21 0-4 1.79-4 4h2c0-1.1.9-2 2-2s2 .9 2 2c0 2-3 1.75-3 5h2c0-2.25 3-2.5 3-5 0-2.21-1.79-4-4-4"></path></svg> <!-- -->Query<!-- --> </div><div class="m-2"><p>我注意到 @attention.py 中在 Attention 层前都会有一个 LayerNorm 层。这是必须要有的吗？有什么作用？</p></div></div>
<div class="rounded-md p-2 my-4 bg-sky-100"><div class="text-base font-bold text-sky-500 cursor-pointer"><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="m15.82 7.22-1 .4c-.21-.16-.43-.29-.67-.39L14 6.17c-.02-.1-.1-.17-.2-.17h-1.6c-.1 0-.18.07-.19.17l-.15 1.06c-.24.1-.47.23-.67.39l-1-.4c-.09-.03-.2 0-.24.09l-.8 1.38c-.05.09-.03.2.05.26l.85.66c-.03.12-.05.26-.05.39q0 .195.03.39l-.84.66c-.08.06-.1.17-.05.25l.8 1.39c.05.09.15.12.25.09l.99-.4c.21.16.43.29.68.39l.14 1.06c.02.1.1.17.2.17h1.6c.1 0 .18-.07.2-.17l.15-1.06c.24-.1.47-.23.67-.39l.99.4c.09.04.2 0 .24-.09l.8-1.39c.05-.09.03-.19-.05-.25l-.83-.66q.03-.195.03-.39 0-.21-.03-.39l.85-.66c.08-.06.1-.17.05-.26l-.8-1.38c-.05-.09-.16-.12-.25-.09M13 11.43c-.79 0-1.43-.64-1.43-1.43s.64-1.43 1.43-1.43 1.43.64 1.43 1.43-.64 1.43-1.43 1.43"></path><path d="M19.94 9.06c-.43-3.27-3.23-5.86-6.53-6.05C13.27 3 13.14 3 13 3 9.47 3 6.57 5.61 6.08 9l-1.93 3.48c-.41.66.07 1.52.85 1.52h1v2c0 1.1.9 2 2 2h1v3h7v-4.68c2.62-1.25 4.35-4.08 3.94-7.26m-5.05 5.57-.89.42V19h-3v-3H8v-4H6.7l1.33-2.33C8.21 7.06 10.35 5 13 5c2.76 0 5 2.24 5 5 0 2.09-1.29 3.88-3.11 4.63"></path></svg> <!-- -->Reasoning<!-- --> <svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-q7mezt" focusable="false" aria-hidden="true" viewBox="0 0 24 24"><path d="M8.59 16.59 13.17 12 8.59 7.41 10 6l6 6-6 6z"></path></svg></div></div>
<p>是的，在<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>attention.py</span></span></code></span>的<span data-rehype-pretty-code-figure=""><code data-language="plaintext" data-theme="plastic"><span data-line=""><span>BasicTransformerBlock</span></span></code></span>中，每个 Attention 层前都有一个归一化层（通常是 LayerNorm），这是非常重要的设计：</p>
<h3 id="layernorm-的必要性"><a href="#layernorm-的必要性">LayerNorm 的必要性</a></h3>
<ol>
<li>
<p><strong>训练稳定性</strong>：</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="py" data-theme="plastic"><code data-language="py" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#5F6672;font-style:italic"># 在Self-Attention前应用归一化</span></span>
<span data-line=""><span style="color:#E06C75">if</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm_type </span><span style="color:#E06C75">==</span><span style="color:#A9B2C3"> &quot;</span><span style="color:#98C379">ada_norm</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">:</span></span>
<span data-line=""><span style="color:#A9B2C3">    norm_hidden_states </span><span style="color:#E06C75">=</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm1(hidden_states, timestep)</span></span>
<span data-line=""><span style="color:#E06C75">elif</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm_type </span><span style="color:#E06C75">==</span><span style="color:#A9B2C3"> &quot;</span><span style="color:#98C379">ada_norm_zero</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">:</span></span>
<span data-line=""><span style="color:#A9B2C3">    norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp </span><span style="color:#E06C75">=</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm1(</span></span>
<span data-line=""><span style="color:#A9B2C3">        hidden_states, timestep, class_labels, </span><span style="color:#C6CCD7">hidden_dtype</span><span style="color:#E06C75">=</span><span style="color:#A9B2C3">hidden_states.dtype</span></span>
<span data-line=""><span style="color:#A9B2C3">    )</span></span>
<span data-line=""><span style="color:#E06C75">elif</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm_type </span><span style="color:#E06C75">in</span><span style="color:#A9B2C3"> [</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">layer_norm</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">, </span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">layer_norm_i2vgen</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">]:</span></span>
<span data-line=""><span style="color:#A9B2C3">    norm_hidden_states </span><span style="color:#E06C75">=</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm1(hidden_states)</span></span></code></pre></figure>
</li>
<li>
<p><strong>梯度流动</strong>：</p>
<ul>
<li>归一化层将数据分布标准化，防止梯度消失或爆炸</li>
<li>特别是在深层网络中，保持梯度流动对训练至关重要</li>
</ul>
</li>
</ol>
<h3 id="layernorm-的作用"><a href="#layernorm-的作用">LayerNorm 的作用</a></h3>
<ol>
<li>
<p><strong>特征分布稳定</strong>：</p>
<ul>
<li>将特征归一化到相同尺度，减少批次间差异</li>
<li>使网络对输入变化更加鲁棒，提高泛化能力</li>
</ul>
</li>
<li>
<p><strong>残差连接的配合</strong>：</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="py" data-theme="plastic"><code data-language="py" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#5F6672;font-style:italic"># 应用残差连接</span></span>
<span data-line=""><span style="color:#A9B2C3">hidden_states </span><span style="color:#E06C75">=</span><span style="color:#A9B2C3"> attn_output </span><span style="color:#E06C75">+</span><span style="color:#A9B2C3"> hidden_states</span></span></code></pre></figure>
<ul>
<li>Pre-LayerNorm 结构（先归一化后 Attention）与残差连接配合使用</li>
<li>这种&quot;归一化 → 操作 → 残差&quot;的模式是 Transformer 架构的基本组成部分</li>
</ul>
</li>
<li>
<p><strong>多种归一化选择</strong>：</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="py" data-theme="plastic"><code data-language="py" data-theme="plastic" style="display:grid"><span data-line=""><span style="color:#5F6672;font-style:italic"># 不同类型的归一化层</span></span>
<span data-line=""><span style="color:#E06C75">if</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm_type </span><span style="color:#E06C75">==</span><span style="color:#A9B2C3"> &quot;</span><span style="color:#98C379">ada_norm</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">:</span></span>
<span data-line=""><span style="color:#A9B2C3">    norm_hidden_states </span><span style="color:#E06C75">=</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm1(hidden_states, timestep)</span></span>
<span data-line=""><span style="color:#E06C75">elif</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm_type </span><span style="color:#E06C75">==</span><span style="color:#A9B2C3"> &quot;</span><span style="color:#98C379">ada_norm_zero</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">:</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">    # ...</span></span>
<span data-line=""><span style="color:#E06C75">elif</span><span style="color:#E06C75"> self</span><span style="color:#A9B2C3">.norm_type </span><span style="color:#E06C75">in</span><span style="color:#A9B2C3"> [</span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">layer_norm</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">, </span><span style="color:#A9B2C3">&quot;</span><span style="color:#98C379">layer_norm_i2vgen</span><span style="color:#A9B2C3">&quot;</span><span style="color:#A9B2C3">]:</span></span>
<span data-line=""><span style="color:#5F6672;font-style:italic">    # ...</span></span></code></pre></figure>
<ul>
<li>基本的 LayerNorm</li>
<li>带条件的 AdaLayerNorm（使用 timestep 信息调整）</li>
<li>其他特殊归一化变体，适应不同场景</li>
</ul>
</li>
</ol>
<p>总之，LayerNorm 在 Attention 层前是必不可少的，它是保证 Transformer 结构有效训练和良好性能的关键组件，特别是在生成模型这样需要处理大量复杂特征的深度网络中。</p></div><div class="w-96 text-gray-700 leading-none"><span class="!text-sm"><a class="!inline-block !p-0 !m-0 align-text-bottom" rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" loading="lazy" width="88" height="31" decoding="async" data-nimg="1" class="!m-0 h-4 w-auto pr-1" style="color:transparent;border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png"/></a>This work is licensed under a<!-- --> <a class="underline" rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.</span></div><div class="TagsBox_tagsBox__WzhAf mt-4"><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">#<!-- -->深度学习</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/stable-diffusion">#<!-- -->Stable Diffusion</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/lora">#<!-- -->LoRA</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83">#<!-- -->参数高效微调</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">#<!-- -->模型训练</a><a class="tag-word TagsBox_tag__Rk32C" href="/blog-next/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0">#<!-- -->学习笔记</a></div><div class="mt-4 mb-4 flex justify-center"><div class="ml-0 mr-auto"><a href="/blog-next/learn_from_ai/golang-scheduler-preemption">&lt;- Go语言的调度机制：协作式与抢占式调度分析</a></div><div class="mr-0 ml-auto"><a href="/blog-next/learn_from_ai/mqtt-protocol-principles-applications">详细介绍 MQTT 协议的内容，原理，应用场景 -&gt;</a></div></div><hr class="mt-4"/></article></div></div><style data-emotion="css vkdybf">.css-vkdybf{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 0 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 0) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));min-width:0;box-sizing:border-box;}@media (min-width:900px){.css-vkdybf{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 3 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 3) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}@media (min-width:1200px){.css-vkdybf{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 2 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 2) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}</style><div class="MuiGrid-root MuiGrid-direction-xs-row MuiGrid-grid-xs-0 MuiGrid-grid-md-3 MuiGrid-grid-lg-2 css-vkdybf"><div class="inset-0 w-full h-full flex items-center justify-center bg-transparent"><style data-emotion="css 14awfyb animation-61bdi0">.css-14awfyb{display:inline-block;-webkit-animation:animation-61bdi0 1.4s linear infinite;animation:animation-61bdi0 1.4s linear infinite;color:#1976d2;}@-webkit-keyframes animation-61bdi0{0%{-webkit-transform:rotate(0deg);-moz-transform:rotate(0deg);-ms-transform:rotate(0deg);transform:rotate(0deg);}100%{-webkit-transform:rotate(360deg);-moz-transform:rotate(360deg);-ms-transform:rotate(360deg);transform:rotate(360deg);}}@keyframes animation-61bdi0{0%{-webkit-transform:rotate(0deg);-moz-transform:rotate(0deg);-ms-transform:rotate(0deg);transform:rotate(0deg);}100%{-webkit-transform:rotate(360deg);-moz-transform:rotate(360deg);-ms-transform:rotate(360deg);transform:rotate(360deg);}}</style><span class="MuiCircularProgress-root MuiCircularProgress-indeterminate MuiCircularProgress-colorPrimary css-14awfyb" style="width:40px;height:40px" role="progressbar"><style data-emotion="css 4ejps8">.css-4ejps8{display:block;}</style><svg class="MuiCircularProgress-svg css-4ejps8" viewBox="22 22 44 44"><style data-emotion="css 13odlrs animation-1o38n3e">.css-13odlrs{stroke:currentColor;stroke-dasharray:80px,200px;stroke-dashoffset:0;-webkit-animation:animation-1o38n3e 1.4s ease-in-out infinite;animation:animation-1o38n3e 1.4s ease-in-out infinite;}@-webkit-keyframes animation-1o38n3e{0%{stroke-dasharray:1px,200px;stroke-dashoffset:0;}50%{stroke-dasharray:100px,200px;stroke-dashoffset:-15px;}100%{stroke-dasharray:1px,200px;stroke-dashoffset:-126px;}}@keyframes animation-1o38n3e{0%{stroke-dasharray:1px,200px;stroke-dashoffset:0;}50%{stroke-dasharray:100px,200px;stroke-dashoffset:-15px;}100%{stroke-dasharray:1px,200px;stroke-dashoffset:-126px;}}</style><circle class="MuiCircularProgress-circle MuiCircularProgress-circleIndeterminate css-13odlrs" cx="44" cy="44" r="20.2" fill="none" stroke-width="3.6"></circle></svg></span></div></div></div><footer class="DefaultLayout_footer__aWV4u"><style data-emotion="css vktxal">.css-vktxal{--Grid-columns:12;--Grid-columnSpacing:0px;--Grid-rowSpacing:0px;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;min-width:0;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:var(--Grid-rowSpacing) var(--Grid-columnSpacing);width:100%;max-width:80rem;margin-left:auto;margin-right:auto;padding:0.5rem;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}.css-vktxal >*{--Grid-parent-columns:12;}.css-vktxal >*{--Grid-parent-columnSpacing:0px;}.css-vktxal >*{--Grid-parent-rowSpacing:0px;}</style><div class="MuiGrid-root MuiGrid-container MuiGrid-direction-xs-row css-vktxal"><style data-emotion="css 9gdssj">.css-9gdssj{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 0 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 0) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));min-width:0;box-sizing:border-box;}@media (min-width:900px){.css-9gdssj{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 0 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 0) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}@media (min-width:1200px){.css-9gdssj{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 2 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 2) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}</style><div class="MuiGrid-root MuiGrid-direction-xs-row MuiGrid-grid-xs-0 MuiGrid-grid-md-0 MuiGrid-grid-lg-2 css-9gdssj"></div><style data-emotion="css 9h67uz">.css-9h67uz{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 12 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 12) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));min-width:0;box-sizing:border-box;}@media (min-width:900px){.css-9h67uz{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 9 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 9) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}@media (min-width:1200px){.css-9h67uz{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 8 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 8) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}</style><div class="MuiGrid-root MuiGrid-direction-xs-row MuiGrid-grid-xs-12 MuiGrid-grid-md-9 MuiGrid-grid-lg-8 css-9h67uz"><div class="flex flex-row justify-center items-center"><div class="DefaultLayout_footerLeft__Qn_VV">© 2023 Ryo Jerry Yu. All rights reserved.</div><div class="DefaultLayout_footerRight__GlReP"><a title="Twitter" href="https://twitter.com/ryo_okami"><svg class="h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out mx-1 md:mx-2" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a title="GitHub" href="https://github.com/RyoJerryYu"><svg class="h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out mx-1 md:mx-2" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a title="Pixiv" href="https://www.pixiv.net/users/9159893"><svg class="h-6 w-6 fill-gray-300 hover:fill-white transition-all ease-in-out mx-1 md:mx-2" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4.935 0A4.924 4.924 0 0 0 0 4.935v14.13A4.924 4.924 0 0 0 4.935 24h14.13A4.924 4.924 0 0 0 24 19.065V4.935A4.924 4.924 0 0 0 19.065 0zm7.81 4.547c2.181 0 4.058.676 5.399 1.847a6.118 6.118 0 0 1 2.116 4.66c.005 1.854-.88 3.476-2.257 4.563-1.375 1.092-3.225 1.697-5.258 1.697-2.314 0-4.46-.842-4.46-.842v2.718c.397.116 1.048.365.635.779H5.79c-.41-.41.19-.65.644-.779V7.666c-1.053.81-1.593 1.51-1.868 2.031.32 1.02-.284.969-.284.969l-1.09-1.73s3.868-4.39 9.553-4.39zm-.19.971c-1.423-.003-3.184.473-4.27 1.244v8.646c.988.487 2.484.832 4.26.832h.01c1.596 0 2.98-.593 3.93-1.533.952-.948 1.486-2.183 1.492-3.683-.005-1.54-.504-2.864-1.42-3.86-.918-.992-2.274-1.645-4.002-1.646Z"></path></svg></a></div></div></div><style data-emotion="css vkdybf">.css-vkdybf{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 0 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 0) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));min-width:0;box-sizing:border-box;}@media (min-width:900px){.css-vkdybf{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 3 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 3) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}@media (min-width:1200px){.css-vkdybf{-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;-webkit-flex-basis:auto;-ms-flex-preferred-size:auto;flex-basis:auto;width:calc(100% * 2 / var(--Grid-parent-columns) - (var(--Grid-parent-columns) - 2) * (var(--Grid-parent-columnSpacing) / var(--Grid-parent-columns)));}}</style><div class="MuiGrid-root MuiGrid-direction-xs-row MuiGrid-grid-xs-0 MuiGrid-grid-md-3 MuiGrid-grid-lg-2 css-vkdybf"></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"stable-diffusion-lora-training-methods","tags":[{"tag":"深度学习","slug":"深度学习","path":"/tags/深度学习","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/deep-learning-model-formats"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-backpropagation-mechanism"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"Stable Diffusion","slug":"stable-diffusion","path":"/tags/stable-diffusion","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"LoRA","slug":"lora","path":"/tags/lora","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"参数高效微调","slug":"参数高效微调","path":"/tags/参数高效微调","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"模型训练","slug":"模型训练","path":"/tags/模型训练","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]},{"tag":"学习笔记","slug":"学习笔记","path":"/tags/学习笔记","postSlugs":[{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/deep-learning-model-formats"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/opencv-coordinate-system-conventions"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/executable-file-formats"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/lora-matrix-initialization-strategy"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/pytorch-backpropagation-mechanism"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/mqtt-protocol-principles-applications"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-unet-structure"},{"postType":"learn_from_ai","postPagePath":"/learn_from_ai/stable-diffusion-lora-training-methods"}]}],"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    figure: \"figure\",\n    h2: \"h2\",\n    h3: \"h3\",\n    h4: \"h4\",\n    hr: \"hr\",\n    li: \"li\",\n    ol: \"ol\",\n    p: \"p\",\n    pre: \"pre\",\n    span: \"span\",\n    strong: \"strong\",\n    table: \"table\",\n    tbody: \"tbody\",\n    td: \"td\",\n    th: \"th\",\n    thead: \"thead\",\n    tr: \"tr\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  }, {ObsidianCallout} = _components;\n  if (!ObsidianCallout) _missingMdxReference(\"ObsidianCallout\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"本文详细介绍了如何使用 PyTorch 从零开始编写 Stable Diffusion LoRA 训练脚本，包括数据预处理、模型修改、训练循环、参数保存与加载等关键步骤。特别强调了 LoRA 层的手动实现和在 UNet 的 Cross-Attention 层注入 LoRA 的原因，以及在其他层应用 LoRA 的可能性和注意事项。此外，还提供了代码示例和参数效率的讨论，帮助读者深入理解 LoRA 在 Stable Diffusion 微调中的应用。\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"q-训练一个-stable-diffusion-的-lora-模型要有哪些步骤\",\n      children: _jsx(_components.a, {\n        href: \"#q-训练一个-stable-diffusion-的-lora-模型要有哪些步骤\",\n        children: \"Q: 训练一个 Stable Diffusion 的 LoRA 模型，要有哪些步骤？\"\n      })\n    }), \"\\n\", _jsxs(ObsidianCallout, {\n      type: \"reasoning\",\n      title: \"\",\n      foldable: true,\n      isCollapsed: true,\n      children: [_jsx(_components.p, {\n        children: \"好的，用户问的是训练 Stable Diffusion 的 LoRA 模型需要哪些步骤。首先，我需要回忆一下 LoRA 的基本概念。LoRA 是低秩适应，一种用于微调大模型的技术，可以在保持原模型参数不变的情况下，通过添加额外的低秩矩阵来调整模型输出。这样做的好处是节省计算资源和存储空间，因为不需要训练整个模型。\"\n      }), _jsx(_components.p, {\n        children: \"接下来，用户可能已经了解 Stable Diffusion 的基本原理，但需要具体步骤。我需要分解训练流程，从准备数据到最终推理。首先，数据准备是关键。用户需要收集高质量图片，可能还需要标注。这里需要注意图片的多样性和数量，以及正确的标注格式，比如 TXT 文件。\"\n      }), _jsx(_components.p, {\n        children: \"然后是环境配置。用户可能需要安装 PyTorch、Diffusers 库、Transformers 库，还有 LoRA 相关的库，比如 peft。如果用户不熟悉这些工具，可能需要给出安装命令示例，比如 pip install。\"\n      }), _jsx(_components.p, {\n        children: \"接下来是模型准备，选择适合的 Stable Diffusion 版本，如 SD 1.5 或 2.1。需要下载预训练模型，并配置 LoRA 参数，比如 rank 值。这里可能需要解释 rank 的意义，如何选择合适的值，比如 8 到 64 之间，根据任务复杂度调整。\"\n      }), _jsx(_components.p, {\n        children: \"然后是训练配置，设置学习率、批次大小、epoch 数等超参数。需要提醒用户学习率不宜过高，使用预热和衰减策略。可能还要提到优化器的选择，比如 AdamW，以及混合精度训练以节省显存。\"\n      }), _jsx(_components.p, {\n        children: \"训练过程中，用户可能需要监控 loss 和生成效果，调整参数。保存检查点也是重要的，防止训练中断。这部分需要强调耐心和多次实验，因为微调可能需要多次尝试。\"\n      }), _jsx(_components.p, {\n        children: \"训练完成后，合并 LoRA 权重到原模型，或者直接加载进行推理。需要给出示例代码，展示如何加载并使用 LoRA 模型生成图片。\"\n      }), _jsx(_components.p, {\n        children: \"最后，常见问题部分要考虑用户可能遇到的困难，比如需要多少数据，是否需要高质量标注，训练时间，以及是否需要高端 GPU。这些是实际训练中常见的问题，提前解答能帮助用户更好准备。\"\n      }), _jsx(_components.p, {\n        children: \"用户可能的深层需求是希望有一个清晰的步骤指南，避免遗漏关键环节。可能他们之前尝试过但遇到问题，或者刚入门需要从零开始的指导。因此，步骤要详细，但不过于技术化，保持易懂。同时，强调数据质量和参数调整的重要性，因为这是影响结果的关键因素。\"\n      })]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"训练 Stable Diffusion 的 LoRA（Low-Rank Adaptation）模型是一种高效的微调方法，可以在保留原模型大部分参数的同时，通过低秩矩阵调整模型行为。以下是详细的步骤指南：\"\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"1-数据准备\",\n      children: _jsx(_components.a, {\n        href: \"#1-数据准备\",\n        children: _jsx(_components.strong, {\n          children: \"1. 数据准备\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"收集数据集\"\n        }), \":\", \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"需要与目标风格/主题相关的高质量图片（建议至少 \", _jsx(_components.strong, {\n              children: \"100-200 张\"\n            }), \"，数量越多效果越好）。\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"图片格式通常为 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \".jpg\"\n                  })\n                })\n              })\n            }), \" 或 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \".png\"\n                  })\n                })\n              })\n            }), \"，分辨率建议与 Stable Diffusion 的基础模型对齐（如 512x512）。\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"标注数据\"\n        }), \":\", \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"每张图片需搭配文本描述（\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \".txt\"\n                  })\n                })\n              })\n            }), \" 文件），描述需清晰简洁（如 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"\\\"a photo of a [keyword], [style], [details]\\\"\"\n                  })\n                })\n              })\n            }), \"）。\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"若训练特定对象（如人物、画风），确保描述中\", _jsx(_components.strong, {\n              children: \"统一使用触发词\"\n            }), \"（如 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"\\\"my_cat_v1\\\"\"\n                  })\n                })\n              })\n            }), \"）。\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"2-环境配置\",\n      children: _jsx(_components.a, {\n        href: \"#2-环境配置\",\n        children: _jsx(_components.strong, {\n          children: \"2. 环境配置\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"安装依赖库\"\n          }), \":\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"bash\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"bash\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  style: {\n                    color: \"#5F6672\",\n                    fontStyle: \"italic\"\n                  },\n                  children: \"# 基础环境\"\n                })\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#B57EDC\"\n                  },\n                  children: \"pip\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" install\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" torch\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" torchvision\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#56B6C2\"\n                  },\n                  children: \" --extra-index-url\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" https://download.pytorch.org/whl/cu117\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#B57EDC\"\n                  },\n                  children: \"pip\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" install\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" diffusers\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" transformers\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" accelerate\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" peft\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#B57EDC\"\n                  },\n                  children: \"pip\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" install\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" datasets\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" huggingface_hub\"\n                })]\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  style: {\n                    color: \"#5F6672\",\n                    fontStyle: \"italic\"\n                  },\n                  children: \"# 可选：xformers 优化（A卡需调整）\"\n                })\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#B57EDC\"\n                  },\n                  children: \"pip\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" install\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \" xformers\"\n                })]\n              })]\n            })\n          })\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"硬件要求\"\n          }), \":\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"GPU 显存至少 \", _jsx(_components.strong, {\n              children: \"8GB\"\n            }), \"（如 RTX 3060/4090），24GB 可支持更大批次。\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"使用 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"accelerate\"\n                  })\n                })\n              })\n            }), \" 库配置分布式训练（可选）。\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"3-模型准备\",\n      children: _jsx(_components.a, {\n        href: \"#3-模型准备\",\n        children: _jsx(_components.strong, {\n          children: \"3. 模型准备\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"选择基础模型\"\n          }), \":\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"常用模型：\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"runwayml/stable-diffusion-v1-5\"\n                  })\n                })\n              })\n            }), \"、\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"stabilityai/stable-diffusion-2-1\"\n                  })\n                })\n              })\n            }), \"。\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"从 Hugging Face 下载并缓存模型：\", \"\\n\", _jsx(_components.figure, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.pre, {\n                tabIndex: \"0\",\n                \"data-language\": \"python\",\n                \"data-theme\": \"plastic\",\n                children: _jsxs(_components.code, {\n                  \"data-language\": \"python\",\n                  \"data-theme\": \"plastic\",\n                  style: {\n                    display: \"grid\"\n                  },\n                  children: [_jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"from\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" diffusers \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"import\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" StableDiffusionPipeline\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"pipeline \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" StableDiffusionPipeline.from_pretrained(\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#98C379\"\n                      },\n                      children: \"runwayml/stable-diffusion-v1-5\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \")\"\n                    })]\n                  })]\n                })\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"配置 LoRA 参数\"\n          }), \":\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"Rank（秩）\"\n            }), \": 控制低秩矩阵的维度（典型值：4-64，越大拟合能力越强，但可能过拟合）。\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"在训练脚本中设置 LoRA 层（通常作用于 Cross-Attention 模块）：\", \"\\n\", _jsx(_components.figure, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.pre, {\n                tabIndex: \"0\",\n                \"data-language\": \"python\",\n                \"data-theme\": \"plastic\",\n                children: _jsxs(_components.code, {\n                  \"data-language\": \"python\",\n                  \"data-theme\": \"plastic\",\n                  style: {\n                    display: \"grid\"\n                  },\n                  children: [_jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"from\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" peft \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"import\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" LoraConfig\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"lora_config \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" LoraConfig(\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#C6CCD7\"\n                      },\n                      children: \"    r\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#56B6C2\"\n                      },\n                      children: \"8\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \",                \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#5F6672\",\n                        fontStyle: \"italic\"\n                      },\n                      children: \"# Rank 值\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#C6CCD7\"\n                      },\n                      children: \"    target_modules\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"[\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#98C379\"\n                      },\n                      children: \"to_k\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \", \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#98C379\"\n                      },\n                      children: \"to_q\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \", \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#98C379\"\n                      },\n                      children: \"to_v\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \", \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#98C379\"\n                      },\n                      children: \"to_out.0\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"],  \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#5F6672\",\n                        fontStyle: \"italic\"\n                      },\n                      children: \"# 注入的模块\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#C6CCD7\"\n                      },\n                      children: \"    init_lora_weights\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#98C379\"\n                      },\n                      children: \"gaussian\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \",\"\n                    })]\n                  }), \"\\n\", _jsx(_components.span, {\n                    \"data-line\": \"\",\n                    children: _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \")\"\n                    })\n                  })]\n                })\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"4-训练配置\",\n      children: _jsx(_components.a, {\n        href: \"#4-训练配置\",\n        children: _jsx(_components.strong, {\n          children: \"4. 训练配置\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"超参数设置\"\n          }), \":\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"yaml\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"yaml\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E5C07B\"\n                  },\n                  children: \"batch_size\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \": \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"1-4（根据显存调整）\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E5C07B\"\n                  },\n                  children: \"learning_rate\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \": \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"1e-5 到 1e-4（常用 1e-4）\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E5C07B\"\n                  },\n                  children: \"num_epochs\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \": \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"50-200（观察 loss 调整）\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E5C07B\"\n                  },\n                  children: \"optimizer\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \": \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"AdamW（带权重衰减）\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E5C07B\"\n                  },\n                  children: \"mixed_precision\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \": \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"fp16（节省显存）\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E5C07B\"\n                  },\n                  children: \"gradient_checkpointing\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \": \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"True（进一步省显存）\"\n                })]\n              })]\n            })\n          })\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"训练脚本\"\n          }), \":\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"使用 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"diffusers\"\n                  })\n                })\n              })\n            }), \" 的 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"train_text_to_image.py\"\n                  })\n                })\n              })\n            }), \" 或自定义训练循环。\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"关键代码片段：\", \"\\n\", _jsx(_components.figure, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.pre, {\n                tabIndex: \"0\",\n                \"data-language\": \"python\",\n                \"data-theme\": \"plastic\",\n                children: _jsxs(_components.code, {\n                  \"data-language\": \"python\",\n                  \"data-theme\": \"plastic\",\n                  style: {\n                    display: \"grid\"\n                  },\n                  children: [_jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"from\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" diffusers \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"import\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" DDPMScheduler\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"scheduler \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" DDPMScheduler.from_pretrained(model_id, \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#C6CCD7\"\n                      },\n                      children: \"subfolder\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#98C379\"\n                      },\n                      children: \"scheduler\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"\\\"\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \")\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"trainer \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \" StableDiffusionTrainer(\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#C6CCD7\"\n                      },\n                      children: \"    model\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"pipeline.unet,\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#C6CCD7\"\n                      },\n                      children: \"    args\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"training_args,\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#C6CCD7\"\n                      },\n                      children: \"    data_collator\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"collate_fn,\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#C6CCD7\"\n                      },\n                      children: \"    train_dataset\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"dataset,\"\n                    })]\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#C6CCD7\"\n                      },\n                      children: \"    scheduler\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#E06C75\"\n                      },\n                      children: \"=\"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"scheduler,\"\n                    })]\n                  }), \"\\n\", _jsx(_components.span, {\n                    \"data-line\": \"\",\n                    children: _jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \")\"\n                    })\n                  }), \"\\n\", _jsxs(_components.span, {\n                    \"data-line\": \"\",\n                    children: [_jsx(_components.span, {\n                      style: {\n                        color: \"#A9B2C3\"\n                      },\n                      children: \"trainer.add_callback(LoRACallback(lora_config))  \"\n                    }), _jsx(_components.span, {\n                      style: {\n                        color: \"#5F6672\",\n                        fontStyle: \"italic\"\n                      },\n                      children: \"# 注入 LoRA\"\n                    })]\n                  })]\n                })\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"5-训练过程\",\n      children: _jsx(_components.a, {\n        href: \"#5-训练过程\",\n        children: _jsx(_components.strong, {\n          children: \"5. 训练过程\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"监控与调试\"\n          }), \":\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"观察 Loss 曲线（理想情况应稳步下降后趋于平稳）。\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"定期生成测试图片（如每 100 步）检查过拟合/欠拟合。\"\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"调整策略：\", \"\\n\", _jsxs(_components.ul, {\n              children: [\"\\n\", _jsx(_components.li, {\n                children: \"Loss 不降：提高学习率、增加 Rank 值。\"\n              }), \"\\n\", _jsx(_components.li, {\n                children: \"Loss 震荡：降低学习率、增大批次大小。\"\n              }), \"\\n\"]\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"保存检查点\"\n          }), \":\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"每隔一定步数保存中间模型（如 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"./output/checkpoint-500\"\n                  })\n                })\n              })\n            }), \"）。\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"最终模型保存为 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \".safetensors\"\n                  })\n                })\n              })\n            }), \" 或合并到原模型。\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"6-推理与部署\",\n      children: _jsx(_components.a, {\n        href: \"#6-推理与部署\",\n        children: _jsx(_components.strong, {\n          children: \"6. 推理与部署\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"加载 LoRA 权重\"\n          }), \":\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"python\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"python\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"from\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" diffusers \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"import\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" StableDiffusionPipeline\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"pipeline \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" StableDiffusionPipeline.from_pretrained(\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"runwayml/stable-diffusion-v1-5\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \")\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"pipeline.unet.load_attn_procs(\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"path/to/lora_weights.safetensors\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \")\"\n                })]\n              })]\n            })\n          })\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"生成图片\"\n          }), \":\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"python\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"python\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"prompt \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" \\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"A photo of my_cat_v1 sitting on a sofa, best quality\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"image \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" pipeline(prompt, \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#C6CCD7\"\n                  },\n                  children: \"num_inference_steps\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#56B6C2\"\n                  },\n                  children: \"50\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \").images[\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#56B6C2\"\n                  },\n                  children: \"0\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"]\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"image.save(\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"output.png\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \")\"\n                })]\n              })]\n            })\n          })\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"常见问题\",\n      children: _jsx(_components.a, {\n        href: \"#常见问题\",\n        children: _jsx(_components.strong, {\n          children: \"常见问题\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"Q\"\n          }), \": 需要多少数据？\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"A\"\n            }), \": 简单风格/物体约 100 张，复杂概念需 500+ 张。\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"Q\"\n          }), \": 训练时间多久？\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"A\"\n            }), \": 单 GPU 约 1-6 小时（取决于数据量和参数）。\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"Q\"\n          }), \": 能否用 Colab 免费版？\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.strong, {\n              children: \"A\"\n            }), \": 可以，但需启用 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"fp16\"\n                  })\n                })\n              })\n            }), \" 和梯度检查点，批次设为 1。\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.p, {\n      children: \"通过以上步骤，你可以高效地训练出针对特定任务的 LoRA 模型，显著降低资源消耗同时保持生成质量。\"\n    }), \"\\n\", _jsxs(ObsidianCallout, {\n      type: \"info\",\n      title: \"关于数据量的疑问\",\n      children: [_jsx(ObsidianCallout, {\n        type: \"query\",\n        title: \"\",\n        children: _jsx(_components.p, {\n          children: \"你确定训练一个 LoRA 需要 100 多张图吗？\"\n        })\n      }), _jsx(_components.p, {\n        children: \"Reasoning 过程省略\"\n      }), _jsx(ObsidianCallout, {\n        type: \"ai\",\n        title: \"\",\n        children: _jsx(_components.p, {\n          children: \"训练 LoRA 模型所需的图片数量并非固定，具体取决于训练目标、数据质量和参数设置。根据多个来源的建议，最低可接受数量为 10-20 张高质量图片，而复杂场景或高精度需求可能需要 50 张以上。\\n...\"\n        })\n      })]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"q-我有-python-与-pytorch-基础需要自己写一个-lora-训练脚本应如何写\",\n      children: _jsx(_components.a, {\n        href: \"#q-我有-python-与-pytorch-基础需要自己写一个-lora-训练脚本应如何写\",\n        children: \"Q: 我有 Python 与 PyTorch 基础。需要自己写一个 LoRA 训练脚本，应如何写？\"\n      })\n    }), \"\\n\", _jsxs(ObsidianCallout, {\n      type: \"reasoning\",\n      title: \"\",\n      foldable: true,\n      isCollapsed: true,\n      children: [_jsx(_components.p, {\n        children: \"嗯，用户之前问过如何训练 Stable Diffusion 的 LoRA 模型，现在他有了 Python 和 PyTorch 基础，想自己写一个 LoRA 训练脚本。我需要仔细分析他的需求。首先，他可能已经了解了 LoRA 的基本步骤，现在需要具体的代码指导和实现细节。用户可能希望从零开始构建训练脚本，而不仅仅是使用现有的库，比如 diffusers 或者 peft。不过，也有可能他希望结合这些库来简化流程。需要明确他的意图。\"\n      }), _jsx(_components.p, {\n        children: \"用户提到“自己写一个”，可能意味着他希望理解底层实现，而不仅仅是调用高级 API。不过，LoRA 的实现涉及到修改模型的注意力层，这部分如果完全从零开始可能会比较复杂。需要平衡代码的复杂性和可理解性。可能需要结合 PyTorch 的基础操作和现有的库，比如 huggingface 的 transformers 或 diffusers，来构建脚本。\"\n      }), _jsx(_components.p, {\n        children: \"首先，确定用户的基础：有 Python 和 PyTorch 基础，可能对深度学习模型训练有基本了解，但对 LoRA 的具体实现不熟悉。需要分步骤解释如何将 LoRA 应用到 Stable Diffusion 的 UNet 模型中，特别是注意力层部分。\"\n      }), _jsx(_components.p, {\n        children: \"接下来，可能需要考虑以下几点：\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"数据准备部分：用户需要加载数据集，可能已经准备好了图片和对应的文本描述，需要将其转换为模型可接受的输入格式，比如 tokenized 的文本和图像编码后的潜在表示。\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"模型加载：需要加载预训练的 Stable Diffusion 模型，特别是 UNet 部分，并修改其注意力层以加入 LoRA 参数。\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"LoRA 层的实现：如何在 PyTorch 中为线性层添加 LoRA 适配器，可能需要创建一个新的模块，将原始权重冻结，并添加低秩矩阵 A 和 B。\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"训练循环的设置：包括损失函数（如 MSE 损失）、优化器（如 AdamW）、学习率调度等。\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"参数更新：确保在训练过程中只更新 LoRA 的参数，而原始模型参数保持冻结。\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"保存和加载 LoRA 权重：训练完成后如何提取 LoRA 的参数，并在推理时加载它们。\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"可能还需要考虑混合精度训练、梯度检查点等优化技术，以节省显存。同时，用户可能在资源有限的环境下训练，比如使用 Colab，所以需要代码尽可能高效。\"\n      }), _jsx(_components.p, {\n        children: \"另外，用户可能在之前的回答中已经看到了使用 peft 库的示例，但现在他希望自己实现，所以需要避免直接使用 peft，而是手动添加 LoRA 层。这需要详细解释 LoRA 的原理，比如低秩分解，如何将原始权重 W 替换为 W + BA，其中 A 和 B 是低秩矩阵。\"\n      }), _jsx(_components.p, {\n        children: \"需要确保代码示例清晰，步骤明确，并且指出关键部分，比如如何修改注意力层的线性层，如何冻结原始参数，如何只训练 LoRA 的参数。同时，可能需要给出数据预处理的具体示例，比如使用 Autoencoder 将图像编码为潜在空间，以及使用 CLIP 文本编码器处理文本输入。\"\n      }), _jsx(_components.p, {\n        children: \"可能还需要处理学习率的设置，因为微调通常使用较低的学习率，以及如何监控训练过程，比如损失值的下降和生成样本的质量检查。\"\n      }), _jsx(_components.p, {\n        children: \"最后，需要提醒用户注意常见问题，比如过拟合、训练时间、超参数调整等，并提供调试建议。可能还需要比较手动实现和使用现有库的优缺点，帮助用户做出选择。\"\n      }), _jsx(_components.p, {\n        children: \"总结，回答需要包括以下部分：\"\n      }), _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"LoRA 层的 PyTorch 实现\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"修改 Stable Diffusion 的 UNet 模型以加入 LoRA\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"数据预处理流程\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"训练循环的构建\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"参数保存与加载\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"示例代码和关键解释\"\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsx(_components.p, {\n            children: \"常见问题与优化建议\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), _jsx(_components.p, {\n        children: \"需要确保代码示例正确，并且用户能够根据步骤逐步实现，同时解释每一步的目的和原理，帮助用户理解而不仅仅是复制代码。\"\n      })]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"以下是基于 PyTorch 实现 Stable Diffusion LoRA 训练脚本的完整指南，从底层原理到代码实现逐步解析。假设你已准备好数据集（图片+文本描述），我们将以 \", _jsx(_components.strong, {\n        children: \"手动实现 LoRA 层\"\n      }), \" 为核心，不依赖 \", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"peft\"\n            })\n          })\n        })\n      }), \" 库。\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"1-lora-层的原理与实现\",\n      children: _jsxs(_components.a, {\n        href: \"#1-lora-层的原理与实现\",\n        children: [\"1. \", _jsx(_components.strong, {\n          children: \"LoRA 层的原理与实现\"\n        })]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LoRA 的核心思想：在原始权重矩阵旁增加一个低秩矩阵，只训练这部分参数。\"\n    }), \"\\n\", _jsx(_components.figure, {\n      \"data-rehype-pretty-code-figure\": \"\",\n      children: _jsx(_components.pre, {\n        tabIndex: \"0\",\n        \"data-language\": \"python\",\n        \"data-theme\": \"plastic\",\n        children: _jsxs(_components.code, {\n          \"data-language\": \"python\",\n          \"data-theme\": \"plastic\",\n          style: {\n            display: \"grid\"\n          },\n          children: [_jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"import\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" torch\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"import\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" torch.nn \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"as\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" nn\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"class\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E5C07B\"\n              },\n              children: \" LoRALayer\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#D19A66\"\n              },\n              children: \"nn\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#D19A66\"\n              },\n              children: \"Module\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"):\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"    def\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" __init__\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"original_layer\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"rank\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"8\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"alpha\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"16\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"):\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E5C07B\"\n              },\n              children: \"        super\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"().\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \"__init__\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"()\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".original \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" original_layer  \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 原始线性层 (e.g., nn.Linear)\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".original.requires_grad_(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"False\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")  \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 冻结原始参数\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        in_dim \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" original_layer.in_features\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        out_dim \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" original_layer.out_features\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".rank \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" rank\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".alpha \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" alpha\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"        # 初始化低秩矩阵 A 和 B\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".A \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" nn.Parameter(torch.randn(in_dim, rank))\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".B \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" nn.Parameter(torch.zeros(rank, out_dim))\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"    def\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" forward\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"x\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"):\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"        # 原始输出 + 低秩增量\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        orig_out \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".original(x)\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        lora_out \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" x \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"@\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".A \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"@\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".B  \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# [batch, out_dim]\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        return\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" orig_out \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"+\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".alpha \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"*\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" lora_out\"\n            })]\n          })]\n        })\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"2-修改-stable-diffusion-的-unet\",\n      children: _jsxs(_components.a, {\n        href: \"#2-修改-stable-diffusion-的-unet\",\n        children: [\"2. \", _jsx(_components.strong, {\n          children: \"修改 Stable Diffusion 的 UNet\"\n        })]\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"需将 LoRA 注入到 UNet 的 \", _jsx(_components.strong, {\n        children: \"Cross-Attention 层\"\n      }), \"（关键步骤）：\"]\n    }), \"\\n\", _jsx(_components.figure, {\n      \"data-rehype-pretty-code-figure\": \"\",\n      children: _jsx(_components.pre, {\n        tabIndex: \"0\",\n        \"data-language\": \"python\",\n        \"data-theme\": \"plastic\",\n        children: _jsxs(_components.code, {\n          \"data-language\": \"python\",\n          \"data-theme\": \"plastic\",\n          style: {\n            display: \"grid\"\n          },\n          children: [_jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"from\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" diffusers \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"import\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" UNet2DConditionModel\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"def\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" inject_lora\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"unet\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"rank\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"8\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"alpha\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"16\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"):\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"    # 遍历 UNet 的所有注意力层\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    for\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" name, module \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" unet.named_modules():\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" \\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"attn2\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" name \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"and\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" \\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"to_\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" name:  \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 定位 Cross-Attention 的 Q/K/V/Out 层\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"            if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" isinstance\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(module, nn.Linear):\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"                # 替换原始 Linear 层为 LoRA 层\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"                lora_layer \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" LoRALayer(module, \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"rank\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"rank, \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"alpha\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"alpha)\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"                # 通过字符串路径替换模块 (需要解析层级结构)\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"                parent \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" unet\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"                parts \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" name.split(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"'\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \".\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"'\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"                for\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" part \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" parts[:\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"-\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"1\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"]:\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"                    parent \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" getattr\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(parent, part)\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \"                setattr\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(parent, parts[\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"-\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"1\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"], lora_layer)\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    return\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" unet\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 加载原始 UNet 并注入 LoRA\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"unet \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" UNet2DConditionModel.from_pretrained(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"runwayml/stable-diffusion-v1-5\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"subfolder\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"unet\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"unet \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" inject_lora(unet, \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"rank\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"8\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          })]\n        })\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"3-数据预处理\",\n      children: _jsxs(_components.a, {\n        href: \"#3-数据预处理\",\n        children: [\"3. \", _jsx(_components.strong, {\n          children: \"数据预处理\"\n        })]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"将图片和文本转换为模型输入：\"\n    }), \"\\n\", _jsx(_components.figure, {\n      \"data-rehype-pretty-code-figure\": \"\",\n      children: _jsx(_components.pre, {\n        tabIndex: \"0\",\n        \"data-language\": \"python\",\n        \"data-theme\": \"plastic\",\n        children: _jsxs(_components.code, {\n          \"data-language\": \"python\",\n          \"data-theme\": \"plastic\",\n          style: {\n            display: \"grid\"\n          },\n          children: [_jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"from\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" torchvision \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"import\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" transforms\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"from\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" transformers \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"import\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" CLIPTokenizer, CLIPTextModel\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 加载 Stable Diffusion 的文本编码器\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"tokenizer \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" CLIPTokenizer.from_pretrained(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"openai/clip-vit-large-patch14\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"text_encoder \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" CLIPTextModel.from_pretrained(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"openai/clip-vit-large-patch14\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 图像预处理（转换为潜在空间）\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"vae \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" AutoencoderKL.from_pretrained(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"runwayml/stable-diffusion-v1-5\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"subfolder\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"vae\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"def\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" preprocess_image\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"image_path\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"):\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    image \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" Image.open(image_path).convert(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"RGB\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    transform \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" transforms.Compose([\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        transforms.Resize(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"512\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"),\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        transforms.CenterCrop(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"512\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"),\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        transforms.ToTensor(),\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    ])\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    image \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" transform(image).unsqueeze(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"0\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")  \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# [1, 3, 512, 512]\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    with\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" torch.no_grad():\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        latents \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" vae.encode(image).latent_dist.sample() \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"*\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \" 0.18215\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"  # [1, 4, 64, 64]\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    return\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" latents\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 文本编码\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"def\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" encode_text\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"text\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"):\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    inputs \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" tokenizer(text, \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"return_tensors\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"pt\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"padding\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"max_length\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"max_length\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"77\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"truncation\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"True\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    with\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" torch.no_grad():\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        text_embeddings \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" text_encoder(inputs.input_ids)[\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"0\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"]  \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# [1, 77, 768]\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    return\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" text_embeddings\"\n            })]\n          })]\n        })\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"4-训练循环\",\n      children: _jsxs(_components.a, {\n        href: \"#4-训练循环\",\n        children: [\"4. \", _jsx(_components.strong, {\n          children: \"训练循环\"\n        })]\n      })\n    }), \"\\n\", _jsx(_components.figure, {\n      \"data-rehype-pretty-code-figure\": \"\",\n      children: _jsx(_components.pre, {\n        tabIndex: \"0\",\n        \"data-language\": \"python\",\n        \"data-theme\": \"plastic\",\n        children: _jsxs(_components.code, {\n          \"data-language\": \"python\",\n          \"data-theme\": \"plastic\",\n          style: {\n            display: \"grid\"\n          },\n          children: [_jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"import\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" torch.optim \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"as\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" optim\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"from\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" diffusers \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"import\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" DDPMScheduler\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 初始化噪声调度器\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"noise_scheduler \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" DDPMScheduler(\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    num_train_timesteps\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"1000\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \",\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    beta_start\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"0.0001\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \",\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    beta_end\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"0.02\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \",\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    beta_schedule\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"linear\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 优化器（仅训练 LoRA 参数）\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"optimizer \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" optim.AdamW(\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \"    filter\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"lambda\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \" p\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \": p.requires_grad, unet.parameters()),  \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 只选择可训练参数\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    lr\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"1e-4\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \",\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    weight_decay\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"1e-4\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 训练循环\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"for\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" epoch \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" range\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(num_epochs):\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    for\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" batch \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" dataloader:\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"        # 加载数据\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        images, texts \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" batch\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        latents \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" preprocess_image(images)\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        text_embeds \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" encode_text(texts)\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"        # 添加噪声\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        noise \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" torch.randn_like(latents)\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        timesteps \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" torch.randint(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"0\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", noise_scheduler.num_train_timesteps, (latents.shape[\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"0\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"],))\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        noisy_latents \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" noise_scheduler.add_noise(latents, noise, timesteps)\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"        # 前向传播\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        noise_pred \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" unet(noisy_latents, timesteps, text_embeds).sample\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"        # 计算损失\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        loss \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" nn.functional.mse_loss(noise_pred, noise)\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"        # 反向传播\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        optimizer.zero_grad()\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        loss.backward()\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        optimizer.step()\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \"    print\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"f\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"\\\"Epoch \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"{\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"epoch\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"}\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \", Loss: \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"{\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"loss.item()\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"}\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          })]\n        })\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"5-保存-lora-权重\",\n      children: _jsxs(_components.a, {\n        href: \"#5-保存-lora-权重\",\n        children: [\"5. \", _jsx(_components.strong, {\n          children: \"保存 LoRA 权重\"\n        })]\n      })\n    }), \"\\n\", _jsx(_components.figure, {\n      \"data-rehype-pretty-code-figure\": \"\",\n      children: _jsx(_components.pre, {\n        tabIndex: \"0\",\n        \"data-language\": \"python\",\n        \"data-theme\": \"plastic\",\n        children: _jsxs(_components.code, {\n          \"data-language\": \"python\",\n          \"data-theme\": \"plastic\",\n          style: {\n            display: \"grid\"\n          },\n          children: [_jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"def\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" save_lora_weights\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"unet\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"save_path\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"):\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    lora_state_dict \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" {}\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    for\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" name, module \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" unet.named_modules():\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" isinstance\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(module, LoRALayer):\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"            # 保存 A 和 B 矩阵\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"            lora_state_dict[\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"f\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"{\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"name\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"}\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \".A\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"] \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" module.A\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"            lora_state_dict[\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"f\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"{\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"name\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"}\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \".B\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"] \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" module.B\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    torch.save(lora_state_dict, save_path)\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"save_lora_weights(unet, \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"lora_weights.pth\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          })]\n        })\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"6-推理时加载-lora\",\n      children: _jsxs(_components.a, {\n        href: \"#6-推理时加载-lora\",\n        children: [\"6. \", _jsx(_components.strong, {\n          children: \"推理时加载 LoRA\"\n        })]\n      })\n    }), \"\\n\", _jsx(_components.figure, {\n      \"data-rehype-pretty-code-figure\": \"\",\n      children: _jsx(_components.pre, {\n        tabIndex: \"0\",\n        \"data-language\": \"python\",\n        \"data-theme\": \"plastic\",\n        children: _jsxs(_components.code, {\n          \"data-language\": \"python\",\n          \"data-theme\": \"plastic\",\n          style: {\n            display: \"grid\"\n          },\n          children: [_jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 加载原始模型\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"pipeline \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" StableDiffusionPipeline.from_pretrained(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"runwayml/stable-diffusion-v1-5\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 注入 LoRA\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"inject_lora(pipeline.unet, \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"rank\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"8\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"pipeline.unet.load_state_dict(torch.load(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"lora_weights.pth\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"), \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"strict\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"False\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")  \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 仅加载 LoRA 参数\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 生成图片\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"image \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" pipeline(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"A photo of [your trigger word]\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \").images[\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"0\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"]\"\n            })]\n          })]\n        })\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"关键注意事项\",\n      children: _jsx(_components.a, {\n        href: \"#关键注意事项\",\n        children: _jsx(_components.strong, {\n          children: \"关键注意事项\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"参数冻结\"\n        }), \"：确保原始模型的参数被冻结，只训练 LoRA 的 \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"A\"\n              })\n            })\n          })\n        }), \" 和 \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"B\"\n              })\n            })\n          })\n        }), \" 矩阵。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Rank 选择\"\n        }), \"：通常 \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"rank=8\"\n              })\n            })\n          })\n        }), \" 足够，复杂任务可尝试 \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"rank=16-64\"\n              })\n            })\n          })\n        }), \"。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"混合精度训练\"\n        }), \"：添加 \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"with torch.autocast(\\\"cuda\\\")\"\n              })\n            })\n          })\n        }), \" 加速训练并减少显存。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"梯度检查点\"\n        }), \"：对 \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"unet.enable_gradient_checkpointing()\"\n              })\n            })\n          })\n        }), \" 以减少显存占用。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"学习率调整\"\n        }), \"：LoRA 学习率通常比全模型微调大（例如 \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"1e-4\"\n              })\n            })\n          })\n        }), \" vs \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"1e-5\"\n              })\n            })\n          })\n        }), \"）。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"通过此脚本，你可以完全控制 LoRA 的实现细节，适用于定制化需求或研究场景。若追求快速开发，仍推荐使用 \", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"peft\"\n            })\n          })\n        })\n      }), \" 库的封装。\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"q-你说需将-lora-注入到-unet-的-cross-attention-层-lora-只能加到-crossattention-层吗为什么\",\n      children: _jsx(_components.a, {\n        href: \"#q-你说需将-lora-注入到-unet-的-cross-attention-层-lora-只能加到-crossattention-层吗为什么\",\n        children: \"Q: 你说“需将 LoRA 注入到 UNet 的 Cross-Attention 层”， LoRA 只能加到 CrossAttention 层吗？为什么？\"\n      })\n    }), \"\\n\", _jsxs(ObsidianCallout, {\n      type: \"reasoning\",\n      title: \"\",\n      foldable: true,\n      isCollapsed: true,\n      children: [_jsx(_components.p, {\n        children: \"嗯，用户之前问过如何训练 Stable Diffusion 的 LoRA 模型，然后接着问如何自己写训练脚本。现在他们又进一步深入，问到是否 LoRA 只能加到 CrossAttention 层，以及原因。我需要仔细分析这个问题，确保回答准确且有深度。\"\n      }), _jsx(_components.p, {\n        children: \"首先，回顾一下之前的对话。用户已经具备一定的 PyTorch 基础，并且对 LoRA 的实现有初步了解。他们在之前的回答中已经看到了如何手动将 LoRA 注入到 UNet 的 Cross-Attention 层，现在可能对 LoRA 的应用范围产生了疑问，想知道是否仅限于这些层。\"\n      }), _jsx(_components.p, {\n        children: \"接下来，我需要思考 LoRA 的工作原理。LoRA 通过在原始权重旁添加低秩矩阵来微调模型，通常用于减少训练参数。在 Stable Diffusion 中，UNet 的 Cross-Attention 层是文本条件与图像生成交互的关键部分，因此调整这些层能有效影响生成结果。但用户的问题是关于是否只能在这些层应用 LoRA，我需要确认其他层是否也能应用，或者是否有其他考虑因素。\"\n      }), _jsx(_components.p, {\n        children: \"可能的误区是用户可能认为 LoRA 只能用于 Cross-Attention 层，但实际上，理论上可以应用到任何线性层。但实践中，选择特定层是为了效率和效果。需要解释为什么在 Stable Diffusion 中推荐这样做，同时说明其他可能性。\"\n      }), _jsx(_components.p, {\n        children: \"需要验证的信息包括：Stable Diffusion 的 UNet 结构，特别是不同层的作用；LoRA 在不同层应用的效果差异；相关研究或实践经验。例如，Diffusers 库的文档或相关论文可能提到最佳实践。\"\n      }), _jsx(_components.p, {\n        children: \"用户可能的深层需求是希望灵活应用 LoRA，了解其限制和可能性，以便在自己的项目中做调整。他们可能想知道是否可以在其他层（如自注意力层或 FFN）添加 LoRA，以及效果如何。\"\n      }), _jsx(_components.p, {\n        children: \"回答时需要分点说明，首先指出 LoRA 理论上可以应用于任何层，但在实际应用中 Cross-Attention 层更为关键，然后解释原因，包括参数效率、文本控制的重要性，以及实践中的效果。同时，补充其他可能的应用场景，让用户有全面的了解。\"\n      }), _jsx(_components.p, {\n        children: \"需要确保语言清晰，避免术语混乱，同时提供足够的例子和解释，帮助用户理解。可能还需要提到相关研究，比如 LoRA 最初的应用场景，或者在 Stable Diffusion 中的具体调整案例，以增强说服力。\"\n      }), _jsx(_components.p, {\n        children: \"最后，总结时强调选择 Cross-Attention 层的原因，但鼓励用户根据需求尝试其他层，并指出可能的效果差异和资源消耗。这样既解答了问题，又提供了进一步探索的方向。\"\n      })]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"你的问题非常关键！\", _jsx(_components.strong, {\n        children: \"LoRA 并不只能加到 Cross-Attention 层\"\n      }), \"，但在 Stable Diffusion 微调中，\", _jsx(_components.strong, {\n        children: \"Cross-Attention 层是最关键的目标\"\n      }), \"。以下是深度解析：\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"1-理论层面lora-可以加在哪里\",\n      children: _jsx(_components.a, {\n        href: \"#1-理论层面lora-可以加在哪里\",\n        children: _jsx(_components.strong, {\n          children: \"1. 理论层面：LoRA 可以加在哪里？\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"从数学原理上，LoRA 是一种通用的参数高效微调方法，\", _jsx(_components.strong, {\n        children: \"可应用于任何线性层\"\n      }), \"（如 \", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"nn.Linear\"\n            })\n          })\n        })\n      }), \" 或等效的矩阵运算）。例如：\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Self-Attention 中的 Q/K/V 层\"\n        }), \"（处理图像自身特征）\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: _jsx(_components.strong, {\n          children: \"Feed-Forward Network (FFN) 的中间层\"\n        })\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"甚至 VAE 或 Text Encoder 的某些层\"\n        }), \"（但较少见）\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"但实际应用中，\", _jsx(_components.strong, {\n        children: \"选择注入位置需权衡效果与效率\"\n      }), \"。\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"2-为什么-stable-diffusion-优先选择-cross-attention-层\",\n      children: _jsx(_components.a, {\n        href: \"#2-为什么-stable-diffusion-优先选择-cross-attention-层\",\n        children: _jsx(_components.strong, {\n          children: \"2. 为什么 Stable Diffusion 优先选择 Cross-Attention 层？\"\n        })\n      })\n    }), \"\\n\", _jsx(_components.h4, {\n      id: \"1-文本-图像对齐的核心枢纽\",\n      children: _jsx(_components.a, {\n        href: \"#1-文本-图像对齐的核心枢纽\",\n        children: _jsx(_components.strong, {\n          children: \"(1) 文本-图像对齐的核心枢纽\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Cross-Attention 层\"\n        }), \"是 UNet 中\", _jsx(_components.strong, {\n          children: \"文本条件（Text Embedding）与图像特征交互的唯一通道\"\n        }), \"。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"修改这些层能直接控制生成内容如何响应文本提示（Prompt），例如：\", \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"学习新概念（如特定物体 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"my_cat_v1\"\n                  })\n                })\n              })\n            }), \"）\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"调整风格（如油画风 \", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"van gogh style\"\n                  })\n                })\n              })\n            }), \"）\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h4, {\n      id: \"2-参数效率最大化\",\n      children: _jsx(_components.a, {\n        href: \"#2-参数效率最大化\",\n        children: _jsx(_components.strong, {\n          children: \"(2) 参数效率最大化\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"Stable Diffusion 的 UNet 参数量巨大（约 860M），但 \", _jsx(_components.strong, {\n          children: \"Cross-Attention 层仅占约 10% 的参数量\"\n        }), \"。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"仅微调这部分，即可用 \", _jsx(_components.strong, {\n          children: \"0.1%~1% 的参数量达到接近全参数微调的效果\"\n        }), \"，性价比极高。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h4, {\n      id: \"3-实践经验验证\",\n      children: _jsx(_components.a, {\n        href: \"#3-实践经验验证\",\n        children: _jsx(_components.strong, {\n          children: \"(3) 实践经验验证\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"大量实验表明，\", _jsx(_components.strong, {\n          children: \"调整 Cross-Attention 层对生成结果影响最显著\"\n        }), \"。\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"若同时微调 Self-Attention 或 FFN 层，可能引入噪声（如破坏预训练模型的构图能力）。\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"3-其他可能的目标层\",\n      children: _jsx(_components.a, {\n        href: \"#3-其他可能的目标层\",\n        children: _jsx(_components.strong, {\n          children: \"3. 其他可能的目标层\"\n        })\n      })\n    }), \"\\n\", _jsx(_components.h4, {\n      id: \"1-self-attention-层\",\n      children: _jsx(_components.a, {\n        href: \"#1-self-attention-层\",\n        children: _jsx(_components.strong, {\n          children: \"(1) Self-Attention 层\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"作用\"\n        }), \"：控制图像内部结构（如物体形状、布局）。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"适用场景\"\n        }), \"：需要修改生成图像的构图规则（如强制对称性）。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"风险\"\n        }), \"：容易导致模型遗忘预训练知识（如失去多样性）。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h4, {\n      id: \"2-ffn-层\",\n      children: _jsx(_components.a, {\n        href: \"#2-ffn-层\",\n        children: _jsx(_components.strong, {\n          children: \"(2) FFN 层\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"作用\"\n        }), \"：非线性特征变换。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"适用场景\"\n        }), \"：调整颜色、纹理等低级特征。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"缺点\"\n        }), \"：参数量大，效率较低。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h4, {\n      id: \"3-组合策略\",\n      children: _jsx(_components.a, {\n        href: \"#3-组合策略\",\n        children: _jsx(_components.strong, {\n          children: \"(3) 组合策略\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"同时微调 \", _jsx(_components.strong, {\n          children: \"Cross-Attention + 部分 Self-Attention\"\n        }), \"，可平衡控制力与稳定性，但需更多显存和训练数据。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"4-代码示例注入到-self-attention\",\n      children: _jsx(_components.a, {\n        href: \"#4-代码示例注入到-self-attention\",\n        children: _jsx(_components.strong, {\n          children: \"4. 代码示例：注入到 Self-Attention\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"只需修改 \", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"inject_lora\"\n            })\n          })\n        })\n      }), \" 函数中的目标模块筛选条件：\"]\n    }), \"\\n\", _jsx(_components.figure, {\n      \"data-rehype-pretty-code-figure\": \"\",\n      children: _jsx(_components.pre, {\n        tabIndex: \"0\",\n        \"data-language\": \"python\",\n        \"data-theme\": \"plastic\",\n        children: _jsxs(_components.code, {\n          \"data-language\": \"python\",\n          \"data-theme\": \"plastic\",\n          style: {\n            display: \"grid\"\n          },\n          children: [_jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#61AFEF\"\n              },\n              children: \"def\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" inject_lora\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"unet\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \", \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"rank\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"8\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"):\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    for\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" name, module \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" unet.named_modules():\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"        # 定位 Self-Attention 的 Q/K/V 层（\\\"attn1\\\" 表示 Self-Attention）\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" \\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"attn1\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" name \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"and\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" \\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#98C379\"\n              },\n              children: \"to_\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"\\\"\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" in\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" name:\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"            if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#B57EDC\"\n              },\n              children: \" isinstance\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"(module, nn.Linear):\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"                # 替换为 LoRALayer\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \"                ...\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    return\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" unet\"\n            })]\n          })]\n        })\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"5-如何选择注入位置\",\n      children: _jsx(_components.a, {\n        href: \"#5-如何选择注入位置\",\n        children: _jsx(_components.strong, {\n          children: \"5. 如何选择注入位置？\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.table, {\n      children: [_jsx(_components.thead, {\n        children: _jsxs(_components.tr, {\n          children: [_jsx(_components.th, {\n            children: _jsx(_components.strong, {\n              children: \"目标层类型\"\n            })\n          }), _jsx(_components.th, {\n            children: _jsx(_components.strong, {\n              children: \"参数量占比\"\n            })\n          }), _jsx(_components.th, {\n            children: _jsx(_components.strong, {\n              children: \"影响范围\"\n            })\n          }), _jsx(_components.th, {\n            children: _jsx(_components.strong, {\n              children: \"推荐场景\"\n            })\n          })]\n        })\n      }), _jsxs(_components.tbody, {\n        children: [_jsxs(_components.tr, {\n          children: [_jsx(_components.td, {\n            children: _jsx(_components.strong, {\n              children: \"Cross-Attention\"\n            })\n          }), _jsx(_components.td, {\n            children: \"~10%\"\n          }), _jsx(_components.td, {\n            children: \"文本控制、概念学习\"\n          }), _jsx(_components.td, {\n            children: \"学习新物体/风格（主流选择）\"\n          })]\n        }), _jsxs(_components.tr, {\n          children: [_jsx(_components.td, {\n            children: _jsx(_components.strong, {\n              children: \"Self-Attention\"\n            })\n          }), _jsx(_components.td, {\n            children: \"~20%\"\n          }), _jsx(_components.td, {\n            children: \"图像结构、布局\"\n          }), _jsx(_components.td, {\n            children: \"特殊构图需求\"\n          })]\n        }), _jsxs(_components.tr, {\n          children: [_jsx(_components.td, {\n            children: _jsx(_components.strong, {\n              children: \"FFN\"\n            })\n          }), _jsx(_components.td, {\n            children: \"~70%\"\n          }), _jsx(_components.td, {\n            children: \"颜色、纹理\"\n          }), _jsx(_components.td, {\n            children: \"低级特征微调\"\n          })]\n        })]\n      })]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(_components.h3, {\n      id: \"6-终极建议\",\n      children: _jsx(_components.a, {\n        href: \"#6-终极建议\",\n        children: _jsx(_components.strong, {\n          children: \"6. 终极建议\"\n        })\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"优先 Cross-Attention\"\n        }), \"：95% 的场景只需调整这里。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"谨慎添加其他层\"\n        }), \"：仅在数据充足且任务特殊时尝试（如医学图像生成需精确形状控制）。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"监控过拟合\"\n        }), \"：添加更多可训练参数会增加过拟合风险，需配合数据增强/早停法。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(ObsidianCallout, {\n      type: \"think\",\n      title: \"亮曰：\",\n      children: _jsxs(_components.p, {\n        children: [_jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"UNet2DConditionModel\"\n              })\n            })\n          })\n        }), \" 的整个流程中， Cross-Attention 主要作用在下采样、中间块、上采样三个步骤中。在 \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"__init__\"\n              })\n            })\n          })\n        }), \" 参数中传入每个步骤中用的 Module ，其中用 \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"CrossAttnDownBlock2D\"\n              })\n            })\n          })\n        }), \" 等 Module \", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"self.has_cross_attention\"\n              })\n            })\n          })\n        }), \" 为 true ，能处理 cross_attention_mask 等参数。其中的 attentions 层即为交叉注意力层。推测，是通过 Attentions 层对下采样层入参做处理，并列的 ResNet 应该是为了提升注意力层的训练性能。\"]\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      id: \"unet-代码中的-cross-attension-层\",\n      children: _jsx(_components.a, {\n        href: \"#unet-代码中的-cross-attension-层\",\n        children: \"UNet 代码中的 Cross-Attension 层\"\n      })\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"以下为在 Cursor 中有 diffusers 代码上下文时与 claude-3.7-sonnet 的对话内容。\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(ObsidianCallout, {\n      type: \"query\",\n      title: \"\",\n      children: _jsx(_components.p, {\n        children: \"UNet 中 Cross-Attension 是做什么的？具体到代码里如何作用？ @unet_2d_condition.py\"\n      })\n    }), \"\\n\", _jsxs(ObsidianCallout, {\n      type: \"reasoning\",\n      title: \"\",\n      foldable: true,\n      isCollapsed: true,\n      children: [_jsx(_components.p, {\n        children: \"这个问题是关于 UNet 中 Cross-Attention 机制的作用，特别是在 UNet2DConditionModel 中的实现和作用。我需要查找和分析代码中与 Cross-Attention 相关的部分。\"\n      }), _jsx(_components.p, {\n        children: \"从代码中可以看出，UNet2DConditionModel 是一个条件式的 2D UNet 模型，它通常用于扩散模型中，接收噪声图像样本、条件状态和时间步长作为输入，并返回处理后的样本。\"\n      }), _jsx(_components.p, {\n        children: \"在扩散模型（如 Stable Diffusion）中，Cross-Attention 是一种非常重要的机制，它允许 UNet 处理两种不同类型的信息：\"\n      }), _jsx(_components.p, {\n        children: \"空间信息（图像特征）\"\n      }), _jsx(_components.p, {\n        children: \"上下文信息（通常是文本编码或其他条件编码）\"\n      }), _jsx(_components.p, {\n        children: \"让我查看代码中与 Cross-Attention 相关的关键部分：\"\n      }), _jsx(_components.p, {\n        children: \"模型在初始化时，设置了一些与 Cross-Attention 相关的参数，如 cross_attention_dim，only_cross_attention 等。\"\n      }), _jsx(_components.p, {\n        children: \"在 forward 方法中，encoder_hidden_states 参数通常是从文本编码器获得的表示，它被传递给具有 cross-attention 功能的下采样、中间和上采样块。\"\n      }), _jsx(_components.p, {\n        children: \"模型使用了某种注意力处理器（AttentionProcessor），可能是为了优化注意力计算。\"\n      }), _jsx(_components.p, {\n        children: \"让我仔细查看 down_blocks 和 up_blocks 中的 Cross-Attention 实现，以及它们是如何与 encoder_hidden_states 交互的。\"\n      })]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Cross-Attention 在 UNet 中主要用于将条件信息（如文本特征）引入到图像生成过程中。从代码来看，主要体现在以下几个方面：\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"数据流向\"\n          }), \"：\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"UNet 接收图像特征(\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"sample\"\n                  })\n                })\n              })\n            }), \")和条件编码(\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"encoder_hidden_states\"\n                  })\n                })\n              })\n            }), \")\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"encoder_hidden_states\"\n                  })\n                })\n              })\n            }), \"通常是文本编码器输出的特征\"]\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"条件信息通过 Cross-Attention 在 UNet 的不同层级与图像特征交互\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"代码实现\"\n          }), \"：\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"py\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"py\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"if\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#B57EDC\"\n                  },\n                  children: \" hasattr\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"(downsample_block, \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"has_cross_attention\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \") \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"and\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" downsample_block.has_cross_attention:\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"    sample, res_samples \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" downsample_block(\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#C6CCD7\"\n                  },\n                  children: \"        hidden_states\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"sample,\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#C6CCD7\"\n                  },\n                  children: \"        temb\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"emb,\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#C6CCD7\"\n                  },\n                  children: \"        encoder_hidden_states\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"encoder_hidden_states,\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#C6CCD7\"\n                  },\n                  children: \"        attention_mask\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"attention_mask,\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#C6CCD7\"\n                  },\n                  children: \"        cross_attention_kwargs\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"cross_attention_kwargs,\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#C6CCD7\"\n                  },\n                  children: \"        encoder_attention_mask\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"encoder_attention_mask,\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"        **\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"additional_residuals,\"\n                })]\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"    )\"\n                })\n              })]\n            })\n          })\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"核心组件\"\n          }), \"：\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"在下采样、中间块和上采样阶段都可能使用 Cross-Attention\"\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"通过\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"has_cross_attention\"\n                  })\n                })\n              })\n            }), \"属性判断块是否需要使用 Cross-Attention\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"当使用 Cross-Attention 时，\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"encoder_hidden_states\"\n                  })\n                })\n              })\n            }), \"会作为额外输入传递\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"参数控制\"\n          }), \"：\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"only_cross_attention\"\n                  })\n                })\n              })\n            }), \"参数控制是否只使用 Cross-Attention 而不使用 Self-Attention\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"cross_attention_dim\"\n                  })\n                })\n              })\n            }), \"指定了条件编码的维度大小\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"transformer_layers_per_block\"\n                  })\n                })\n              })\n            }), \"控制每个块中 Transformer 层的数量\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Cross-Attention 允许模型在保持图像空间结构的同时，根据文本或其他条件调整生成内容，实现了条件生成的核心机制。\"\n    }), \"\\n\", _jsx(ObsidianCallout, {\n      type: \"query\",\n      title: \"\",\n      children: _jsx(_components.p, {\n        children: \"TransformerBlock 具体是在@transformer_2d.py 里实现吗？如何体现 Self-Attention 与 Cross-Attention ？\"\n      })\n    }), \"\\n\", _jsxs(ObsidianCallout, {\n      type: \"reasoning\",\n      title: \"\",\n      foldable: true,\n      isCollapsed: true,\n      children: [_jsx(_components.p, {\n        children: \"让我来分析一下 TransformerBlock 的实现以及 Self-Attention 和 Cross-Attention 的具体体现。\"\n      }), _jsx(_components.p, {\n        children: \"在 transformer_2d.py 文件中，可以看到 Transformer2DModel 类，但 TransformerBlock 的具体实现似乎是引用自其他地方。注意到以下导入:\"\n      }), _jsx(_components.p, {\n        children: \"这表明 BasicTransformerBlock 类实际上是在 attention.py 文件中定义的，而不是直接在 transformer_2d.py 中。\"\n      }), _jsx(_components.p, {\n        children: \"在 transformer_2d.py 中，BasicTransformerBlock 被用在以下位置:\"\n      }), _jsx(_components.p, {\n        children: \"从这段代码中可以看出:\"\n      }), _jsx(_components.p, {\n        children: \"BasicTransformerBlock 是接收 self-attention 和 cross-attention 相关参数的，包括:\"\n      }), _jsx(_components.p, {\n        children: \"cross_attention_dim: 交叉注意力的维度\"\n      }), _jsx(_components.p, {\n        children: \"only_cross_attention: 是否只使用交叉注意力而不使用自注意力\"\n      }), _jsx(_components.p, {\n        children: \"double_self_attention: 是否使用双重自注意力\"\n      }), _jsx(_components.p, {\n        children: \"在转发(forward)函数中，我们可以看到如何传递注意力相关的参数:\"\n      }), _jsx(_components.p, {\n        children: \"这里的关键参数是:\"\n      }), _jsx(_components.p, {\n        children: \"hidden_states: 输入隐藏状态，用于 Self-Attention\"\n      }), _jsx(_components.p, {\n        children: \"encoder_hidden_states: 编码器隐藏状态，用于 Cross-Attention\"\n      }), _jsx(_components.p, {\n        children: \"attention_mask: 自注意力的掩码\"\n      }), _jsx(_components.p, {\n        children: \"encoder_attention_mask: 交叉注意力的掩码\"\n      }), _jsx(_components.p, {\n        children: \"虽然我们可以从这些代码推断出 BasicTransformerBlock 中如何实现自注意力和交叉注意力，但由于实际实现不在 transformer_2d.py 文件中，我们需要查看 attention.py 文件才能了解完整的实现细节。\"\n      }), _jsx(_components.p, {\n        children: \"基于现有的代码，我们可以推断:\"\n      }), _jsx(_components.p, {\n        children: \"Self-Attention 是使用 hidden_states 对自身进行注意力计算\"\n      }), _jsx(_components.p, {\n        children: \"Cross-Attention 是将 hidden_states 作为查询(query)，将 encoder_hidden_states 作为键(key)和值(value)进行注意力计算\"\n      }), _jsx(_components.p, {\n        children: \"可以通过 only_cross_attention 参数控制是否只使用 Cross-Attention 而不使用 Self-Attention\"\n      })]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"TransformerBlock 的核心实现是在\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"attention.py\"\n            })\n          })\n        })\n      }), \"中的\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"BasicTransformerBlock\"\n            })\n          })\n        })\n      }), \"类中，而不是直接在\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"transformer_2d.py\"\n            })\n          })\n        })\n      }), \"中。在\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"transformer_2d.py\"\n            })\n          })\n        })\n      }), \"中只是调用了这个类来构建 Transformer 模型。\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"从\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"transformer_2d.py\"\n            })\n          })\n        })\n      }), \"代码中可以看到 Self-Attention 和 Cross-Attention 的几个关键特点：\"]\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"实例化 transformer 块时\"\n          }), \":\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"156:169:src/diffusers/models/transformers/transformer_2d.py\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"156:169:src/diffusers/models/transformers/transformer_2d.py\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"BasicTransformerBlock(\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    self.inner_dim,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    self.config.num_attention_heads,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    self.config.attention_head_dim,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    dropout=self.config.dropout,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    cross_attention_dim=self.config.cross_attention_dim,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    activation_fn=self.config.activation_fn,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    num_embeds_ada_norm=self.config.num_embeds_ada_norm,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    attention_bias=self.config.attention_bias,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    only_cross_attention=self.config.only_cross_attention,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    double_self_attention=self.config.double_self_attention,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    ...\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \")\"\n                })\n              })]\n            })\n          })\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"调用时的数据流\"\n          }), \":\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"386:396:src/diffusers/models/transformers/transformer_2d.py\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"386:396:src/diffusers/models/transformers/transformer_2d.py\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"hidden_states = block(\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    hidden_states,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    attention_mask=attention_mask,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    encoder_hidden_states=encoder_hidden_states,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    encoder_attention_mask=encoder_attention_mask,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    timestep=timestep,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    cross_attention_kwargs=cross_attention_kwargs,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \"    class_labels=class_labels,\"\n                })\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  children: \")\"\n                })\n              })]\n            })\n          })\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"核心区别：\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Self-Attention\"\n        }), \": 使用\", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"hidden_states\"\n              })\n            })\n          })\n        }), \"自身进行注意力计算，\", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"attention_mask\"\n              })\n            })\n          })\n        }), \"作为掩码\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"Cross-Attention\"\n        }), \": 使用\", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"hidden_states\"\n              })\n            })\n          })\n        }), \"作为 query，\", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"encoder_hidden_states\"\n              })\n            })\n          })\n        }), \"作为 key 和 value，\", _jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"encoder_attention_mask\"\n              })\n            })\n          })\n        }), \"作为掩码\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"配置选项：\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"only_cross_attention\"\n              })\n            })\n          })\n        }), \": 如果为 True，只使用 Cross-Attention 而跳过 Self-Attention\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"double_self_attention\"\n              })\n            })\n          })\n        }), \": 如果为 True，使用两次 Self-Attention\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"cross_attention_dim\"\n              })\n            })\n          })\n        }), \": 指定 Cross-Attention 的维度大小\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"在实际运行时，BasicTransformerBlock 内部会先进行 Self-Attention 操作，然后进行 Cross-Attention 操作（如果提供了\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"encoder_hidden_states\"\n            })\n          })\n        })\n      }), \"）。这样就实现了 UNet 中文本条件对图像生成的影响。\"]\n    }), \"\\n\", _jsx(ObsidianCallout, {\n      type: \"query\",\n      title: \"\",\n      children: _jsx(_components.p, {\n        children: \"@attention.py 中 BasicTransformerBlock 如何体现 Self-Attention 与 Cross-Attention ?\"\n      })\n    }), \"\\n\", _jsxs(ObsidianCallout, {\n      type: \"reasoning\",\n      title: \"\",\n      children: [_jsx(_components.p, {\n        children: \"让我看看 attention.py 中的 BasicTransformerBlock 类，特别是它如何实现 Self-Attention 和 Cross-Attention 的部分。\"\n      }), _jsx(_components.p, {\n        children: \"从代码中，我可以看到 BasicTransformerBlock 类包含了以下几个关键组件：\"\n      }), _jsx(_components.p, {\n        children: \"初始化函数中，它设置了多个 Attention 模块和标准化层：\"\n      }), _jsx(_components.p, {\n        children: \"self.norm1 和 self.attn1 用于 Self-Attention\"\n      }), _jsx(_components.p, {\n        children: \"self.norm2 和 self.attn2 用于 Cross-Attention（如果需要的话）\"\n      }), _jsx(_components.p, {\n        children: \"self.norm3 和 self.ff 用于前馈网络\"\n      }), _jsx(_components.p, {\n        children: \"在 forward 函数中，处理流程如下：\"\n      }), _jsx(_components.p, {\n        children: \"首先对输入进行 normalization\"\n      }), _jsx(_components.p, {\n        children: \"然后应用 Self-Attention（self.attn1）\"\n      }), _jsx(_components.p, {\n        children: \"如果有 cross_attention_dim 或 double_self_attention，则应用 Cross-Attention（self.attn2）\"\n      }), _jsx(_components.p, {\n        children: \"最后是 Feed-Forward 网络\"\n      }), _jsx(_components.p, {\n        children: \"特别需要注意的是，对于 Self-Attention 和 Cross-Attention 的区别：\"\n      }), _jsx(_components.p, {\n        children: \"Self-Attention：使用 self.attn1，只使用 hidden_states 作为输入，在 only_cross_attention 为 True 时可能使用 encoder_hidden_states。\"\n      }), _jsx(_components.p, {\n        children: \"Cross-Attention：使用 self.attn2，hidden_states 作为 query，encoder_hidden_states 作为 key 和 value。\"\n      }), _jsx(_components.p, {\n        children: \"在 forward 方法中，我们可以清楚看到这一点：\"\n      }), _jsx(_components.p, {\n        children: \"对于 Self-Attention，attn_output = self.attn1(norm_hidden_states, ...)\"\n      }), _jsx(_components.p, {\n        children: \"对于 Cross-Attention，attn_output = self.attn2(norm_hidden_states, encoder_hidden_states=encoder_hidden_states, ...)\"\n      }), _jsx(_components.p, {\n        children: \"我会重点关注 forward 方法中实现 Self-Attention 和 Cross-Attention 的关键部分。\"\n      })]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"在\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"attention.py\"\n            })\n          })\n        })\n      }), \"中，\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"BasicTransformerBlock\"\n            })\n          })\n        })\n      }), \"类通过以下方式实现了 Self-Attention 和 Cross-Attention：\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"1-架构设计\",\n      children: _jsx(_components.a, {\n        href: \"#1-架构设计\",\n        children: \"1. 架构设计\"\n      })\n    }), \"\\n\", _jsx(_components.figure, {\n      \"data-rehype-pretty-code-figure\": \"\",\n      children: _jsx(_components.pre, {\n        tabIndex: \"0\",\n        \"data-language\": \"py\",\n        \"data-theme\": \"plastic\",\n        children: _jsxs(_components.code, {\n          \"data-language\": \"py\",\n          \"data-theme\": \"plastic\",\n          style: {\n            display: \"grid\"\n          },\n          children: [_jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 1. Self-Attn\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".attn1 \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" Attention(\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    query_dim\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"dim,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    heads\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"num_attention_heads,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    dim_head\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"attention_head_dim,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    dropout\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"dropout,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    bias\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"attention_bias,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    cross_attention_dim\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"cross_attention_dim \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" only_cross_attention \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"else\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \" None\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \",\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    upcast_attention\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"upcast_attention,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    out_bias\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"attention_out_bias,\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 2. Cross-Attn\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" cross_attention_dim \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"is\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" not\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \" None\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" or\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" double_self_attention:\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".norm2 \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" nn.LayerNorm(dim, norm_eps, norm_elementwise_affine)\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".attn2 \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" Attention(\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        query_dim\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"dim,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        cross_attention_dim\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"cross_attention_dim \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" not\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" double_self_attention \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"else\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \" None\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \",\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        heads\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"num_attention_heads,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        dim_head\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"attention_head_dim,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        dropout\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"dropout,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        bias\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"attention_bias,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        upcast_attention\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"upcast_attention,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        out_bias\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"attention_out_bias,\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    )\"\n            })\n          })]\n        })\n      })\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"2-前向传播过程\",\n      children: _jsx(_components.a, {\n        href: \"#2-前向传播过程\",\n        children: \"2. 前向传播过程\"\n      })\n    }), \"\\n\", _jsx(_components.figure, {\n      \"data-rehype-pretty-code-figure\": \"\",\n      children: _jsx(_components.pre, {\n        tabIndex: \"0\",\n        \"data-language\": \"py\",\n        \"data-theme\": \"plastic\",\n        children: _jsxs(_components.code, {\n          \"data-language\": \"py\",\n          \"data-theme\": \"plastic\",\n          style: {\n            display: \"grid\"\n          },\n          children: [_jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 1. Self-Attention\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"norm_hidden_states \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".norm1(hidden_states)\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"attn_output \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".attn1(\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    norm_hidden_states,\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    encoder_hidden_states\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"encoder_hidden_states \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".only_cross_attention \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"else\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \" None\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \",\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"    attention_mask\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"attention_mask,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"    **\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"cross_attention_kwargs,\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \")\"\n            })\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# ... (处理输出)\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"hidden_states \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" attn_output \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"+\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" hidden_states\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: \" \"\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#5F6672\",\n                fontStyle: \"italic\"\n              },\n              children: \"# 2. Cross-Attention\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"if\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".attn2 \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"is\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" not\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#56B6C2\"\n              },\n              children: \" None\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \":\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    norm_hidden_states \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".norm2(hidden_states)\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    attn_output \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \" self\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \".attn2(\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"        norm_hidden_states,\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        encoder_hidden_states\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"encoder_hidden_states,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#C6CCD7\"\n              },\n              children: \"        attention_mask\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"encoder_attention_mask,\"\n            })]\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"        **\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"cross_attention_kwargs,\"\n            })]\n          }), \"\\n\", _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    )\"\n            })\n          }), \"\\n\", _jsxs(_components.span, {\n            \"data-line\": \"\",\n            children: [_jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \"    hidden_states \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"=\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" attn_output \"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#E06C75\"\n              },\n              children: \"+\"\n            }), _jsx(_components.span, {\n              style: {\n                color: \"#A9B2C3\"\n              },\n              children: \" hidden_states\"\n            })]\n          })]\n        })\n      })\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"3-关键区别\",\n      children: _jsx(_components.a, {\n        href: \"#3-关键区别\",\n        children: \"3. 关键区别\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"Self-Attention (attn1)\"\n          }), \"：\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [\"默认情况下只使用\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"hidden_states\"\n                  })\n                })\n              })\n            }), \"作为输入（query, key, value 都来自同一源）\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"只有当\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"only_cross_attention=True\"\n                  })\n                })\n              })\n            }), \"时才会用\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"encoder_hidden_states\"\n                  })\n                })\n              })\n            })]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"使用\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"attention_mask\"\n                  })\n                })\n              })\n            }), \"作为掩码\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"Cross-Attention (attn2)\"\n          }), \"：\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"norm_hidden_states\"\n                  })\n                })\n              })\n            }), \"作为 query\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [_jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"encoder_hidden_states\"\n                  })\n                })\n              })\n            }), \"作为 key 和 value\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"使用\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"encoder_attention_mask\"\n                  })\n                })\n              })\n            }), \"作为掩码\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            children: [\"当\", _jsx(_components.span, {\n              \"data-rehype-pretty-code-figure\": \"\",\n              children: _jsx(_components.code, {\n                \"data-language\": \"plaintext\",\n                \"data-theme\": \"plastic\",\n                children: _jsx(_components.span, {\n                  \"data-line\": \"\",\n                  children: _jsx(_components.span, {\n                    children: \"encoder_hidden_states=None\"\n                  })\n                })\n              })\n            }), \"时会退化为 Self-Attention\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"4-配置控制\",\n      children: _jsx(_components.a, {\n        href: \"#4-配置控制\",\n        children: \"4. 配置控制\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"only_cross_attention\"\n              })\n            })\n          })\n        }), \"：是否在 attn1 中使用 cross-attention\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"double_self_attention\"\n              })\n            })\n          })\n        }), \"：是否将 attn2 配置为另一个 self-attention 层\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.span, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.code, {\n            \"data-language\": \"plaintext\",\n            \"data-theme\": \"plastic\",\n            children: _jsx(_components.span, {\n              \"data-line\": \"\",\n              children: _jsx(_components.span, {\n                children: \"cross_attention_dim\"\n              })\n            })\n          })\n        }), \"：指定条件编码的维度\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"这种设计允许 UNet 模型在不同层级灵活组合 Self-Attention 和 Cross-Attention，实现对图像特征的内部处理和对条件信息的整合。\"\n    }), \"\\n\", _jsx(ObsidianCallout, {\n      type: \"query\",\n      title: \"\",\n      children: _jsx(_components.p, {\n        children: \"我注意到 @attention.py 中在 Attention 层前都会有一个 LayerNorm 层。这是必须要有的吗？有什么作用？\"\n      })\n    }), \"\\n\", _jsx(ObsidianCallout, {\n      type: \"reasoning\",\n      title: \"\",\n      foldable: true,\n      isCollapsed: true,\n      children: _jsx(_components.p, {\n        children: \"这个问题询问的是关于在 Attention 层前使用 LayerNorm 的必要性和作用。我需要解释 LayerNorm 在 Transformer 结构中的重要性。\\n在 attention.py 中的 BasicTransformerBlock 实现中，每个 Attention 操作前确实都有一个 normalization 层（通常是 LayerNorm），我应该分析这种设计的原因。\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"是的，在\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"attention.py\"\n            })\n          })\n        })\n      }), \"的\", _jsx(_components.span, {\n        \"data-rehype-pretty-code-figure\": \"\",\n        children: _jsx(_components.code, {\n          \"data-language\": \"plaintext\",\n          \"data-theme\": \"plastic\",\n          children: _jsx(_components.span, {\n            \"data-line\": \"\",\n            children: _jsx(_components.span, {\n              children: \"BasicTransformerBlock\"\n            })\n          })\n        })\n      }), \"中，每个 Attention 层前都有一个归一化层（通常是 LayerNorm），这是非常重要的设计：\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"layernorm-的必要性\",\n      children: _jsx(_components.a, {\n        href: \"#layernorm-的必要性\",\n        children: \"LayerNorm 的必要性\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"训练稳定性\"\n          }), \"：\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"py\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"py\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  style: {\n                    color: \"#5F6672\",\n                    fontStyle: \"italic\"\n                  },\n                  children: \"# 在Self-Attention前应用归一化\"\n                })\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"if\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm_type \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"==\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" \\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"ada_norm\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \":\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"    norm_hidden_states \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm1(hidden_states, timestep)\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"elif\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm_type \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"==\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" \\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"ada_norm_zero\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \":\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"    norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm1(\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"        hidden_states, timestep, class_labels, \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#C6CCD7\"\n                  },\n                  children: \"hidden_dtype\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"hidden_states.dtype\"\n                })]\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"    )\"\n                })\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"elif\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm_type \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"in\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" [\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"layer_norm\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \", \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"layer_norm_i2vgen\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"]:\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"    norm_hidden_states \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm1(hidden_states)\"\n                })]\n              })]\n            })\n          })\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"梯度流动\"\n          }), \"：\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"归一化层将数据分布标准化，防止梯度消失或爆炸\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"特别是在深层网络中，保持梯度流动对训练至关重要\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      id: \"layernorm-的作用\",\n      children: _jsx(_components.a, {\n        href: \"#layernorm-的作用\",\n        children: \"LayerNorm 的作用\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"特征分布稳定\"\n          }), \"：\"]\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"将特征归一化到相同尺度，减少批次间差异\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"使网络对输入变化更加鲁棒，提高泛化能力\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"残差连接的配合\"\n          }), \"：\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"py\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"py\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  style: {\n                    color: \"#5F6672\",\n                    fontStyle: \"italic\"\n                  },\n                  children: \"# 应用残差连接\"\n                })\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"hidden_states \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" attn_output \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"+\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" hidden_states\"\n                })]\n              })]\n            })\n          })\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"Pre-LayerNorm 结构（先归一化后 Attention）与残差连接配合使用\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"这种\\\"归一化 → 操作 → 残差\\\"的模式是 Transformer 架构的基本组成部分\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsxs(_components.p, {\n          children: [_jsx(_components.strong, {\n            children: \"多种归一化选择\"\n          }), \"：\"]\n        }), \"\\n\", _jsx(_components.figure, {\n          \"data-rehype-pretty-code-figure\": \"\",\n          children: _jsx(_components.pre, {\n            tabIndex: \"0\",\n            \"data-language\": \"py\",\n            \"data-theme\": \"plastic\",\n            children: _jsxs(_components.code, {\n              \"data-language\": \"py\",\n              \"data-theme\": \"plastic\",\n              style: {\n                display: \"grid\"\n              },\n              children: [_jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  style: {\n                    color: \"#5F6672\",\n                    fontStyle: \"italic\"\n                  },\n                  children: \"# 不同类型的归一化层\"\n                })\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"if\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm_type \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"==\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" \\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"ada_norm\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \":\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"    norm_hidden_states \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm1(hidden_states, timestep)\"\n                })]\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"elif\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm_type \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"==\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" \\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"ada_norm_zero\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \":\"\n                })]\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  style: {\n                    color: \"#5F6672\",\n                    fontStyle: \"italic\"\n                  },\n                  children: \"    # ...\"\n                })\n              }), \"\\n\", _jsxs(_components.span, {\n                \"data-line\": \"\",\n                children: [_jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"elif\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \" self\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \".norm_type \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#E06C75\"\n                  },\n                  children: \"in\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \" [\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"layer_norm\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \", \"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#98C379\"\n                  },\n                  children: \"layer_norm_i2vgen\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"\\\"\"\n                }), _jsx(_components.span, {\n                  style: {\n                    color: \"#A9B2C3\"\n                  },\n                  children: \"]:\"\n                })]\n              }), \"\\n\", _jsx(_components.span, {\n                \"data-line\": \"\",\n                children: _jsx(_components.span, {\n                  style: {\n                    color: \"#5F6672\",\n                    fontStyle: \"italic\"\n                  },\n                  children: \"    # ...\"\n                })\n              })]\n            })\n          })\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"基本的 LayerNorm\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"带条件的 AdaLayerNorm（使用 timestep 信息调整）\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"其他特殊归一化变体，适应不同场景\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"总之，LayerNorm 在 Attention 层前是必不可少的，它是保证 Transformer 结构有效训练和良好性能的关键组件，特别是在生成模型这样需要处理大量复杂特征的深度网络中。\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}},"meta":{"content":"\n\u003e 本文详细介绍了如何使用 PyTorch 从零开始编写 Stable Diffusion LoRA 训练脚本，包括数据预处理、模型修改、训练循环、参数保存与加载等关键步骤。特别强调了 LoRA 层的手动实现和在 UNet 的 Cross-Attention 层注入 LoRA 的原因，以及在其他层应用 LoRA 的可能性和注意事项。此外，还提供了代码示例和参数效率的讨论，帮助读者深入理解 LoRA 在 Stable Diffusion 微调中的应用。\n\n## Q: 训练一个 Stable Diffusion 的 LoRA 模型，要有哪些步骤？\n\n\u003e [!reasoning]-\n\u003e\n\u003e 好的，用户问的是训练 Stable Diffusion 的 LoRA 模型需要哪些步骤。首先，我需要回忆一下 LoRA 的基本概念。LoRA 是低秩适应，一种用于微调大模型的技术，可以在保持原模型参数不变的情况下，通过添加额外的低秩矩阵来调整模型输出。这样做的好处是节省计算资源和存储空间，因为不需要训练整个模型。\n\u003e\n\u003e 接下来，用户可能已经了解 Stable Diffusion 的基本原理，但需要具体步骤。我需要分解训练流程，从准备数据到最终推理。首先，数据准备是关键。用户需要收集高质量图片，可能还需要标注。这里需要注意图片的多样性和数量，以及正确的标注格式，比如 TXT 文件。\n\u003e\n\u003e 然后是环境配置。用户可能需要安装 PyTorch、Diffusers 库、Transformers 库，还有 LoRA 相关的库，比如 peft。如果用户不熟悉这些工具，可能需要给出安装命令示例，比如 pip install。\n\u003e\n\u003e 接下来是模型准备，选择适合的 Stable Diffusion 版本，如 SD 1.5 或 2.1。需要下载预训练模型，并配置 LoRA 参数，比如 rank 值。这里可能需要解释 rank 的意义，如何选择合适的值，比如 8 到 64 之间，根据任务复杂度调整。\n\u003e\n\u003e 然后是训练配置，设置学习率、批次大小、epoch 数等超参数。需要提醒用户学习率不宜过高，使用预热和衰减策略。可能还要提到优化器的选择，比如 AdamW，以及混合精度训练以节省显存。\n\u003e\n\u003e 训练过程中，用户可能需要监控 loss 和生成效果，调整参数。保存检查点也是重要的，防止训练中断。这部分需要强调耐心和多次实验，因为微调可能需要多次尝试。\n\u003e\n\u003e 训练完成后，合并 LoRA 权重到原模型，或者直接加载进行推理。需要给出示例代码，展示如何加载并使用 LoRA 模型生成图片。\n\u003e\n\u003e 最后，常见问题部分要考虑用户可能遇到的困难，比如需要多少数据，是否需要高质量标注，训练时间，以及是否需要高端 GPU。这些是实际训练中常见的问题，提前解答能帮助用户更好准备。\n\u003e\n\u003e 用户可能的深层需求是希望有一个清晰的步骤指南，避免遗漏关键环节。可能他们之前尝试过但遇到问题，或者刚入门需要从零开始的指导。因此，步骤要详细，但不过于技术化，保持易懂。同时，强调数据质量和参数调整的重要性，因为这是影响结果的关键因素。\n\n训练 Stable Diffusion 的 LoRA（Low-Rank Adaptation）模型是一种高效的微调方法，可以在保留原模型大部分参数的同时，通过低秩矩阵调整模型行为。以下是详细的步骤指南：\n\n---\n\n### **1. 数据准备**\n\n- **收集数据集**:\n  - 需要与目标风格/主题相关的高质量图片（建议至少 **100-200 张**，数量越多效果越好）。\n  - 图片格式通常为 `.jpg` 或 `.png`，分辨率建议与 Stable Diffusion 的基础模型对齐（如 512x512）。\n- **标注数据**:\n  - 每张图片需搭配文本描述（`.txt` 文件），描述需清晰简洁（如 `\"a photo of a [keyword], [style], [details]\"`）。\n  - 若训练特定对象（如人物、画风），确保描述中**统一使用触发词**（如 `\"my_cat_v1\"`）。\n\n---\n\n### **2. 环境配置**\n\n- **安装依赖库**:\n\n  ```bash\n  # 基础环境\n  pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu117\n  pip install diffusers transformers accelerate peft\n  pip install datasets huggingface_hub\n  # 可选：xformers 优化（A卡需调整）\n  pip install xformers\n  ```\n\n- **硬件要求**:\n  - GPU 显存至少 **8GB**（如 RTX 3060/4090），24GB 可支持更大批次。\n  - 使用 `accelerate` 库配置分布式训练（可选）。\n\n---\n\n### **3. 模型准备**\n\n- **选择基础模型**:\n\n  - 常用模型：`runwayml/stable-diffusion-v1-5`、`stabilityai/stable-diffusion-2-1`。\n  - 从 Hugging Face 下载并缓存模型：\n    ```python\n    from diffusers import StableDiffusionPipeline\n    pipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n    ```\n\n- **配置 LoRA 参数**:\n  - **Rank（秩）**: 控制低秩矩阵的维度（典型值：4-64，越大拟合能力越强，但可能过拟合）。\n  - 在训练脚本中设置 LoRA 层（通常作用于 Cross-Attention 模块）：\n    ```python\n    from peft import LoraConfig\n    lora_config = LoraConfig(\n        r=8,                # Rank 值\n        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],  # 注入的模块\n        init_lora_weights=\"gaussian\",\n    )\n    ```\n\n---\n\n### **4. 训练配置**\n\n- **超参数设置**:\n\n  ```yaml\n  batch_size: 1-4（根据显存调整）\n  learning_rate: 1e-5 到 1e-4（常用 1e-4）\n  num_epochs: 50-200（观察 loss 调整）\n  optimizer: AdamW（带权重衰减）\n  mixed_precision: fp16（节省显存）\n  gradient_checkpointing: True（进一步省显存）\n  ```\n\n- **训练脚本**:\n  - 使用 `diffusers` 的 `train_text_to_image.py` 或自定义训练循环。\n  - 关键代码片段：\n    ```python\n    from diffusers import DDPMScheduler\n    scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n    trainer = StableDiffusionTrainer(\n        model=pipeline.unet,\n        args=training_args,\n        data_collator=collate_fn,\n        train_dataset=dataset,\n        scheduler=scheduler,\n    )\n    trainer.add_callback(LoRACallback(lora_config))  # 注入 LoRA\n    ```\n\n---\n\n### **5. 训练过程**\n\n- **监控与调试**:\n\n  - 观察 Loss 曲线（理想情况应稳步下降后趋于平稳）。\n  - 定期生成测试图片（如每 100 步）检查过拟合/欠拟合。\n  - 调整策略：\n    - Loss 不降：提高学习率、增加 Rank 值。\n    - Loss 震荡：降低学习率、增大批次大小。\n\n- **保存检查点**:\n  - 每隔一定步数保存中间模型（如 `./output/checkpoint-500`）。\n  - 最终模型保存为 `.safetensors` 或合并到原模型。\n\n---\n\n### **6. 推理与部署**\n\n- **加载 LoRA 权重**:\n\n  ```python\n  from diffusers import StableDiffusionPipeline\n  pipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n  pipeline.unet.load_attn_procs(\"path/to/lora_weights.safetensors\")\n  ```\n\n- **生成图片**:\n  ```python\n  prompt = \"A photo of my_cat_v1 sitting on a sofa, best quality\"\n  image = pipeline(prompt, num_inference_steps=50).images[0]\n  image.save(\"output.png\")\n  ```\n\n---\n\n### **常见问题**\n\n- **Q**: 需要多少数据？\n\n  - **A**: 简单风格/物体约 100 张，复杂概念需 500+ 张。\n\n- **Q**: 训练时间多久？\n\n  - **A**: 单 GPU 约 1-6 小时（取决于数据量和参数）。\n\n- **Q**: 能否用 Colab 免费版？\n  - **A**: 可以，但需启用 `fp16` 和梯度检查点，批次设为 1。\n\n---\n\n通过以上步骤，你可以高效地训练出针对特定任务的 LoRA 模型，显著降低资源消耗同时保持生成质量。\n\n\u003e [!info] 关于数据量的疑问\n\u003e\n\u003e \u003e [!query]\n\u003e \u003e 你确定训练一个 LoRA 需要 100 多张图吗？\n\u003e\n\u003e Reasoning 过程省略\n\u003e\n\u003e \u003e [!ai]\n\u003e \u003e 训练 LoRA 模型所需的图片数量并非固定，具体取决于训练目标、数据质量和参数设置。根据多个来源的建议，最低可接受数量为 10-20 张高质量图片，而复杂场景或高精度需求可能需要 50 张以上。\n\u003e \u003e ...\n\n## Q: 我有 Python 与 PyTorch 基础。需要自己写一个 LoRA 训练脚本，应如何写？\n\n\u003e [!reasoning]-\n\u003e\n\u003e 嗯，用户之前问过如何训练 Stable Diffusion 的 LoRA 模型，现在他有了 Python 和 PyTorch 基础，想自己写一个 LoRA 训练脚本。我需要仔细分析他的需求。首先，他可能已经了解了 LoRA 的基本步骤，现在需要具体的代码指导和实现细节。用户可能希望从零开始构建训练脚本，而不仅仅是使用现有的库，比如 diffusers 或者 peft。不过，也有可能他希望结合这些库来简化流程。需要明确他的意图。\n\u003e\n\u003e 用户提到“自己写一个”，可能意味着他希望理解底层实现，而不仅仅是调用高级 API。不过，LoRA 的实现涉及到修改模型的注意力层，这部分如果完全从零开始可能会比较复杂。需要平衡代码的复杂性和可理解性。可能需要结合 PyTorch 的基础操作和现有的库，比如 huggingface 的 transformers 或 diffusers，来构建脚本。\n\u003e\n\u003e 首先，确定用户的基础：有 Python 和 PyTorch 基础，可能对深度学习模型训练有基本了解，但对 LoRA 的具体实现不熟悉。需要分步骤解释如何将 LoRA 应用到 Stable Diffusion 的 UNet 模型中，特别是注意力层部分。\n\u003e\n\u003e 接下来，可能需要考虑以下几点：\n\u003e\n\u003e 1. 数据准备部分：用户需要加载数据集，可能已经准备好了图片和对应的文本描述，需要将其转换为模型可接受的输入格式，比如 tokenized 的文本和图像编码后的潜在表示。\n\u003e\n\u003e 2. 模型加载：需要加载预训练的 Stable Diffusion 模型，特别是 UNet 部分，并修改其注意力层以加入 LoRA 参数。\n\u003e\n\u003e 3. LoRA 层的实现：如何在 PyTorch 中为线性层添加 LoRA 适配器，可能需要创建一个新的模块，将原始权重冻结，并添加低秩矩阵 A 和 B。\n\u003e\n\u003e 4. 训练循环的设置：包括损失函数（如 MSE 损失）、优化器（如 AdamW）、学习率调度等。\n\u003e\n\u003e 5. 参数更新：确保在训练过程中只更新 LoRA 的参数，而原始模型参数保持冻结。\n\u003e\n\u003e 6. 保存和加载 LoRA 权重：训练完成后如何提取 LoRA 的参数，并在推理时加载它们。\n\u003e\n\u003e 可能还需要考虑混合精度训练、梯度检查点等优化技术，以节省显存。同时，用户可能在资源有限的环境下训练，比如使用 Colab，所以需要代码尽可能高效。\n\u003e\n\u003e 另外，用户可能在之前的回答中已经看到了使用 peft 库的示例，但现在他希望自己实现，所以需要避免直接使用 peft，而是手动添加 LoRA 层。这需要详细解释 LoRA 的原理，比如低秩分解，如何将原始权重 W 替换为 W + BA，其中 A 和 B 是低秩矩阵。\n\u003e\n\u003e 需要确保代码示例清晰，步骤明确，并且指出关键部分，比如如何修改注意力层的线性层，如何冻结原始参数，如何只训练 LoRA 的参数。同时，可能需要给出数据预处理的具体示例，比如使用 Autoencoder 将图像编码为潜在空间，以及使用 CLIP 文本编码器处理文本输入。\n\u003e\n\u003e 可能还需要处理学习率的设置，因为微调通常使用较低的学习率，以及如何监控训练过程，比如损失值的下降和生成样本的质量检查。\n\u003e\n\u003e 最后，需要提醒用户注意常见问题，比如过拟合、训练时间、超参数调整等，并提供调试建议。可能还需要比较手动实现和使用现有库的优缺点，帮助用户做出选择。\n\u003e\n\u003e 总结，回答需要包括以下部分：\n\u003e\n\u003e 1. LoRA 层的 PyTorch 实现\n\u003e\n\u003e 2. 修改 Stable Diffusion 的 UNet 模型以加入 LoRA\n\u003e\n\u003e 3. 数据预处理流程\n\u003e\n\u003e 4. 训练循环的构建\n\u003e\n\u003e 5. 参数保存与加载\n\u003e\n\u003e 6. 示例代码和关键解释\n\u003e\n\u003e 7. 常见问题与优化建议\n\u003e\n\u003e 需要确保代码示例正确，并且用户能够根据步骤逐步实现，同时解释每一步的目的和原理，帮助用户理解而不仅仅是复制代码。\n\n以下是基于 PyTorch 实现 Stable Diffusion LoRA 训练脚本的完整指南，从底层原理到代码实现逐步解析。假设你已准备好数据集（图片+文本描述），我们将以 **手动实现 LoRA 层** 为核心，不依赖 `peft` 库。\n\n---\n\n### 1. **LoRA 层的原理与实现**\n\nLoRA 的核心思想：在原始权重矩阵旁增加一个低秩矩阵，只训练这部分参数。\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass LoRALayer(nn.Module):\n    def __init__(self, original_layer, rank=8, alpha=16):\n        super().__init__()\n        self.original = original_layer  # 原始线性层 (e.g., nn.Linear)\n        self.original.requires_grad_(False)  # 冻结原始参数\n\n        in_dim = original_layer.in_features\n        out_dim = original_layer.out_features\n        self.rank = rank\n        self.alpha = alpha\n\n        # 初始化低秩矩阵 A 和 B\n        self.A = nn.Parameter(torch.randn(in_dim, rank))\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n\n    def forward(self, x):\n        # 原始输出 + 低秩增量\n        orig_out = self.original(x)\n        lora_out = x @ self.A @ self.B  # [batch, out_dim]\n        return orig_out + self.alpha * lora_out\n```\n\n---\n\n### 2. **修改 Stable Diffusion 的 UNet**\n\n需将 LoRA 注入到 UNet 的 **Cross-Attention 层**（关键步骤）：\n\n```python\nfrom diffusers import UNet2DConditionModel\n\ndef inject_lora(unet, rank=8, alpha=16):\n    # 遍历 UNet 的所有注意力层\n    for name, module in unet.named_modules():\n        if \"attn2\" in name and \"to_\" in name:  # 定位 Cross-Attention 的 Q/K/V/Out 层\n            if isinstance(module, nn.Linear):\n                # 替换原始 Linear 层为 LoRA 层\n                lora_layer = LoRALayer(module, rank=rank, alpha=alpha)\n                # 通过字符串路径替换模块 (需要解析层级结构)\n                parent = unet\n                parts = name.split('.')\n                for part in parts[:-1]:\n                    parent = getattr(parent, part)\n                setattr(parent, parts[-1], lora_layer)\n    return unet\n\n# 加载原始 UNet 并注入 LoRA\nunet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\nunet = inject_lora(unet, rank=8)\n```\n\n---\n\n### 3. **数据预处理**\n\n将图片和文本转换为模型输入：\n\n```python\nfrom torchvision import transforms\nfrom transformers import CLIPTokenizer, CLIPTextModel\n\n# 加载 Stable Diffusion 的文本编码器\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# 图像预处理（转换为潜在空间）\nvae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\ndef preprocess_image(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    transform = transforms.Compose([\n        transforms.Resize(512),\n        transforms.CenterCrop(512),\n        transforms.ToTensor(),\n    ])\n    image = transform(image).unsqueeze(0)  # [1, 3, 512, 512]\n    with torch.no_grad():\n        latents = vae.encode(image).latent_dist.sample() * 0.18215  # [1, 4, 64, 64]\n    return latents\n\n# 文本编码\ndef encode_text(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=77, truncation=True)\n    with torch.no_grad():\n        text_embeddings = text_encoder(inputs.input_ids)[0]  # [1, 77, 768]\n    return text_embeddings\n```\n\n---\n\n### 4. **训练循环**\n\n```python\nimport torch.optim as optim\nfrom diffusers import DDPMScheduler\n\n# 初始化噪声调度器\nnoise_scheduler = DDPMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.0001,\n    beta_end=0.02,\n    beta_schedule=\"linear\"\n)\n\n# 优化器（仅训练 LoRA 参数）\noptimizer = optim.AdamW(\n    filter(lambda p: p.requires_grad, unet.parameters()),  # 只选择可训练参数\n    lr=1e-4,\n    weight_decay=1e-4\n)\n\n# 训练循环\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # 加载数据\n        images, texts = batch\n        latents = preprocess_image(images)\n        text_embeds = encode_text(texts)\n\n        # 添加噪声\n        noise = torch.randn_like(latents)\n        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.shape[0],))\n        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n        # 前向传播\n        noise_pred = unet(noisy_latents, timesteps, text_embeds).sample\n\n        # 计算损失\n        loss = nn.functional.mse_loss(noise_pred, noise)\n\n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n```\n\n---\n\n### 5. **保存 LoRA 权重**\n\n```python\ndef save_lora_weights(unet, save_path):\n    lora_state_dict = {}\n    for name, module in unet.named_modules():\n        if isinstance(module, LoRALayer):\n            # 保存 A 和 B 矩阵\n            lora_state_dict[f\"{name}.A\"] = module.A\n            lora_state_dict[f\"{name}.B\"] = module.B\n    torch.save(lora_state_dict, save_path)\n\nsave_lora_weights(unet, \"lora_weights.pth\")\n```\n\n---\n\n### 6. **推理时加载 LoRA**\n\n```python\n# 加载原始模型\npipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n\n# 注入 LoRA\ninject_lora(pipeline.unet, rank=8)\npipeline.unet.load_state_dict(torch.load(\"lora_weights.pth\"), strict=False)  # 仅加载 LoRA 参数\n\n# 生成图片\nimage = pipeline(\"A photo of [your trigger word]\").images[0]\n```\n\n---\n\n### **关键注意事项**\n\n1. **参数冻结**：确保原始模型的参数被冻结，只训练 LoRA 的 `A` 和 `B` 矩阵。\n2. **Rank 选择**：通常 `rank=8` 足够，复杂任务可尝试 `rank=16-64`。\n3. **混合精度训练**：添加 `with torch.autocast(\"cuda\")` 加速训练并减少显存。\n4. **梯度检查点**：对 `unet.enable_gradient_checkpointing()` 以减少显存占用。\n5. **学习率调整**：LoRA 学习率通常比全模型微调大（例如 `1e-4` vs `1e-5`）。\n\n通过此脚本，你可以完全控制 LoRA 的实现细节，适用于定制化需求或研究场景。若追求快速开发，仍推荐使用 `peft` 库的封装。\n\n## Q: 你说“需将 LoRA 注入到 UNet 的 Cross-Attention 层”， LoRA 只能加到 CrossAttention 层吗？为什么？\n\n\u003e [!reasoning]-\n\u003e\n\u003e 嗯，用户之前问过如何训练 Stable Diffusion 的 LoRA 模型，然后接着问如何自己写训练脚本。现在他们又进一步深入，问到是否 LoRA 只能加到 CrossAttention 层，以及原因。我需要仔细分析这个问题，确保回答准确且有深度。\n\u003e\n\u003e 首先，回顾一下之前的对话。用户已经具备一定的 PyTorch 基础，并且对 LoRA 的实现有初步了解。他们在之前的回答中已经看到了如何手动将 LoRA 注入到 UNet 的 Cross-Attention 层，现在可能对 LoRA 的应用范围产生了疑问，想知道是否仅限于这些层。\n\u003e\n\u003e 接下来，我需要思考 LoRA 的工作原理。LoRA 通过在原始权重旁添加低秩矩阵来微调模型，通常用于减少训练参数。在 Stable Diffusion 中，UNet 的 Cross-Attention 层是文本条件与图像生成交互的关键部分，因此调整这些层能有效影响生成结果。但用户的问题是关于是否只能在这些层应用 LoRA，我需要确认其他层是否也能应用，或者是否有其他考虑因素。\n\u003e\n\u003e 可能的误区是用户可能认为 LoRA 只能用于 Cross-Attention 层，但实际上，理论上可以应用到任何线性层。但实践中，选择特定层是为了效率和效果。需要解释为什么在 Stable Diffusion 中推荐这样做，同时说明其他可能性。\n\u003e\n\u003e 需要验证的信息包括：Stable Diffusion 的 UNet 结构，特别是不同层的作用；LoRA 在不同层应用的效果差异；相关研究或实践经验。例如，Diffusers 库的文档或相关论文可能提到最佳实践。\n\u003e\n\u003e 用户可能的深层需求是希望灵活应用 LoRA，了解其限制和可能性，以便在自己的项目中做调整。他们可能想知道是否可以在其他层（如自注意力层或 FFN）添加 LoRA，以及效果如何。\n\u003e\n\u003e 回答时需要分点说明，首先指出 LoRA 理论上可以应用于任何层，但在实际应用中 Cross-Attention 层更为关键，然后解释原因，包括参数效率、文本控制的重要性，以及实践中的效果。同时，补充其他可能的应用场景，让用户有全面的了解。\n\u003e\n\u003e 需要确保语言清晰，避免术语混乱，同时提供足够的例子和解释，帮助用户理解。可能还需要提到相关研究，比如 LoRA 最初的应用场景，或者在 Stable Diffusion 中的具体调整案例，以增强说服力。\n\u003e\n\u003e 最后，总结时强调选择 Cross-Attention 层的原因，但鼓励用户根据需求尝试其他层，并指出可能的效果差异和资源消耗。这样既解答了问题，又提供了进一步探索的方向。\n\n你的问题非常关键！**LoRA 并不只能加到 Cross-Attention 层**，但在 Stable Diffusion 微调中，**Cross-Attention 层是最关键的目标**。以下是深度解析：\n\n---\n\n### **1. 理论层面：LoRA 可以加在哪里？**\n\n从数学原理上，LoRA 是一种通用的参数高效微调方法，**可应用于任何线性层**（如 `nn.Linear` 或等效的矩阵运算）。例如：\n\n- **Self-Attention 中的 Q/K/V 层**（处理图像自身特征）\n- **Feed-Forward Network (FFN) 的中间层**\n- **甚至 VAE 或 Text Encoder 的某些层**（但较少见）\n\n但实际应用中，**选择注入位置需权衡效果与效率**。\n\n---\n\n### **2. 为什么 Stable Diffusion 优先选择 Cross-Attention 层？**\n\n#### **(1) 文本-图像对齐的核心枢纽**\n\n- **Cross-Attention 层**是 UNet 中**文本条件（Text Embedding）与图像特征交互的唯一通道**。\n- 修改这些层能直接控制生成内容如何响应文本提示（Prompt），例如：\n  - 学习新概念（如特定物体 `my_cat_v1`）\n  - 调整风格（如油画风 `van gogh style`）\n\n#### **(2) 参数效率最大化**\n\n- Stable Diffusion 的 UNet 参数量巨大（约 860M），但 **Cross-Attention 层仅占约 10% 的参数量**。\n- 仅微调这部分，即可用 **0.1%~1% 的参数量达到接近全参数微调的效果**，性价比极高。\n\n#### **(3) 实践经验验证**\n\n- 大量实验表明，**调整 Cross-Attention 层对生成结果影响最显著**。\n- 若同时微调 Self-Attention 或 FFN 层，可能引入噪声（如破坏预训练模型的构图能力）。\n\n---\n\n### **3. 其他可能的目标层**\n\n#### **(1) Self-Attention 层**\n\n- **作用**：控制图像内部结构（如物体形状、布局）。\n- **适用场景**：需要修改生成图像的构图规则（如强制对称性）。\n- **风险**：容易导致模型遗忘预训练知识（如失去多样性）。\n\n#### **(2) FFN 层**\n\n- **作用**：非线性特征变换。\n- **适用场景**：调整颜色、纹理等低级特征。\n- **缺点**：参数量大，效率较低。\n\n#### **(3) 组合策略**\n\n- 同时微调 **Cross-Attention + 部分 Self-Attention**，可平衡控制力与稳定性，但需更多显存和训练数据。\n\n---\n\n### **4. 代码示例：注入到 Self-Attention**\n\n只需修改 `inject_lora` 函数中的目标模块筛选条件：\n\n```python\ndef inject_lora(unet, rank=8):\n    for name, module in unet.named_modules():\n        # 定位 Self-Attention 的 Q/K/V 层（\"attn1\" 表示 Self-Attention）\n        if \"attn1\" in name and \"to_\" in name:\n            if isinstance(module, nn.Linear):\n                # 替换为 LoRALayer\n                ...\n    return unet\n```\n\n---\n\n### **5. 如何选择注入位置？**\n\n| **目标层类型**      | **参数量占比** | **影响范围**       | **推荐场景**                |\n| ------------------- | -------------- | ------------------ | --------------------------- |\n| **Cross-Attention** | ~10%           | 文本控制、概念学习 | 学习新物体/风格（主流选择） |\n| **Self-Attention**  | ~20%           | 图像结构、布局     | 特殊构图需求                |\n| **FFN**             | ~70%           | 颜色、纹理         | 低级特征微调                |\n\n---\n\n### **6. 终极建议**\n\n- **优先 Cross-Attention**：95% 的场景只需调整这里。\n- **谨慎添加其他层**：仅在数据充足且任务特殊时尝试（如医学图像生成需精确形状控制）。\n- **监控过拟合**：添加更多可训练参数会增加过拟合风险，需配合数据增强/早停法。\n\n\u003e [!think] 亮曰：\n\u003e\n\u003e `UNet2DConditionModel` 的整个流程中， Cross-Attention 主要作用在下采样、中间块、上采样三个步骤中。在 `__init__` 参数中传入每个步骤中用的 Module ，其中用 `CrossAttnDownBlock2D` 等 Module `self.has_cross_attention` 为 true ，能处理 cross_attention_mask 等参数。其中的 attentions 层即为交叉注意力层。推测，是通过 Attentions 层对下采样层入参做处理，并列的 ResNet 应该是为了提升注意力层的训练性能。\n\n## UNet 代码中的 Cross-Attension 层\n\n\u003e 以下为在 Cursor 中有 diffusers 代码上下文时与 claude-3.7-sonnet 的对话内容。\n\n\u003e [!query]\n\u003e UNet 中 Cross-Attension 是做什么的？具体到代码里如何作用？ @unet_2d_condition.py\n\n\u003e [!reasoning]-\n\u003e\n\u003e 这个问题是关于 UNet 中 Cross-Attention 机制的作用，特别是在 UNet2DConditionModel 中的实现和作用。我需要查找和分析代码中与 Cross-Attention 相关的部分。\n\u003e\n\u003e 从代码中可以看出，UNet2DConditionModel 是一个条件式的 2D UNet 模型，它通常用于扩散模型中，接收噪声图像样本、条件状态和时间步长作为输入，并返回处理后的样本。\n\u003e\n\u003e 在扩散模型（如 Stable Diffusion）中，Cross-Attention 是一种非常重要的机制，它允许 UNet 处理两种不同类型的信息：\n\u003e\n\u003e 空间信息（图像特征）\n\u003e\n\u003e 上下文信息（通常是文本编码或其他条件编码）\n\u003e\n\u003e 让我查看代码中与 Cross-Attention 相关的关键部分：\n\u003e\n\u003e 模型在初始化时，设置了一些与 Cross-Attention 相关的参数，如 cross_attention_dim，only_cross_attention 等。\n\u003e\n\u003e 在 forward 方法中，encoder_hidden_states 参数通常是从文本编码器获得的表示，它被传递给具有 cross-attention 功能的下采样、中间和上采样块。\n\u003e\n\u003e 模型使用了某种注意力处理器（AttentionProcessor），可能是为了优化注意力计算。\n\u003e\n\u003e 让我仔细查看 down_blocks 和 up_blocks 中的 Cross-Attention 实现，以及它们是如何与 encoder_hidden_states 交互的。\n\nCross-Attention 在 UNet 中主要用于将条件信息（如文本特征）引入到图像生成过程中。从代码来看，主要体现在以下几个方面：\n\n1. **数据流向**：\n\n   - UNet 接收图像特征(`sample`)和条件编码(`encoder_hidden_states`)\n   - `encoder_hidden_states`通常是文本编码器输出的特征\n   - 条件信息通过 Cross-Attention 在 UNet 的不同层级与图像特征交互\n\n2. **代码实现**：\n\n   ```py 837:844:src/diffusers/models/unets/unet_2d_condition.py\n   if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n       sample, res_samples = downsample_block(\n           hidden_states=sample,\n           temb=emb,\n           encoder_hidden_states=encoder_hidden_states,\n           attention_mask=attention_mask,\n           cross_attention_kwargs=cross_attention_kwargs,\n           encoder_attention_mask=encoder_attention_mask,\n           **additional_residuals,\n       )\n   ```\n\n3. **核心组件**：\n\n   - 在下采样、中间块和上采样阶段都可能使用 Cross-Attention\n   - 通过`has_cross_attention`属性判断块是否需要使用 Cross-Attention\n   - 当使用 Cross-Attention 时，`encoder_hidden_states`会作为额外输入传递\n\n4. **参数控制**：\n   - `only_cross_attention`参数控制是否只使用 Cross-Attention 而不使用 Self-Attention\n   - `cross_attention_dim`指定了条件编码的维度大小\n   - `transformer_layers_per_block`控制每个块中 Transformer 层的数量\n\nCross-Attention 允许模型在保持图像空间结构的同时，根据文本或其他条件调整生成内容，实现了条件生成的核心机制。\n\n\u003e [!query]\n\u003e TransformerBlock 具体是在@transformer_2d.py 里实现吗？如何体现 Self-Attention 与 Cross-Attention ？\n\n\u003e [!reasoning]-\n\u003e\n\u003e 让我来分析一下 TransformerBlock 的实现以及 Self-Attention 和 Cross-Attention 的具体体现。\n\u003e\n\u003e 在 transformer_2d.py 文件中，可以看到 Transformer2DModel 类，但 TransformerBlock 的具体实现似乎是引用自其他地方。注意到以下导入:\n\u003e\n\u003e 这表明 BasicTransformerBlock 类实际上是在 attention.py 文件中定义的，而不是直接在 transformer_2d.py 中。\n\u003e\n\u003e 在 transformer_2d.py 中，BasicTransformerBlock 被用在以下位置:\n\u003e\n\u003e 从这段代码中可以看出:\n\u003e\n\u003e BasicTransformerBlock 是接收 self-attention 和 cross-attention 相关参数的，包括:\n\u003e\n\u003e cross_attention_dim: 交叉注意力的维度\n\u003e\n\u003e only_cross_attention: 是否只使用交叉注意力而不使用自注意力\n\u003e\n\u003e double_self_attention: 是否使用双重自注意力\n\u003e\n\u003e 在转发(forward)函数中，我们可以看到如何传递注意力相关的参数:\n\u003e\n\u003e 这里的关键参数是:\n\u003e\n\u003e hidden_states: 输入隐藏状态，用于 Self-Attention\n\u003e\n\u003e encoder_hidden_states: 编码器隐藏状态，用于 Cross-Attention\n\u003e\n\u003e attention_mask: 自注意力的掩码\n\u003e\n\u003e encoder_attention_mask: 交叉注意力的掩码\n\u003e\n\u003e 虽然我们可以从这些代码推断出 BasicTransformerBlock 中如何实现自注意力和交叉注意力，但由于实际实现不在 transformer_2d.py 文件中，我们需要查看 attention.py 文件才能了解完整的实现细节。\n\u003e\n\u003e 基于现有的代码，我们可以推断:\n\u003e\n\u003e Self-Attention 是使用 hidden_states 对自身进行注意力计算\n\u003e\n\u003e Cross-Attention 是将 hidden_states 作为查询(query)，将 encoder_hidden_states 作为键(key)和值(value)进行注意力计算\n\u003e\n\u003e 可以通过 only_cross_attention 参数控制是否只使用 Cross-Attention 而不使用 Self-Attention\n\nTransformerBlock 的核心实现是在`attention.py`中的`BasicTransformerBlock`类中，而不是直接在`transformer_2d.py`中。在`transformer_2d.py`中只是调用了这个类来构建 Transformer 模型。\n\n从`transformer_2d.py`代码中可以看到 Self-Attention 和 Cross-Attention 的几个关键特点：\n\n1. **实例化 transformer 块时**:\n\n   ```156:169:src/diffusers/models/transformers/transformer_2d.py\n   BasicTransformerBlock(\n       self.inner_dim,\n       self.config.num_attention_heads,\n       self.config.attention_head_dim,\n       dropout=self.config.dropout,\n       cross_attention_dim=self.config.cross_attention_dim,\n       activation_fn=self.config.activation_fn,\n       num_embeds_ada_norm=self.config.num_embeds_ada_norm,\n       attention_bias=self.config.attention_bias,\n       only_cross_attention=self.config.only_cross_attention,\n       double_self_attention=self.config.double_self_attention,\n       ...\n   )\n   ```\n\n2. **调用时的数据流**:\n   ```386:396:src/diffusers/models/transformers/transformer_2d.py\n   hidden_states = block(\n       hidden_states,\n       attention_mask=attention_mask,\n       encoder_hidden_states=encoder_hidden_states,\n       encoder_attention_mask=encoder_attention_mask,\n       timestep=timestep,\n       cross_attention_kwargs=cross_attention_kwargs,\n       class_labels=class_labels,\n   )\n   ```\n\n核心区别：\n\n- **Self-Attention**: 使用`hidden_states`自身进行注意力计算，`attention_mask`作为掩码\n- **Cross-Attention**: 使用`hidden_states`作为 query，`encoder_hidden_states`作为 key 和 value，`encoder_attention_mask`作为掩码\n\n配置选项：\n\n- `only_cross_attention`: 如果为 True，只使用 Cross-Attention 而跳过 Self-Attention\n- `double_self_attention`: 如果为 True，使用两次 Self-Attention\n- `cross_attention_dim`: 指定 Cross-Attention 的维度大小\n\n在实际运行时，BasicTransformerBlock 内部会先进行 Self-Attention 操作，然后进行 Cross-Attention 操作（如果提供了`encoder_hidden_states`）。这样就实现了 UNet 中文本条件对图像生成的影响。\n\n\u003e [!query]\n\u003e @attention.py 中 BasicTransformerBlock 如何体现 Self-Attention 与 Cross-Attention ?\n\n\u003e [!reasoning]\n\u003e\n\u003e 让我看看 attention.py 中的 BasicTransformerBlock 类，特别是它如何实现 Self-Attention 和 Cross-Attention 的部分。\n\u003e\n\u003e 从代码中，我可以看到 BasicTransformerBlock 类包含了以下几个关键组件：\n\u003e\n\u003e 初始化函数中，它设置了多个 Attention 模块和标准化层：\n\u003e\n\u003e self.norm1 和 self.attn1 用于 Self-Attention\n\u003e\n\u003e self.norm2 和 self.attn2 用于 Cross-Attention（如果需要的话）\n\u003e\n\u003e self.norm3 和 self.ff 用于前馈网络\n\u003e\n\u003e 在 forward 函数中，处理流程如下：\n\u003e\n\u003e 首先对输入进行 normalization\n\u003e\n\u003e 然后应用 Self-Attention（self.attn1）\n\u003e\n\u003e 如果有 cross_attention_dim 或 double_self_attention，则应用 Cross-Attention（self.attn2）\n\u003e\n\u003e 最后是 Feed-Forward 网络\n\u003e\n\u003e 特别需要注意的是，对于 Self-Attention 和 Cross-Attention 的区别：\n\u003e\n\u003e Self-Attention：使用 self.attn1，只使用 hidden_states 作为输入，在 only_cross_attention 为 True 时可能使用 encoder_hidden_states。\n\u003e\n\u003e Cross-Attention：使用 self.attn2，hidden_states 作为 query，encoder_hidden_states 作为 key 和 value。\n\u003e\n\u003e 在 forward 方法中，我们可以清楚看到这一点：\n\u003e\n\u003e 对于 Self-Attention，attn_output = self.attn1(norm_hidden_states, ...)\n\u003e\n\u003e 对于 Cross-Attention，attn_output = self.attn2(norm_hidden_states, encoder_hidden_states=encoder_hidden_states, ...)\n\u003e\n\u003e 我会重点关注 forward 方法中实现 Self-Attention 和 Cross-Attention 的关键部分。\n\n在`attention.py`中，`BasicTransformerBlock`类通过以下方式实现了 Self-Attention 和 Cross-Attention：\n\n### 1. 架构设计\n\n```py 326:361:src/diffusers/models/attention.py\n# 1. Self-Attn\nself.attn1 = Attention(\n    query_dim=dim,\n    heads=num_attention_heads,\n    dim_head=attention_head_dim,\n    dropout=dropout,\n    bias=attention_bias,\n    cross_attention_dim=cross_attention_dim if only_cross_attention else None,\n    upcast_attention=upcast_attention,\n    out_bias=attention_out_bias,\n)\n\n# 2. Cross-Attn\nif cross_attention_dim is not None or double_self_attention:\n    self.norm2 = nn.LayerNorm(dim, norm_eps, norm_elementwise_affine)\n\n    self.attn2 = Attention(\n        query_dim=dim,\n        cross_attention_dim=cross_attention_dim if not double_self_attention else None,\n        heads=num_attention_heads,\n        dim_head=attention_head_dim,\n        dropout=dropout,\n        bias=attention_bias,\n        upcast_attention=upcast_attention,\n        out_bias=attention_out_bias,\n    )\n```\n\n### 2. 前向传播过程\n\n```py 500:525:src/diffusers/models/attention.py\n# 1. Self-Attention\nnorm_hidden_states = self.norm1(hidden_states)\nattn_output = self.attn1(\n    norm_hidden_states,\n    encoder_hidden_states=encoder_hidden_states if self.only_cross_attention else None,\n    attention_mask=attention_mask,\n    **cross_attention_kwargs,\n)\n\n# ... (处理输出)\nhidden_states = attn_output + hidden_states\n\n# 2. Cross-Attention\nif self.attn2 is not None:\n    norm_hidden_states = self.norm2(hidden_states)\n    attn_output = self.attn2(\n        norm_hidden_states,\n        encoder_hidden_states=encoder_hidden_states,\n        attention_mask=encoder_attention_mask,\n        **cross_attention_kwargs,\n    )\n    hidden_states = attn_output + hidden_states\n```\n\n### 3. 关键区别\n\n- **Self-Attention (attn1)**：\n\n  - 默认情况下只使用`hidden_states`作为输入（query, key, value 都来自同一源）\n  - 只有当`only_cross_attention=True`时才会用`encoder_hidden_states`\n  - 使用`attention_mask`作为掩码\n\n- **Cross-Attention (attn2)**：\n  - `norm_hidden_states`作为 query\n  - `encoder_hidden_states`作为 key 和 value\n  - 使用`encoder_attention_mask`作为掩码\n  - 当`encoder_hidden_states=None`时会退化为 Self-Attention\n\n### 4. 配置控制\n\n- `only_cross_attention`：是否在 attn1 中使用 cross-attention\n- `double_self_attention`：是否将 attn2 配置为另一个 self-attention 层\n- `cross_attention_dim`：指定条件编码的维度\n\n这种设计允许 UNet 模型在不同层级灵活组合 Self-Attention 和 Cross-Attention，实现对图像特征的内部处理和对条件信息的整合。\n\n\u003e [!query]\n\u003e 我注意到 @attention.py 中在 Attention 层前都会有一个 LayerNorm 层。这是必须要有的吗？有什么作用？\n\n\u003e [!reasoning]-\n\u003e 这个问题询问的是关于在 Attention 层前使用 LayerNorm 的必要性和作用。我需要解释 LayerNorm 在 Transformer 结构中的重要性。\n\u003e 在 attention.py 中的 BasicTransformerBlock 实现中，每个 Attention 操作前确实都有一个 normalization 层（通常是 LayerNorm），我应该分析这种设计的原因。\n\n是的，在`attention.py`的`BasicTransformerBlock`中，每个 Attention 层前都有一个归一化层（通常是 LayerNorm），这是非常重要的设计：\n\n### LayerNorm 的必要性\n\n1. **训练稳定性**：\n\n   ```py 453:460:src/diffusers/models/attention.py\n   # 在Self-Attention前应用归一化\n   if self.norm_type == \"ada_norm\":\n       norm_hidden_states = self.norm1(hidden_states, timestep)\n   elif self.norm_type == \"ada_norm_zero\":\n       norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(\n           hidden_states, timestep, class_labels, hidden_dtype=hidden_states.dtype\n       )\n   elif self.norm_type in [\"layer_norm\", \"layer_norm_i2vgen\"]:\n       norm_hidden_states = self.norm1(hidden_states)\n   ```\n\n2. **梯度流动**：\n   - 归一化层将数据分布标准化，防止梯度消失或爆炸\n   - 特别是在深层网络中，保持梯度流动对训练至关重要\n\n### LayerNorm 的作用\n\n1. **特征分布稳定**：\n\n   - 将特征归一化到相同尺度，减少批次间差异\n   - 使网络对输入变化更加鲁棒，提高泛化能力\n\n2. **残差连接的配合**：\n\n   ```py 510:511:src/diffusers/models/attention.py\n   # 应用残差连接\n   hidden_states = attn_output + hidden_states\n   ```\n\n   - Pre-LayerNorm 结构（先归一化后 Attention）与残差连接配合使用\n   - 这种\"归一化 → 操作 → 残差\"的模式是 Transformer 架构的基本组成部分\n\n3. **多种归一化选择**：\n   ```py 454:461:src/diffusers/models/attention.py\n   # 不同类型的归一化层\n   if self.norm_type == \"ada_norm\":\n       norm_hidden_states = self.norm1(hidden_states, timestep)\n   elif self.norm_type == \"ada_norm_zero\":\n       # ...\n   elif self.norm_type in [\"layer_norm\", \"layer_norm_i2vgen\"]:\n       # ...\n   ```\n   - 基本的 LayerNorm\n   - 带条件的 AdaLayerNorm（使用 timestep 信息调整）\n   - 其他特殊归一化变体，适应不同场景\n\n总之，LayerNorm 在 Attention 层前是必不可少的，它是保证 Transformer 结构有效训练和良好性能的关键组件，特别是在生成模型这样需要处理大量复杂特征的深度网络中。\n","title":"PyTorch 实现 Stable Diffusion LoRA 训练脚本：从数据预处理到模型修改与训练循环","abstract":"\u003e 本文详细介绍了如何使用 PyTorch 从零开始编写 Stable Diffusion LoRA 训练脚本，包括数据预处理、模型修改、训练循环、参数保存与加载等关键步骤。特别强调了 LoRA 层的手动实现和在 UNet 的 Cross-Attention 层注入 LoRA 的原因，以及在其他层应用 LoRA 的可能性和注意事项。此外，还提供了代码示例和参数效率的讨论，帮助读者深入理解 LoRA 在 Stable Diffusion 微调中的应用。\n\u003e [!reasoning]-\n\u003e","length":883,"created_at":"2025-03-29T02:00:00.000Z","updated_at":"2025-03-29T02:00:00.000Z","tags":["深度学习","Stable Diffusion","LoRA","参数高效微调","模型训练","学习笔记"],"license":true,"headingTrees":[{"key":"q-训练一个-stable-diffusion-的-lora-模型要有哪些步骤","href":"#q-训练一个-stable-diffusion-的-lora-模型要有哪些步骤","heading":2,"title":"Q: 训练一个 Stable Diffusion 的 LoRA 模型，要有哪些步骤？","children":[{"key":"1-数据准备","href":"#1-数据准备","heading":3,"title":"1. 数据准备","children":[],"id":"1-数据准备"},{"key":"2-环境配置","href":"#2-环境配置","heading":3,"title":"2. 环境配置","children":[],"id":"2-环境配置"},{"key":"3-模型准备","href":"#3-模型准备","heading":3,"title":"3. 模型准备","children":[],"id":"3-模型准备"},{"key":"4-训练配置","href":"#4-训练配置","heading":3,"title":"4. 训练配置","children":[],"id":"4-训练配置"},{"key":"5-训练过程","href":"#5-训练过程","heading":3,"title":"5. 训练过程","children":[],"id":"5-训练过程"},{"key":"6-推理与部署","href":"#6-推理与部署","heading":3,"title":"6. 推理与部署","children":[],"id":"6-推理与部署"},{"key":"常见问题","href":"#常见问题","heading":3,"title":"常见问题","children":[],"id":"常见问题"}],"id":"q-训练一个-stable-diffusion-的-lora-模型要有哪些步骤"},{"key":"q-我有-python-与-pytorch-基础需要自己写一个-lora-训练脚本应如何写","href":"#q-我有-python-与-pytorch-基础需要自己写一个-lora-训练脚本应如何写","heading":2,"title":"Q: 我有 Python 与 PyTorch 基础。需要自己写一个 LoRA 训练脚本，应如何写？","children":[{"key":"1-lora-层的原理与实现","href":"#1-lora-层的原理与实现","heading":3,"title":"1. LoRA 层的原理与实现","children":[],"id":"1-lora-层的原理与实现"},{"key":"2-修改-stable-diffusion-的-unet","href":"#2-修改-stable-diffusion-的-unet","heading":3,"title":"2. 修改 Stable Diffusion 的 UNet","children":[],"id":"2-修改-stable-diffusion-的-unet"},{"key":"3-数据预处理","href":"#3-数据预处理","heading":3,"title":"3. 数据预处理","children":[],"id":"3-数据预处理"},{"key":"4-训练循环","href":"#4-训练循环","heading":3,"title":"4. 训练循环","children":[],"id":"4-训练循环"},{"key":"5-保存-lora-权重","href":"#5-保存-lora-权重","heading":3,"title":"5. 保存 LoRA 权重","children":[],"id":"5-保存-lora-权重"},{"key":"6-推理时加载-lora","href":"#6-推理时加载-lora","heading":3,"title":"6. 推理时加载 LoRA","children":[],"id":"6-推理时加载-lora"},{"key":"关键注意事项","href":"#关键注意事项","heading":3,"title":"关键注意事项","children":[],"id":"关键注意事项"}],"id":"q-我有-python-与-pytorch-基础需要自己写一个-lora-训练脚本应如何写"},{"key":"q-你说需将-lora-注入到-unet-的-cross-attention-层-lora-只能加到-crossattention-层吗为什么","href":"#q-你说需将-lora-注入到-unet-的-cross-attention-层-lora-只能加到-crossattention-层吗为什么","heading":2,"title":"Q: 你说“需将 LoRA 注入到 UNet 的 Cross-Attention 层”， LoRA 只能加到 CrossAttention 层吗？为什么？","children":[{"key":"1-理论层面lora-可以加在哪里","href":"#1-理论层面lora-可以加在哪里","heading":3,"title":"1. 理论层面：LoRA 可以加在哪里？","children":[],"id":"1-理论层面lora-可以加在哪里"},{"key":"2-为什么-stable-diffusion-优先选择-cross-attention-层","href":"#2-为什么-stable-diffusion-优先选择-cross-attention-层","heading":3,"title":"2. 为什么 Stable Diffusion 优先选择 Cross-Attention 层？","children":[{"key":"1-文本-图像对齐的核心枢纽","href":"#1-文本-图像对齐的核心枢纽","heading":4,"title":"(1) 文本-图像对齐的核心枢纽","children":[],"id":"1-文本-图像对齐的核心枢纽"},{"key":"2-参数效率最大化","href":"#2-参数效率最大化","heading":4,"title":"(2) 参数效率最大化","children":[],"id":"2-参数效率最大化"},{"key":"3-实践经验验证","href":"#3-实践经验验证","heading":4,"title":"(3) 实践经验验证","children":[],"id":"3-实践经验验证"}],"id":"2-为什么-stable-diffusion-优先选择-cross-attention-层"},{"key":"3-其他可能的目标层","href":"#3-其他可能的目标层","heading":3,"title":"3. 其他可能的目标层","children":[{"key":"1-self-attention-层","href":"#1-self-attention-层","heading":4,"title":"(1) Self-Attention 层","children":[],"id":"1-self-attention-层"},{"key":"2-ffn-层","href":"#2-ffn-层","heading":4,"title":"(2) FFN 层","children":[],"id":"2-ffn-层"},{"key":"3-组合策略","href":"#3-组合策略","heading":4,"title":"(3) 组合策略","children":[],"id":"3-组合策略"}],"id":"3-其他可能的目标层"},{"key":"4-代码示例注入到-self-attention","href":"#4-代码示例注入到-self-attention","heading":3,"title":"4. 代码示例：注入到 Self-Attention","children":[],"id":"4-代码示例注入到-self-attention"},{"key":"5-如何选择注入位置","href":"#5-如何选择注入位置","heading":3,"title":"5. 如何选择注入位置？","children":[],"id":"5-如何选择注入位置"},{"key":"6-终极建议","href":"#6-终极建议","heading":3,"title":"6. 终极建议","children":[],"id":"6-终极建议"}],"id":"q-你说需将-lora-注入到-unet-的-cross-attention-层-lora-只能加到-crossattention-层吗为什么"},{"key":"unet-代码中的-cross-attension-层","href":"#unet-代码中的-cross-attension-层","heading":2,"title":"UNet 代码中的 Cross-Attension 层","children":[{"key":"1-架构设计","href":"#1-架构设计","heading":3,"title":"1. 架构设计","children":[],"id":"1-架构设计"},{"key":"2-前向传播过程","href":"#2-前向传播过程","heading":3,"title":"2. 前向传播过程","children":[],"id":"2-前向传播过程"},{"key":"3-关键区别","href":"#3-关键区别","heading":3,"title":"3. 关键区别","children":[],"id":"3-关键区别"},{"key":"4-配置控制","href":"#4-配置控制","heading":3,"title":"4. 配置控制","children":[],"id":"4-配置控制"},{"key":"layernorm-的必要性","href":"#layernorm-的必要性","heading":3,"title":"LayerNorm 的必要性","children":[],"id":"layernorm-的必要性"},{"key":"layernorm-的作用","href":"#layernorm-的作用","heading":3,"title":"LayerNorm 的作用","children":[],"id":"layernorm-的作用"}],"id":"unet-代码中的-cross-attension-层"}],"wikiRefAliases":[],"richRefAliases":[]},"prevNextInfo":{"prevInfo":{"pathMapping":{"pagePath":"/learn_from_ai/golang-scheduler-preemption","filePath":"public/content/learn_from_ai/2025-03-29-golang-scheduler-preemption.md"},"meta":{"title":"Go语言的调度机制：协作式与抢占式调度分析","created_at":"2025-03-29T22:15:39.000Z","updated_at":"2025-03-29T22:15:39.000Z"}},"nextInfo":{"pathMapping":{"pagePath":"/learn_from_ai/mqtt-protocol-principles-applications","filePath":"public/content/learn_from_ai/2025-03-28-mqtt-protocol-principles-applications.md"},"meta":{"title":"详细介绍 MQTT 协议的内容，原理，应用场景","created_at":"2025-03-28T06:35:00.000Z","updated_at":"2025-03-28T06:35:00.000Z"}}},"backRefResources":[]},"__N_SSG":true},"page":"/learn_from_ai/[slug]","query":{"slug":"stable-diffusion-lora-training-methods"},"buildId":"1A7lYf5o3SCViJReqMN9a","assetPrefix":"/blog-next","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>