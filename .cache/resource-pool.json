{
  "staticResources": [
    {
      "pathMapping": {
        "filePath": "public/content/articles/2020-01-27-Building-this-blog.md",
        "pagePath": "/content/articles/2020-01-27-Building-this-blog.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2020-01-27-hello-world.md",
        "pagePath": "/content/articles/2020-01-27-hello-world.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2020-01-28-the-using-in-cpp.md",
        "pagePath": "/content/articles/2020-01-28-the-using-in-cpp.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2020-08-02-python-dict.md",
        "pagePath": "/content/articles/2020-08-02-python-dict.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2021-01-11-Sort-algorithm.md",
        "pagePath": "/content/articles/2021-01-11-Sort-algorithm.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2021-03-21-Handy-heap-cheat-sheet.md",
        "pagePath": "/content/articles/2021-03-21-Handy-heap-cheat-sheet.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2021-08-21-The-beauty-of-design-parten.md",
        "pagePath": "/content/articles/2021-08-21-The-beauty-of-design-parten.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2021-12-12-init-a-new-hexo-project.md",
        "pagePath": "/content/articles/2021-12-12-init-a-new-hexo-project.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-03-26-create-blog-cicd-by-github.md",
        "pagePath": "/content/articles/2022-03-26-create-blog-cicd-by-github.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-04-03-use-paste-image-and-vscode-memo.md",
        "pagePath": "/content/articles/2022-04-03-use-paste-image-and-vscode-memo.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-04-03-use-paste-image-and-vscode-memo/这部分内容会被转换为图片.png",
        "pagePath": "/content/articles/2022-04-03-use-paste-image-and-vscode-memo/这部分内容会被转换为图片.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-06-28-graph-for-economics-1.md",
        "pagePath": "/content/articles/2022-06-28-graph-for-economics-1.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-06-28-graph-for-economics-1/_manim/scene.py",
        "pagePath": "/content/articles/2022-06-28-graph-for-economics-1/_manim/scene.py"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-06-28-graph-for-economics-1/media/texts/5ebb16e84fc1e7d7.svg",
        "pagePath": "/content/articles/2022-06-28-graph-for-economics-1/media/texts/5ebb16e84fc1e7d7.svg"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-06-28-graph-for-economics-1/media/videos/scene/480p15/Economic2.mp4",
        "pagePath": "/content/articles/2022-06-28-graph-for-economics-1/media/videos/scene/480p15/Economic2.mp4"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-06-28-graph-for-economics-1/media/videos/scene/480p15/partial_movie_files/Economic2/3163782288_190800790_3625839045.mp4",
        "pagePath": "/content/articles/2022-06-28-graph-for-economics-1/media/videos/scene/480p15/partial_movie_files/Economic2/3163782288_190800790_3625839045.mp4"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-06-28-graph-for-economics-1/media/videos/scene/480p15/partial_movie_files/Economic2/partial_movie_file_list.txt",
        "pagePath": "/content/articles/2022-06-28-graph-for-economics-1/media/videos/scene/480p15/partial_movie_files/Economic2/partial_movie_file_list.txt"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-07-19-graph-for-economics-2.md",
        "pagePath": "/content/articles/2022-07-19-graph-for-economics-2.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-07-31-why-homogeneous.md",
        "pagePath": "/content/articles/2022-07-31-why-homogeneous.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-07-31-why-homogeneous/_manim/scene.py",
        "pagePath": "/content/articles/2022-07-31-why-homogeneous/_manim/scene.py"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-07-31-why-homogeneous/HomogeneousTransform_ManimCE_v0.16.0.post0.gif",
        "pagePath": "/content/articles/2022-07-31-why-homogeneous/HomogeneousTransform_ManimCE_v0.16.0.post0.gif"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-07-31-why-homogeneous/OnOneLineWillStillOneLine_ManimCE_v0.16.0.post0.gif",
        "pagePath": "/content/articles/2022-07-31-why-homogeneous/OnOneLineWillStillOneLine_ManimCE_v0.16.0.post0.gif"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-07-31-why-homogeneous/SliceOnHomogeneousWithGraph_ManimCE_v0.16.0.post0.gif",
        "pagePath": "/content/articles/2022-07-31-why-homogeneous/SliceOnHomogeneousWithGraph_ManimCE_v0.16.0.post0.gif"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-07-31-why-homogeneous/SliceScaleRotateForOrigin_ManimCE_v0.16.0.post0.gif",
        "pagePath": "/content/articles/2022-07-31-why-homogeneous/SliceScaleRotateForOrigin_ManimCE_v0.16.0.post0.gif"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-08-13-introduction-for-k8s.md",
        "pagePath": "/content/articles/2022-08-13-introduction-for-k8s.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-08-20-introduction-for-k8s-2.md",
        "pagePath": "/content/articles/2022-08-20-introduction-for-k8s-2.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-08-31-introduction-for-k8s-4/aws-elb-controller-lines.png",
        "pagePath": "/content/articles/2022-08-31-introduction-for-k8s-4/aws-elb-controller-lines.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2024-11-16-try-cursor-and-thinking.md",
        "pagePath": "/content/articles/2024-11-16-try-cursor-and-thinking.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2024-11-16-try-cursor-and-thinking/Pasted image 20241116200148.png",
        "pagePath": "/content/articles/2024-11-16-try-cursor-and-thinking/Pasted image 20241116200148.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2024-11-16-try-cursor-and-thinking/Pasted image 20241116200220.png",
        "pagePath": "/content/articles/2024-11-16-try-cursor-and-thinking/Pasted image 20241116200220.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/blog-in-next.md",
        "pagePath": "/content/ideas/blog-in-next.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/blog-syntax.md",
        "pagePath": "/content/ideas/blog-syntax.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/blog-syntax/Drawing 2024-04-13 17.33.27.excalidraw",
        "pagePath": "/content/ideas/blog-syntax/Drawing 2024-04-13 17.33.27.excalidraw"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/blog-syntax/test-img-show-image.png",
        "pagePath": "/content/ideas/blog-syntax/test-img-show-image.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/first-idea.md",
        "pagePath": "/content/ideas/first-idea.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux Clone VS Fork.md",
        "pagePath": "/content/ideas/Linux Clone VS Fork.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux Systemd.md",
        "pagePath": "/content/ideas/Linux Systemd.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 信号处理 —— Signal.md",
        "pagePath": "/content/ideas/Linux 信号处理 —— Signal.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 信号处理 —— Signal/Pasted image 20240819014740.png",
        "pagePath": "/content/ideas/Linux 信号处理 —— Signal/Pasted image 20240819014740.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 内存分页、分段.md",
        "pagePath": "/content/ideas/Linux 内存 —— 内存分页、分段.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110131944.png",
        "pagePath": "/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110131944.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110132032.png",
        "pagePath": "/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110132032.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110132824.png",
        "pagePath": "/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110132824.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110133623.png",
        "pagePath": "/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110133623.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110134320.png",
        "pagePath": "/content/ideas/Linux 内存 —— 内存分页、分段/Pasted image 20231110134320.png"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 堆和栈.md",
        "pagePath": "/content/ideas/Linux 内存 —— 堆和栈.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 虚拟内存.md",
        "pagePath": "/content/ideas/Linux 内存 —— 虚拟内存.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 文件描述符.md",
        "pagePath": "/content/ideas/Linux 文件描述符.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 调度 —— 进程与线程.md",
        "pagePath": "/content/ideas/Linux 调度 —— 进程与线程.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/newest.mdx",
        "pagePath": "/content/ideas/newest.mdx"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/using-chart-js.mdx",
        "pagePath": "/content/ideas/using-chart-js.mdx"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2024-11-29-deep-learning-model-formats.md",
        "pagePath": "/content/learn_from_ai/2024-11-29-deep-learning-model-formats.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2024-11-29-opencv-coordinate-system-conventions.md",
        "pagePath": "/content/learn_from_ai/2024-11-29-opencv-coordinate-system-conventions.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2024-12-08-cpp-rvo-and-rust-move-semantics.md",
        "pagePath": "/content/learn_from_ai/2024-12-08-cpp-rvo-and-rust-move-semantics.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2024-12-08-executable-file-formats.md",
        "pagePath": "/content/learn_from_ai/2024-12-08-executable-file-formats.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-05-lora-matrix-initialization-strategy.md",
        "pagePath": "/content/learn_from_ai/2025-03-05-lora-matrix-initialization-strategy.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-06-pytorch-backpropagation-mechanism.md",
        "pagePath": "/content/learn_from_ai/2025-03-06-pytorch-backpropagation-mechanism.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-07-golang-new-and-memory-management.md",
        "pagePath": "/content/learn_from_ai/2025-03-07-golang-new-and-memory-management.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-07-pytorch-model-save-and-load.md",
        "pagePath": "/content/learn_from_ai/2025-03-07-pytorch-model-save-and-load.md"
      },
      "meta": {}
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-07-resnet-math-and-gradient-vanishing.md",
        "pagePath": "/content/learn_from_ai/2025-03-07-resnet-math-and-gradient-vanishing.md"
      },
      "meta": {}
    }
  ],
  "articles": [
    {
      "pathMapping": {
        "filePath": "public/content/articles/2020-01-27-Building-this-blog.md",
        "pagePath": "/articles/Building-this-blog",
        "slug": "Building-this-blog"
      },
      "meta": {
        "content": "\n> “Stop Trying to Reinvent the Wheel.”\n\n## 博客构建\n\n\n#### 把仓库clone到本地\n\n参考[BruceZhao][BruceZhao]提供的中文翻译：[README.zh.md][READMEzh]，先将[Huxpro][Huxpro]提供的[博客模板仓库][origin_repo]fork出来，`git clone`到本地。\n\n整个网站文件夹大致结构如下：\n\n```\n├── _config.yml\n|\n├── _posts/\n|   ├── 2007-10-29-awsome-file-name.md\n|   └── 2009-04-26-stupid-file-name.md\n├── img/\n|   ├── in-post/\n|   ├── awsome-bg.jpg\n|   ├── avatar-ryo.png\n|   ├── favicon.ico\n|   └── icon_wechat.jpg\n├── other_awsome_directory/\n|   └── awsomefiles\n|\n|\n├── 404.html\n├── about.html\n├── index.html\n└── other_awsome_files\n```\n\n博客的文章上传到`_posts`文件夹中，网站中用到的图片上传到`img`文件夹中，网站的全局设置在`_config.yml`中进行。\n\n\n\n#### 修改**_config.yml**文件\n\n修改根目录下的`_config.yml`文件，将其中的内容更改为自己的信息。\n\n```yml\n# Site settings\ntitle: Ryo's Blog\nSEOTitle: 阿亮仔的博客 | 亮のブログ | Ryo's Blog\nheader-img: img/home-bg.jpg\nemail: qq250707340@163.com\ndescription: \"君の夢が　叶うのは　誰かのおかげじゃないぜ。\"\nkeyword: \"Ryo, Blog, 阿亮仔, りょう, 博客, ブログ, Algorithm, Unity, Python, C-Sharp\"\nurl: \"http://RyoJerryYu.github.io\"              # your host, for absolute URL\nbaseurl: \"\"         # for example, '/blog' if your blog hosted on 'host/blog'\ngithub_repo: \"https://github.com/RyoJerryYu/RyoJerryYu.github.io.git\" # you code repository\n```\n- `SEOTitle`: `<title>`标签，即显示在浏览器标题中的文字。\n- `header-img`: 首页显示的图像，可以把路径更改为自己的图片。\n- `description`: `<meta name=\"description\">`中的内容。\n- `keyword`: `<meta name=\"keyword\">`中的内容。\n- `url`, `baseurl`: 分别为博客域名地址与其下路径。如不想将博客直接架在根路径下，需要对`baseurl`进行设置。\n- `github_repo`: 博客所在的GitHub仓库。\n\n---\n\n\n```yml\n# SNS settings\nRSS: false\n# weibo_username:     huxpro\n# zhihu_username:     huxpro\ngithub_username:    RyoJerryYu\ntwitter_username:  ryo_okami\n# facebook_username:  huxpro\n```\n分别为各个社交网站上的账号信息，以供在侧边栏中直接跳转到对应的页面。可通过在行首添加或删除`#`进行注释或取消注释。\n\n从[原仓库][origin_repo]中直接fork出来时，社交网站的图标可能会有[无法显示的问题](https://github.com/Huxpro/huxblog-boilerplate/issues/17)，其解决方法在[后面](#FixSNS)介绍。\n\n---\n\n\n```yml\n# Disqus settings\n#disqus_username: _your_disqus_short_name_\n\n# Duoshuo settings\n# duoshuo_username: huxblog\n# Share component is depend on Comment so we can NOT use share only.\n# duoshuo_share: true                     # set to false if you want to use Comment without Sharing\n\n# Gitalk\ngitalk:\n    enable: false    #是否开启Gitalk评论\n    clientID: f2c84e7629bb1446c1a4                            #生成的clientID\n    clientSecret: ca6d6139d1e1b8c43f8b2e19492ddcac8b322d0d    #生成的clientSecret\n    repo: qiubaiying.github.io    #仓库名称\n    owner: qiubaiying    #github用户名\n    admin: qiubaiying\n    distractionFreeMode: true #是否启用类似FB的阴影遮罩  \n```\n分别为各种评论系统。均未开启。\n\n---\n\n\n```yml\n# Analytics settings\n# Baidu Analytics\n# ba_track_id: 4cc1f2d8f3067386cc5cdb626a202900\n# Google Analytics\nga_track_id: 'UA-156933256-1'            # Format: UA-xxxxxx-xx\nga_domain: auto\n```\n分别为百度与谷歌的网站统计。我只启用了Google Analytics。可先到[Google Marketing Platform](https://marketingplatform.google.com/about/)注册，开启Google Analytics。在`设置`->`媒体资源设置`中获得Track ID，并填入`ga_track_id`中。\n\n---\n\n\n```yml\n# Sidebar settings\nsidebar: true                           # whether or not using Sidebar.\nsidebar-about-description: \"记录平时遇到的问题，以及对应的解决方法。偶尔上传些许宅活或是娱乐方面的记录。\"\nsidebar-avatar: /img/avatar-ryo.png      # use absolute URL, seeing it's used in both `/` and `/about/`\n```\n`sidebar`: 是否开启侧边栏，为`true`或`false`。\n`sidebar-about-description`: 显示在侧边栏中的个人简介。\n`sidebar-avatar`: 显示在侧边栏中的头像。\n\n---\n\n\n```yml\n# Featured Tags\nfeatured-tags: true                     # whether or not using Feature-Tags\nfeatured-condition-size: 2              # A tag will be featured if the size of it is more than this condition value\n```\n是否开启tag功能，以及最少要达到多少篇文章才能使tag显示在首页上。\n\n\n\n#### 修改主页等信息\n\n修改`index.html`、`404.html`、`about.html`、`tags.html`等文件，将其中的内容更改为自己的信息。\n\n- 在`index`中，修改`description`对应的内容，亦即主页中标题下方的描述。\n- 在`404`、`tags`、`about`中，修改`description`的内容，亦即404页面中的描述信息。如有需要，也可以修改`header-img`，即404页面的图片地址。\n- 在`about`中，还有修改自我介绍对应的内容。\n\n\n\n#### 修改图片信息\n\n修改`img/`下的图片，替换为自己的图片。要记得替换以下图片：\n- `avatar-ryo.png`\n- `favicon.ico`\n- `icon_wechat.png`\n\n\n\n#### 修改README.md\n\nREADME.md为Github仓库的介绍，可以在README.md中写上这个博客主要的内容，让别人了解这个博客。\n\n\n\n#### 完成\n\n将`_posts`中的博文全部删除后，将本地文件全部push到GitHub仓库中。稍等后用浏览器浏览`<用户名>.github.io`（或是你在`_config.yml`中设定的路径）。若发现网页已更新，即博客搭建成功，可以开始写博文了。\n\n*然而，并没有成功。*\n\n\n\n## Fix Bug\n\n<p id = \"FixReadmeCh\"></p>\n\n#### 修复README.zh.md引发的错误\n\n按上述步骤搭建完毕后，网页并没有正常显示。此时GitHub账号所关联的邮箱中收到标题为**Page build failure**的邮件，内容如下：\n> The page build failed for the `master` branch with the following error:\n> The tag `if` on line 235 in `README.zh.md` was not properly closed.\n\n如[原仓库][origin_repo]中的[issue#11](https://github.com/Huxpro/huxblog-boilerplate/issues/11)所示，在`README.zh.md`中存在`if`语句，会触发错误。\n\n因并无其他特别的需求，此处采用暴力删除`README.zh.md`的方法解决。\n\n对应commit：[删除README.zh.md，尝试修复因...](https://github.com/RyoJerryYu/RyoJerryYu.GitHub.io/commit/098d710160775df9b6d2cf04d7d4eec526a67bf4)\n\n\n<p id = \"FixSNS\"></p>\n\n#### 修复SNS链接不正常显示\n\n修复上述错误后，稍等即可正常打开网页。但是，我们在`_config.yml`中设置好的SNS链接并没有在侧边栏以及网页底部正常显示。如原仓库中的[issue#17](https://github.com/Huxpro/huxblog-boilerplate/issues/17)所示，原因是gitpage必须通过https访问bootcss.com等的cdn。\n\n此处采用原仓库[pull request#21](https://github.com/Huxpro/huxblog-boilerplate/pull/21)的方法，修改`_includes/head.html`, `_includes/footer.html`, `_layouts/keynote.html`, `_layouts/post.html`文件，将其中`http`修改为`https`。\n\n对应commit：[fix: change http into https](https://github.com/RyoJerryYu/RyoJerryYu.GitHub.io/commit/ec954c380472f30f09efdfadd074cb7967c2fa11)\n\n\n\n## 上传文章\n\n文章主要放在_posts文件夹中，用`git push`的方式推送到GitHub仓库，即可完成文章上传。\n\n文章正文以**markdown**语法书写，在文本头部增加如下格式的信息：\n\n```yaml\n---\nlayout:     post\ntitle:      \"Welcome to Ryo's Blog!\"\nsubtitle:   \" \\\"Hello World, Hello Blog\\\"\"\ndate:       2020-01-27 12:00:00\nauthor:     \"Ryo\"\nheader-img: \"img/post-bg-default.jpg\"\ntags:\n    - 杂技\n    - 杂谈\n---\n```\n\n其中：\n- `layout`为文章所用的模板，可选`post`或`keynote`，也可自己写一个模板html放在`_layouts`文件夹下。\n- `title`为文章标题，`subtitle`为文章副标题。\n- `date`为博客中显示的文章发表时间。\n- `author`为博客中显示的作者。\n- `header-img`为文章顶部显示的封面。\n- `tags`为文章的标签，我们的博客网站可以通过标签来快速寻找文章。\n\n把文章的文件名命名为时间+标题的形式，后缀名使用markdown文本的通用后缀名`md`，如`2020-01-27-hello-world.md`。完成后将此文本文件放到`_posts/`文件夹下。文章中使用到的图片建议放到`img/in-post/`文件夹下。\n\n完成后，使用`git push`推送到GitHub仓库，稍等后刷新博客网页即可看见刚才上传的文章。文章的url一般为：`<博客地址>/<文章文件名中的年>/<月>/<日>/<文件名中剩余部分>`。\n\n\n\n\n## 祝你开始愉快的博客生活。\n\n\n#### 感谢\n\n- [Huxpro][Huxpro]提供的博客模板：[huxblog-boilerplate][origin_repo]\n- [BruceZhao][BruceZhao]提供的中文翻译：[README.zh.md][READMEzh]\n- [Luo Yifan（罗一凡）](https://github.com/iVanlIsh)提供的Bug解决方案。\n\n\n\n\n[Huxpro]: https://github.com/huxpro\n[BruceZhao]: https://github.com/BruceZhaoR\n[origin_repo]: https://github.com/Huxpro/huxblog-boilerplate\n[READMEzh]: https://github.com/Huxpro/huxpro.github.io/blob/master/README.zh.md",
        "title": "搭建博客的过程",
        "abstract": "> “Stop Trying to Reinvent the Wheel.”\n参考[BruceZhao][BruceZhao]提供的中文翻译：[README.zh.md][READMEzh]，先将[Huxpro][Huxpro]提供的[博客模板仓库][origin_repo]fork出来，`git clone`到本地。\n整个网站文件夹大致结构如下：",
        "length": 252,
        "created_at": "2020-01-27T14:00:00.000Z",
        "updated_at": "2024-04-16T13:30:33.000Z",
        "tags": [
          "杂技",
          "Blog"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2020-01-27-hello-world.md",
        "pagePath": "/articles/hello-world",
        "slug": "hello-world"
      },
      "meta": {
        "content": "\n> “Hello World!”\n\n## 这是我的第一篇博文\n\n自己盲人摸象折腾了一两天，终于利用GitHub Pages，把自己的博客搭好了。\n\n感谢[Huxpro][Huxpro]提供的博客模板，以及[BruceZhao][BruceZhao]编写的中文ReadMe。\n\n这个博客的使用流程：\n- 写作时利用**Markdown**语法书写，与日常编写GitHub上的文档相同。\n- 使用**Git Workflow**进行博客的更新。\n- 利用**GitHub Pages**提供的域名与免费空间，以及其支持的**Jekyll**进行网站搭建。\n\n我以后会利用这个博客，记录些许编程中遇到的问题。同时还有记录一下生活娱乐上的琐事。\n\n这第一篇博文主要用于测试一下博客是否运行成功，不打算写太多东西。今后有时间的话会记录一下搭建博客的过程。\n\n\n#### 感谢\n\n- [Huxpro][Huxpro]提供的博客模板：[huxblog-boilerplate](https://github.com/Huxpro/huxblog-boilerplate)\n- [BruceZhao][BruceZhao]提供的中文翻译：[README.zh.md](https://github.com/Huxpro/huxpro.github.io/blob/master/README.zh.md)\n\n\n\n\n[Huxpro]: https://github.com/huxpro\n[BruceZhao]: https://github.com/BruceZhaoR",
        "title": "Welcome to Ryo's Blog!",
        "abstract": "> “Hello World!”\n自己盲人摸象折腾了一两天，终于利用GitHub Pages，把自己的博客搭好了。\n感谢[Huxpro][Huxpro]提供的博客模板，以及[BruceZhao][BruceZhao]编写的中文ReadMe。",
        "length": 29,
        "created_at": "2020-01-27T12:00:00.000Z",
        "updated_at": "2022-03-27T13:30:33.000Z",
        "tags": [
          "杂技",
          "杂谈"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2020-01-28-the-using-in-cpp.md",
        "pagePath": "/articles/the-using-in-cpp",
        "slug": "the-using-in-cpp"
      },
      "meta": {
        "content": "\n## using的用法\n#### using与命名空间\n\n1. 引入整个命名空间中的成员\n   \n   不引入命名空间时，使用其中变量需要使用`<命名空间名>::<变量名>`的方式使用。\n   ```C++\n   using namespace foo;\n   ```\n   如此会将命名空间foo下所有的成员名称引入，可在直接以 `<变量名>` 的形式使用。但如此做有可能会使得命名空间foo中部分变量与当前定义的变量名冲突，违反命名空间隔离编译时名称冲突的初衷，因此不建议如此使用。\n\n2. 引入命名空间中的部分成员\n   \n   可通过仅引入命名空间中部分的成员，避免命名冲突。\n   ```C++\n   using foo::bar;\n   ```\n   这种方法仅会引入在语句中明确声明的名称。如using一个枚举类时，不会连其定义的枚举常量也一同引入。\n\n#### using与基类成员\n\n1. 子类中引入基类名称\n   \n   ```C++\n    class Base {\n    public:\n        std::size_t size() const { return n; }\n    protected:\n        std::size_t n;\n    };\n\n    class Derived : private Base {\n    public:\n        using Base::size;\n    protected:\n        using Base::n;\n    // ...\n    };\n    ```\n    例中子类private继承基类，由于private继承使得`Base::size`与`Base::n`可视性变为private。而使用`using Base::size`、`using Base::n`后，可分别使其变为public与protected。\n\n2. 子类成员函数与基类同名时保留基类函数用以重载\n   \n   ```C++\n    class Base\n    {\n    public:\n        int Func(){return 0;}\n    };\n    class Derived : Base\n    {\n    public:\n        using Base::Func;\n        int Func(int);\n    };\n    ```\n    子类中定义的成员函数与基类中重名时，即使函数原型不同，子类函数也会覆盖基类函数。\n    \n    如果基类中定义了一个函数的多个重载，而子类中又重写或重定义了其中某些版本，或是定义了一个新的重载，则基类中该函数的所有重载均被隐藏。\n\n    此时可以在子类中使用`using Base::Func`，令基类中所有重载版本在子类中可见，再重定义需要更改的版本。\n\n又如cppreference中的[例子](https://en.cppreference.com/w/cpp/language/using_declaration#In_class_definition)：\n```C++\n#include <iostream>\nstruct B {\n    virtual void f(int) { std::cout << \"B::f\\n\"; }\n    void g(char)        { std::cout << \"B::g\\n\"; }\n    void h(int)         { std::cout << \"B::h\\n\"; }\nprotected:\n    int m; // B::m is protected\n    typedef int value_type;\n};\n\nstruct D : B {\n    using B::m; // D::m is public\n    using B::value_type; // D::value_type is public\n\n    using B::f;\n    void f(int) { std::cout << \"D::f\\n\"; } // D::f(int) overrides B::f(int)\n    using B::g;\n    void g(int) { std::cout << \"D::g\\n\"; } // both g(int) and g(char) are visible\n                                        // as members of D\n    using B::h;\n    void h(int) { std::cout << \"D::h\\n\"; } // D::h(int) hides B::h(int)\n};\n\nint main()\n{\n    D d;\n    B& b = d;\n\n//    b.m = 2; // error, B::m is protected\n    d.m = 1; // protected B::m is accessible as public D::m\n    b.f(1); // calls derived f()\n    d.f(1); // calls derived f()\n    d.g(1); // calls derived g(int)\n    d.g('a'); // calls base g(char)\n    b.h(1); // calls base h()\n    d.h(1); // calls derived h()\n}\n```\n`using`语句可以改变基类成员的可访问性，也能在子类中重载(Overload)、重写(Override)基类的函数，或是通过重定义隐藏(Hide)对应的基类函数。\n\n\n#### using与别名\n\nusing在C++11开始，可用于别名的声明。用法如下：\n```C++\nusing UPtrMapSS = std::unique_ptr<std::unordered_map<std::string, std::string>>;//普通别名\nusing FP = void (*) (int, const std::string&);//函数指针别名\n\ntemplate <typename T>\nusing Vec = MyVector<T, MyAlloc<T>>;//模板别名\nVec<int> vec;//模板别名的使用\n```\n\n## using关键字与typedef关键字定义别名的不同\n\n在STL容器或是其他泛型中若是再接受一个容器类型，类型名称就会写得很长。使用typedef或using定义别名会变得比较方便：\n```C++\ntypedef std::unique_ptr<std::unordered_map<std::string, std::string>> UPtrMapSS;\n\nusing UPtrMapSS = std::unique_ptr<std::unordered_map<std::string, std::string>>;\n```\n\n对于函数指针，使用using语句可以把函数原型与别名强制分到左右两边，比使用typedef易读得多：\n```C++\ntypedef void (*FP) (int, const std::string&);\n\nusing FP = void (*) (int, const std::string&);\n```\n\n---\n\n在C++中，若试图使用typedef定义一个模板：\n```C++\ntemplate <typename T>\ntypedef MyVector<T, MyAlloc<T>> Vec;\n\n// usage\nVec<int> vec;\n```\n编译就会报错，提示：\n> error: a typedef cannot be a template\n\n在一些STL中，通过如下方式包装一层来使用：\n```C++\ntemplate <typename T>\nstruct Vec\n{\n  typedef MyVector<T, MyAlloc<T>> type;\n};\n\n// usage\nVec<int>::type vec;\n```\n\n如此显得十分不美观，且要是在模板类中或参数传递时使用typename强制这为类型，而不是其他如静态成员等语法：\n```C++\ntemplate <typename T>\nclass Widget\n{\n  typename Vec<T>::type vec;\n};\n```\n\n而using关键字可定义模板别名，则一切都会显得十分自然：\n```C++\ntemplate <typename T>\nusing Vec = MyVector<T, MyAlloc<T>>;\n\n// usage\nVec<int> vec;\n\n// in a class template\ntemplate <typename T>\nclass Widget\n{\n  Vec<T> vec;\n};\n```\n\n---\n\n能做到类似别名功能的，还有宏#define。但#define运行在编译前的宏处理阶段，对代码进行字符串替换。没有类型检查或其他编译、链接阶段才能进行的检查，不具备安全性。在C++11中不提倡使用#define。\n\n\n\n   \n   ",
        "title": "C++中using关键字的使用",
        "abstract": "1. 引入整个命名空间中的成员\n   不引入命名空间时，使用其中变量需要使用`<命名空间名>::<变量名>`的方式使用。\n   ```C++",
        "length": 192,
        "created_at": "2020-01-28T18:00:00.000Z",
        "updated_at": "2022-03-27T13:30:33.000Z",
        "tags": [
          "C++",
          "杂技"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2020-08-02-python-dict.md",
        "pagePath": "/articles/python-dict",
        "slug": "python-dict"
      },
      "meta": {
        "content": "\n> CPython从3.6开始，字典（dict）不再是无序的了——字典的修改了原先的底层实现，变得能按字典插入的顺序进行遍历。而Python从3.7开始将字典的有序性写入语言特性，不管是Jython、IronPython还是其他Python实现，从3.7开始大家的字典都是有序的了。\n\n# 前言\n\n以前参加Python相关的面试时，面试官经常都会问一个问题：Python里的字典（dict）是有序的吗？\n\n这自然难不倒我，我也照本宣科地讲：Python的字典底层是用哈希表实现的，在不发生冲突时读写的时间复杂度是O（1），比读写时间复杂度为O（logn）的红黑树要更快。但红黑树可以按下标的大小顺序进行遍历，而Dict遍历时是无序的。\n\n我讲的时候没感觉到任何的违和感，估计面试官们也没觉得任何的不对。直到有一天，我查Python各个版本的新特性时，发现Python 3.6的What's New里有[这么一条](https://docs.python.org/3/whatsnew/3.6.html#new-dict-implementation)：\n\n> New dict implementation\n> \n> The dict type now uses a “compact” representation based on a proposal by Raymond Hettinger which was first implemented by PyPy. The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5.\n> \n> The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5).\n\n啥情况？CPython的dict竟然优化了内存，还变有序了！？\n\n# Python 3.5 以前dict的实现\n\n先不着急看Python 3.6 里的dict，我们先来看看Python 3.5之前的dict是怎么实现的，再拿3.6来做对比。\n\n在Python 3.5以前，dict是用Hash表来实现的，而且Key和Value直接储存在Hash表上。想通过Key获取Value，只需通过Python内部的Hash函数计算出Key对应的Hash值，再映射到Hash表上对应的地址，访问该地址即可获取Key对应的Value。如下图所示：\n\n我们知道，Hash表读写时间复杂度在不发生冲突的情况下都是O（1）。\n\n为什么呢？我们可以把Hash表读写的步骤分开来看：\n\n1. 首先用Hash函数计算key的Hash值，Hash函数一般来说时间复杂度都是O（1）的。\n2. 计算出Hash值后，映射到Hash表内的数组下标，一般用取余数或是取二进制后几位的方式实现，时间复杂度也是O（1）。\n3. 然后用数组下标读取数组中实际储存的键值，数组的下标读取时间复杂度也是O（1）。\n\n这三个步骤串起来后复杂度并没有提升，总的时间复杂度自然也是O（1）的。\n\n而内部储存空间，Python字典中称为entries。entries相当于一个数组，是一段连续的内存空间，每个位置储存一个（Hash值，指向Key的指针，指向Value的指针）三元组。\n\n当然，由于抽屉原理，我们知道Hash表不可避免的会出现Hash冲突，Python的dict也不例外。\n\n而解决Hash冲突的方法有很多，比如C++的unordered_map和Go的map就用链地址法来解决冲突，用链表储存发生冲突的值。而Java更进一步，当链表长度超过8时就转换成红黑树，将链表O（n）的查找复杂度降为O（logn）。C#的HashTable则是用再散列法，内部有多个Hash函数，一次冲突了就换一个函数再算，直到不冲突为止。\n\n而Python的dict则是利用开放寻址法。当插入数据发生冲突时，就会从那个位置往后找，直到找到有空位的地址为止。要查的时候，也是把下标值映射到到地址后，先对比一下下标值相不相等，若不相等则往后继续对比。\n\n这也造成个问题，dict中的元素不能直接从entries中清理掉，不然往后寻找的查找链就会断掉了。只能是先标记住删除，等到一定时机再一并清理。\n\n此外我们也知道，当冲突过发生得过多，dict读写所需的时间也会变多，时间复杂度不再是O(1)，这也是Hash表的通病了。\n\nPython中dict初始化时，内部储存空间entries容量为8。当内部储存空间占用到一定程度（entries容量×装填因子，Python的dict中装填因子是2/3）后，就会进行倍增扩容。每次扩容都要遍历原先的元素，时间复杂度为O(n)，但基本上插入O(n)次之后才会进行一次扩容，所以扩容的均摊时间复杂度为O(1)。而扩容时会重新进行Hash值到entries位置的映射，此时就是把标记删除但仍留在entries中的元素清理掉的最佳时机。\n\nPython3.5之前这种dict的实现就有两个毛病：\n\n1. 元素的顺序不被记录。两个Key值通过Hash函数的出来的Hash值不一定能保证原来的大小关系，由于Hash冲突、扩容等影响元素的顺序也会变化。当然这种无序性也是Hash表通用的特点了。\n2. 占用了太多了无用空间。上面说到entries中每个位置储存一个（Hash值，指向Key的指针，指向Value的指针）三元组，没用到或是标记删除的位置占用了大量的空间。\n\n于是，Raymond Hettinger就提出了一种新的dict实现方式。在CPython3.6中就使用了这种新的实现方式。\n\n# CPython3.6中dict的实现\n\n当要实现一个如下的dict时：\n\n```python\nd = {\n    'timmy': 'red', \n    'barry': 'green', \n    'guido': 'blue'\n}\n```\n\n如在上一节中所讲，在Python3.5以前，在内存储存的形式可以表示成这样子：\n\n```python\nentries = [['--', '--', '--'],\n           [-8522787127447073495, 'barry', 'green'],\n           ['--', '--', '--'],\n           ['--', '--', '--'],\n           ['--', '--', '--'],\n           [-9092791511155847987, 'timmy', 'red'],\n           ['--', '--', '--'],\n           [-6480567542315338377, 'guido', 'blue']]\n```\n\n而CPython3.6以后，是以这种形式储存在内存中的：\n\n```python\nindices =  [None, 1, None, None, None, 0, None, 2]\nentries =  [[-9092791511155847987, 'timmy', 'red'],\n            [-8522787127447073495, 'barry', 'green'],\n            [-6480567542315338377, 'guido', 'blue']]\n```\n\n改变了什么？\n\n1. dict内部的entries改为按插入顺序存储，新增了一个indices用于储存元素在entries中的下标。dict整体仍是Hash表结构，但Hash值映射到indices中，而不是直接映射到entries中。\n2. 由于entries改为了按插入顺序存储，使得申请entries容量时只要申请Hash表长度的2/3即可，省去了Hash表中的无用空间，储存更紧凑。\n3. dict读写步骤从原先的3步变为4步：计算key的Hash值，映射到indices内存空间，从indices读取entries的下标值，用下标从entries中读写数据。读写时间复杂度仍保持为O(1)，冲突、删除标记等Hash表的特性也仍然存在。indices的扩容策略也仍然是倍增扩容，但因为填充因子仍然为2/3，entries每次扩容时只需申请indices长度的2/3即可。\n\n有什么好处？\n\n1. 压缩空间：原先Hash映射是直接映射到entries上，会有大量的空隙。现在Hash映射到indices上，而entries中可更紧凑地存储元素。而indices中储存的entries下标占用内存可以比entries元素要小得多——当entries长度足够短时每个下标只需占一个字节。indices中确实也还仍有空隙，但占用空间总要比旧的dict实现要小得多了。\n2. 更快的遍历：以前的实现遍历dict要遍历整个Hash表，需要挨个位置读取一下，判断它是空闲位置还是实际存在的元素。而现在只需要对变得更紧凑的entries遍历就行了。这也带来一个新的特性：entries是按照元素插入的顺序存储的，遍历entries自然也会按元素插入的顺序输出。这就给dict带来了有序性。\n3. 扩容时关注的内存块更少。原先的entries扩容时所有数据都要重新映射到内存上，cache利用率不好。现在扩容时基本可以整个entries直接复制（当然，有删除标记的数据这时要忽略）。\n\n综上，CPython3.6以后通过增加了一个indices增加了空间利用率，在维持读写时间复杂度不变的情况下增加了遍历与扩容效率。至于dict遍历变得有序，倒是有点次要的特性了。\n\n# 我们是否应利用新dict的有序性？\n\n既然Python中dict变得有序了，那我们是否应该主动去利用它呢？我是这么认为的：\n\n1. 在Python3.6中，我们不推荐利用dict的有序性。3.6时dict的有序性还只是CPython的一个实现细节，并不是Python的语言特性。当我们的代码不是在CPython环境下运行，dict的有序性就不起作用，就容易出莫名其妙的BUG了。\n2. 在Python3.7后，dict按插入顺序进行遍历的性质被写入Python语言特性中。这时确实在代码中利用dict有序性也没什么大问题。但dict这种数据结构，最主要的特性还是表现在Key映射到Value的这种关系，以及O(1)的读写时间复杂度。当我们的代码中需要关注到dict的遍历顺序时，我们就要先质问一下自己：是否应该改为用队列或是其他数据结构来实现？\n\n\n# 参考文献\n\n- [Are dictionaries ordered in Python 3.6+?](https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6)\n- [[Python-Dev] Python 3.6 dict becomes compact and gets a private version; and keywords become ordered](https://mail.python.org/pipermail/python-dev/2016-September/146327.html)\n- [[Python-Dev] More compact dictionaries with faster iteration](https://mail.python.org/pipermail/python-dev/2012-December/123028.html)\n- [关于python3.6中dict如何保证有序](https://zhuanlan.zhihu.com/p/36167600)\n- [python3.7源码分析－字典_小屋子大侠的博客-CSDN博客_python 字典源码](https://blog.csdn.net/qq_33339479/article/details/90446988)\n- [《深度剖析CPython解释器》9. 解密Python中字典和集合的底层实现，深度分析哈希表](https://www.cnblogs.com/traditional/p/13503114.html)\n- [CPython 源码阅读 - dict](http://blog.dreamfever.me/2018/03/12/cpython-yuan-ma-yue-du-dict/)",
        "title": "Python字典的实现原理",
        "abstract": "> CPython从3.6开始，字典（dict）不再是无序的了——字典的修改了原先的底层实现，变得能按字典插入的顺序进行遍历。而Python从3.7开始将字典的有序性写入语言特性，不管是Jython、IronPython还是其他Python实现，从3.7开始大家的字典都是有序的了。\n以前参加Python相关的面试时，面试官经常都会问一个问题：Python里的字典（dict）是有序的吗？\n这自然难不倒我，我也照本宣科地讲：Python的字典底层是用哈希表实现的，在不发生冲突时读写的时间复杂度是O（1），比读写时间复杂度为O（logn）的红黑树要更快。但红黑树可以按下标的大小顺序进行遍历，而Dict遍历时是无序的。",
        "length": 121,
        "created_at": "2020-08-02T00:10:10.000Z",
        "updated_at": "2022-03-27T13:30:33.000Z",
        "tags": [
          "Python",
          "数据结构"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2021-01-11-Sort-algorithm.md",
        "pagePath": "/articles/Sort-algorithm",
        "slug": "Sort-algorithm"
      },
      "meta": {
        "content": "\n# 序言\n\n我们知道排序是算法入门基本功，排序算法有多重要想必也不需要我在这里说明了。因此这一篇就按着我的理解，聊一聊排序算法。\n\n当然我不打算随便弄个什么十大排序算法或是经典排序总结之类响当当的名头，各个算法走马看花一样拉出来遛一遍，最后变得跟网上搜索到的其他讲排序的文章一样换汤不换药。你会发现这篇文章的结构跟在网上搜索到的任何讲排序的文章都有所不同：\n\n在这篇文章里，你会发现你找不到冒泡排序——因为我认为冒泡排序只不过是一种低效率的选择排序。\n\n你会发现堆排序被当成是选择排序的一种优化——因为我认为堆排序主要在于使用了堆这种数据结构，而总体思想与选择排序相比没有太大变化。\n\n你还会找到其他与别的文章不一样的地方。因为这篇文章是我按照自己的理解来写的，我脑子里是这样想的，那文章里就是这样写的。我会按照我的理解，从纵向与横向两个维度，来理清楚各个排序算法的特性与异同。\n\n# 整篇文章的要点\n\n整篇文章以纵向——算法分类、以及横向——算法评价两个维度来进行组织。\n\n排序算法可以按照以下方式来进行分类：\n\n- 基于比较的排序算法\n    - 基于分治思想\n        - 快速排序\n        - 归并排序\n    - 基于有序区域扩展\n        - 插入排序\n        - 选择排序\n- 不基于比较的排序算法\n    - 计数排序\n    - 桶排序\n    - 基数排序\n\n文章中还会讲一讲为什么会这么分，每种分类有什么共性，分类之间有什么差异。此外，在最后还会稍微提一提外部排序与适用于并行运算的排序等。\n\n而对于纵向分类中的每一个端点，我们又会从以下五个方面，来对各个算法进行一个总体评价：\n\n- 时间复杂度（最坏，最好，平均※）\n- 空间复杂度\n- 是否原地排序\n- 是否稳定排序\n- 能否用于链表排序\n\n而由于复杂度主要只关注数量级，因此在这篇文章里会在不影响计算结果的前提下对复杂度计算进行适当的近似与简化。\n\n# 快排\n\n## 思想\n\n分治法：先把序列分为小的部分和大的部分，再将两部分分别排序。即：复杂分割，简单合并，主要操作在于分割。\n\n## 要点\n\n### 时间复杂度\n\n推导式：T(n) = T(找) + T(左) + T(右) = O(n) + 两个子问题时间复杂度。\n\n- 最好时间复杂度为每次都正好找到最中间的一个数时时间复杂度为O（nlogn）。证明略。\n- 最坏时间复杂度为每次都正好找到最旁边的数时时间复杂度为O(n^2)。证明略。\n- 平均时间复杂度为O（nlogn），推导式如下：\n\n    快速排序每一步中，将元素分为左右两边需要遍历整个列表，耗时T(n)。假设最后定位的元素为最终第i个元素，则两个子问题复杂度分别为T(i)和T(n-i-1)。\n\n    则有：\n\n    $$\n    \\begin{aligned}\n    T(n) &= n + \\frac{\\sum_{i=0}^{n-1}{T(i)+T(n-i-1)}}{n} \\\\\n    &=n + \\frac{2}{n}\\times\\sum_{i=0}^{n-1}{T(i)} \\\\\n    \\end{aligned}\n    $$\n\n    令 $$\\sum_{i=0}^{n}T(i) = Sum(n)$$ ，即有：\n\n    $$\n    \\begin{aligned}\n    T(n) &= n + \\frac{2}{n} \\times Sum(n-1) \\\\\n    Sum(n) &= \\frac{n+1}{2}T(n+1) - \\frac{n+1}{2}(n+1)\n    \\end{aligned}\n    $$\n\n    错位相减：\n\n    $$\n    \\begin{aligned}\n    Sum(n) - Sum(n-1) &= \\frac{n+1}{2}T(n+1) - \\frac{n+1}{2}(n+1) - \\frac{n}{2}T(n) + \\frac{n}{2}(n) \n    \\\\\n    T(n) &= \\frac{n+1}{2}T(n+1) - \\frac{n}{2}T(n) - \\frac{2n+1}{2} \n    \\\\\n    \\frac{T(n+1)}{n+2} &= \\frac{T(n)}{n+1}+\\frac{2n+1}{(n+1)(n+2)} \n    \\\\\n    &= \\frac{T(n)}{n+1}+\\frac{1}{n}+\\frac{1}{n+1} \n    \\\\\n    &=...\n    \\\\\n    &= \\frac{T(1)}{2}+1+2\\times(\\frac{1}{2}+\\frac{1}{3}+...+\\frac{1}{n})+\\frac{1}{n+1}\n    \\\\\n    \\\\\n    \\frac{T(n)}{n+1}&=\\frac{T(1)}{2}+1+2\\times(\\frac{1}{2}+...+\\frac{1}{n-1})+\\frac{1}{n}\n    \\\\\n    &= O(1)+O(1)+O(logn)+O(\\frac{1}{n})\n    \\\\\n    &= O(logn)\n    \\end{aligned}\n    $$\n\n    其中由于 $$\\frac{1}{x}=\\frac{d(logx)}{dx}$$ ，因此 $$\\frac{1}{2}+...+\\frac{1}{n-1}=O(logn)$$ 。\n\n    则有：\n\n    $$\n    \\begin{aligned}\n    T(n)&=(n+1)\\times O(logn)\\\\\n    &=O(n)\\times O(logn)\\\\\n    &=O(nlogn)\n    \\end{aligned}\n    $$\n\n### 额外空间复杂度\n\n考虑栈深度，额外空间复杂度为O（logn）。由于快速排序主要步骤在于分，因此必须自上而下的进行递归，无法避免栈深度。\n\n### 原地排序\n\n虽然快速排序有额外空间复杂度，但并不妨碍它是一个原地排序。\n\n### 不稳定\n\n堆排序在分操作时将元素左右交换，会破坏稳定性。\n\n### 链表形式特点\n\n- 时间复杂度不变\n- 空间复杂度不变\n- 变为稳定排序※\n\n## 手写时的易错点\n\n- 分成左右子序列时最好完全分开（一边用`<=`一边用`>`），不然容易造成死循环。\n- 分左右子序列时仔细考虑最初下标位置与最终下标位置，以及对应位置的值的大小。\n- 不要忘记递归的结束条件。\n\n# 归并排序\n\n## 思想\n\n分治法：先把两个子序列各自排好序，然后再合并两个子序列。即：简单分割，复杂合并。主要步骤在于合并。\n\n## 要点\n\n- 时间复杂度推导式：T(n) = 2T(n/2) + T(合)\n- 平均、最好、最坏时间复杂度都是O（nlogn），推导过程略。\n- 额外空间复杂度为O（n），合并时必须准备额外空间。但由于主要步骤在于合并，可以自下而上地进行迭代合并，可以不使用栈。\n- 非原地排序\n- 稳定排序\n\n### 链表形式\n\n- 时间复杂度不变\n- 额外空间复杂度变为O(1)※\n- 稳定排序\n- 只能使用迭代形式，不能使用递归形式\n\n## 易错点\n\n- 合并时一边结束时另一边还未结束，需要把那一边也放入合并后序列中\n- 保持稳定排序：合并时左序列等于右序列时也采用左序列\n- 不要忘记递归结束条件\n- 不要忘记循环递进条件\n\n## 进阶\n\n- 原地归并排序：时间复杂度为O（log^2n），牺牲合并的时间复杂度进行原地排序。\n- 多路归并排序：使用竞标树，多路归并，用于磁盘IO。\n\n# 插入排序\n\n## 思想\n\n有序序列不断扩张，每次从无序序列中取出元素加入有序序列，直至长度为N则完成排序。\n\n每次将无序序列当前元素插入有序序列，复杂度取决于有序序列。\n\n## 要点\n\n- 最坏、平均时间复杂度：O（n2）\n- 最好时间复杂度：基本有序情况下，O（n）\n- 额外空间复杂度：O（1）\n- 原地排序\n- 稳定排序\n- 每次插入时都要移动序列，写次数较多\n- 若查找插入位置时使用二分法查找，则可加快时间。（但不足以对时间复杂度造成影响，且最好时间复杂度也会上升为O(nlogn)）\n\n### 链表形式\n\n- 最好，最坏，平均时间复杂度不变\n- 额外空间复杂度不变\n- 稳定排序\n- 插入时不再需要移动序列，但也不能使用二分法查找\n\n## 进阶——希尔排序\n\n- 要点：\n\n    插入排序的优点在于当序列基本有序时，时间复杂度可逼近为O(n)。\n\n    但插入时移动有序序列中元素所耗时间较多，而每次只移动一步。但实际上当序列分布均匀时，有序序列中排靠后的元素在整个序列中也会排靠后。\n\n    可以把序列分为几个大步长序列，在最初的几次插入放开移动步长，让大的元素直接移动到较后位置。再往后慢慢缩小步长，此时序列基本有序，可以利用基本有序时插入排序的优势。\n\n- 时间复杂度\n    1. 当步长为2^i时，不能使时间复杂度缩短为O(nlogn)。因为一个子序列所有元素有可能比另一个子序列最大元素都要大，这时插入排序仍需进行约n^2次操作\n    2. 当步长为2^i时效率较低，因为当步长为4已经有序时，步长为2再比较是无用比较。但由于1.的问题，不能节省比较时间。\n    3. 当步长之间最小公约数较少，甚至互质时，无用比较次数会降低。\n    4. 最坏时间复杂度下限为 $$O(nlog^2n)$$ （当步长采用 $$2^i3^j$$ 时），但一般希尔排序平均时间复杂度都为 $$O(n^{\\frac{3}{2}})$$\n- 额外空间复杂度O(1)\n- 原地排序\n- 不稳定排序：希尔排序步长较大时会发生前后跳转。\n- 不能写为链表形式\n\n# 选择排序\n\n## 思想\n\n有序序列不断扩张，每次从无序序列中取出元素加入有序序列，直至长度为N则完成排序。\n\n每次将选无序序列中最小元素加到有序序列末尾，复杂度取决于无序序列。\n\n## 要点\n\n- 最坏、平均时间复杂度：O（n2）\n- 最好时间复杂度：O（n2），如能保证无序部分的最小元素所在位置一定（堆排序），能降低时间复杂度\n- 额外空间复杂度：O（1）\n- 原地排序\n- 不稳定排序（采用元素交换策略时）\n- 每次找到最小元素后，只需交换一次位置即可，写次数较少。\n- 若找到最小元素后，不直接交换而是进行数组移动，则可进行稳定排序，但写次数变多，与插入排序相比没有优势，也不能使用二分查找进行简化。\n\n### 链表形式\n\n- 最好，最坏，平均时间复杂度不变\n- 额外空间复杂度不变\n- 变为稳定排序※（因为链表不需要数组移动，稳定排序方式的缺点得以消除）\n\n## 进阶——堆排序\n\n- 要点：\n\n    选择排序中耗时最多的是取出无序序列中最小值的时间，需要遍历整个无序序列。\n\n    但实际上我们只关心无序序列中的最小值，而不关心其他值的位置。通过将无序序列建为堆，减少选择时间，降低总的时间复杂度。\n\n- 时间复杂度O（nlogn）\n- 额外空间复杂度O（1）\n- 原地排序\n- 不稳定排序\n- 操作时间复杂度：每次向下比较关注一个节点与其左右子堆顶元素，每次向上比较只关注节点与其父元素（大顶堆，堆大小为n）\n    - 下沉：向下比较，若顶元素不是最大，将顶元素与较大的子堆堆顶元素交换。递归处理该子堆顶元素，直到向下比较顶元素最大。\n\n        最好时间复杂度O（1），最坏时间复杂度为O（h）=O（logn），平均O（logn）\n\n    - 上浮：向上比较，若元素比其上层要大，交换该元素与其上层元素。递归处理其上层元素，直到向上比较不比上层要大。\n\n        最好时间复杂度O（1），最坏时间复杂度O（h）=O（logn），平均O（logn）\n\n    - 入堆：堆扩容一位，将新元素插到尾部，将该元素上浮，最坏、平均时间复杂度O（logn）\n    - 出堆：取出堆顶元素，将尾部放到堆顶，将该元素下沉，最坏、平均时间复杂度O（logn）\n    - 缺点：通常堆尾元素较小，出堆时将堆尾元素放到堆顶再下沉基本要沉到堆底，无用比较较多\n- 建堆时间复杂度O（n）\n    - 策略1：从头开始建堆，逐个元素插入，时间复杂度取决于最后一层，时间复杂度为O（nlogn）\n        - 每次将堆扩容一位，将末尾元素上浮。\n        - 时间复杂度推导：\n\n            每次插入时间：\n\n            $$\n            \\begin{aligned} T(i) &= h\\\\\n            &= logi\n            \\end{aligned}\n            $$\n\n            则有总时间：\n\n            $$\n            \\begin{aligned}\n            T(n) &= \\sum_{i=0}^{n}logi\\\\\n            &= 1\\times1 + 2\\times2+ 3\\times4 + ...+h\\times \\frac{n}{2} \\\\\n            \\\\\n            2T(n) &= 1\\times2 + 2\\times4+ 3\\times8 + ...+h\\times n \\\\\n            \\\\\n            2T(n)-T(n) &= h\\times n - (1+2+4+...+2^{h-1}) \\\\\n            &= h\\times n - O(2^h) \\\\\n            \\\\\n            T(n) &= nlogn - O(2^h) \\\\\n            &= O(nlogn)\n            \\end{aligned}\n            $$\n\n            即时间复杂度为 $$O(nlogn)$$\n\n    - 策略2：从后开始建堆，小堆合并（逐个元素下沉），时间复杂度为O（n）\n        - 每次堆合并时，有三部分：左子堆，右子堆，顶元素。下沉顶元素。\n        - 时间复杂度推导：\n\n            第i个元素合并时，时间为：\n\n            $$\n            \\begin{aligned}\n            T(i) &= h_{子堆}\n            \\end{aligned}\n            $$\n\n            则有总时间：\n\n            $$\n            \\begin{aligned}\n            T(n) &= \\sum_{i=0}^{n} (h_{子堆}) \\\\\n            &= h + (h-1)\\times2 + ... + 2 \\times \\frac{n}{4} + 1 \\times\\frac{n}{2}\\\\\n            \\\\\n            2T(n) &= h \\times 2 + (h-1) \\times 4 + ... + 2 \\times \\frac{n}{2} + n \\\\\n            \\\\\n            2T(n) - T(n) &= n + \\frac{n}{2} + ... + 2 - h \\\\\n            \\\\\n            T(n) &= O(n) - h \\\\ \n            &= O(n)\n            \\end{aligned}\n            $$\n\n            即时间复杂度为 $$O(n)$$ \n\n        - 虽说每步是做一个小堆合并，但实际上从堆尾到堆头遍历，相当于仅关注元素没有稳定，相当于可以直接使用下沉操作。\n\n# 基于比较的排序算法时间复杂度下限：逆序对思想\n\n基于比较的排序算法可以看作序列逆序对的消除。完全随机序列逆序对数量为O(n^2)，若一次元操作只消除一个逆序对，则时间复杂度不会低于O(n^2)。降低时间复杂度关键在于一次消除多个逆序对。\n\n1. 希尔排序通过增大最初的步长来企图一次消除多个逆序对。\n2. 归并排序消除逆序对最主要在于归并步骤。最后几次合并每个子步骤用O(1)时间消除O(n)个逆序对。\n3. 快速排序消除逆序对最主要在于划分步骤。每个划分步骤用O(n)时间消除O(n)+O(左长度×右长度)个逆序对。\n4. 堆排序逆序对消除方式比较Tricky，但可以看出消除逆序对大致在于出堆步骤，通过O(logn)时间复杂度消除O(n)个逆序对。（左小右大排序时需要建立左大右小的大顶堆，建堆时基本没有消除逆序对）\n\n# 最后\n\n这篇文章我们主要关注了排序算法中的大头——基于比较的排序算法。在下篇文章，我们再来看一下不基于比较的排序算法，以及外排序与并行排序。",
        "title": "排序算法",
        "abstract": "我们知道排序是算法入门基本功，排序算法有多重要想必也不需要我在这里说明了。因此这一篇就按着我的理解，聊一聊排序算法。\n当然我不打算随便弄个什么十大排序算法或是经典排序总结之类响当当的名头，各个算法走马看花一样拉出来遛一遍，最后变得跟网上搜索到的其他讲排序的文章一样换汤不换药。你会发现这篇文章的结构跟在网上搜索到的任何讲排序的文章都有所不同：\n在这篇文章里，你会发现你找不到冒泡排序——因为我认为冒泡排序只不过是一种低效率的选择排序。",
        "length": 342,
        "created_at": "2021-01-11T22:57:10.000Z",
        "updated_at": "2024-04-14T13:30:33.000Z",
        "tags": [
          "数据结构",
          "算法",
          "排序"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2021-03-21-Handy-heap-cheat-sheet.md",
        "pagePath": "/articles/Handy-heap-cheat-sheet",
        "slug": "Handy-heap-cheat-sheet"
      },
      "meta": {
        "content": "\n# 如何手撕一个堆\n\n# 写在前面\n\n在参加如AtCoder等算法竞技，或是刷Leetcode等算法题时，我们总是不可避免地遇到堆这种数据结构。\n\n当然，一般来说我们只要理解堆，知道堆的性质，知道怎么样用堆就足够了。在做题时只需要调用系统类库即可——在参加AtCoder时你甚至不会有时间去自己实现一个堆。\n\n但是，如果哪一天你把编程语言的类库全忘光了，又遇到一题需要频繁求最值的题目——你明知这里要用堆，却又忘记该调用的类名了，咋办？我还真遇到过这问题：三年没刷算法，只能对着一道自己明显会的题干着急，愣是想不起PriorityQueue的名字。这时候，只能自己实现一个堆出来了。\n\n# 首先要理解，然后才能实现\n\n就像人总不会忘记自行车怎么骑一样，只要理解了数据结构的原理，身体就会自动来帮我们记忆，总不会忘。那要怎么理解一个堆呢？\n\n## 先抓住重点：堆是一种树结构\n\n首先最重要的，要理解堆是一种树结构。不管实际是基于数组实现还是别的什么实现，逻辑结构是树结构没变的。\n\n再进一步，在堆这种树结构中，最重要的约束就是：**对于树中的每个节点，总有父节点大于两个子节点**（以大顶堆为例，下同）。\n\n如此一来，大小关系在树中层层传递，最终可得树的根节点（堆顶）就是整个堆的最大节点，读取堆中最大值的时间复杂度为O(1)。而我们使用堆也一般是为了利用这种堆顶元素就是最大值的特点，读取、删除操作一般会限制为只允许读取、删除堆顶元素。\n\n而且我们可以注意到，与二叉查找树比起来，堆的约束十分之弱：堆只约束父节点与子节点的大小关系，而不需要管左右子树的大小关系，甚至不需要管左右两个子节点之间谁大谁小。这样一来堆就有很多很好的性质了：\n\n1. 堆并不关注左右子树之间的大小情况，那么**要维护一个堆，基本只需要做交换父节点与子节点的操作**，而不需要像二叉查找树那样做各种旋转操作。\n2. 因为维护一个堆不需要做旋转操作，那么几乎不需要花任何代价，就可以把堆的树结构维持在完全二叉树状态。因此堆的物理结构可以设计得很紧凑，**可以使用数组进行实现**。\n3. 因为堆可以维持在完全二叉树状态，那么堆的树结构的高度就可以控制为O(logn)范围内。而如上所述，要维护一个堆我们不需要关注左右子树的关系。因此我们要在堆上做增删操作，都只需要上下交换若干次父子节点。而交换次数最多时，也只是从树根一直交换到树叶，或是从树叶一直交换到树根，最多交换logn次。那么我们可得：**堆的增删操作最坏时间复杂度为O(logn)**。\n\n## 再抓基本操作：上浮与下沉\n\n上面也提到，要维护一个堆，我们只需要上下交换若干次父子节点即可。若一个节点**过大**，就跟他的父节点**向上交换**；若一个节点**过小**，就跟他的子节点**向下交换**。\n\n假设p节点过大破坏了堆结构，即p节点比其父节点g还要大，向上交换如下图：\n\n![p与g交换](/img/in-post/2021-03-21-Handly-heap-cheat-sheet/change.png)\n\n由于除了p过大破坏堆结构以外，其他节点都符合堆结构，则有：\n\n1. p > g > p2\n2. g > 原p > c1与c2\n\n则向上交换后有只有一种破坏堆结构的可能性：p节点过大，比gg节点还要大。而解决方法也很简单，就是递归地进行向上交换，最坏情况下一直交换到堆根节点为止。\n\n同理可得，p节点过小，小于他的子节点时，向下交换后有可能需要递归地向下交换，最坏情况下一直交换到叶子节点为止。要注意向下交换时需要先比较一下两个子节点的大小，再跟较大的子节点交换，才能交换后的大小关系符合堆的要求。\n\n为了简化，我们把前面那种递归地向上交换称为**上浮操作**，把后面这种递归地向下交换称为**下沉操作**。所有需要维护堆结构的操作：增、删、建堆，都可以拆分为上浮操作或是下沉操作的组合。\n\n# 各种接口的逻辑\n\n## 插入元素——入堆\n\n把一个元素p加入堆中，我们可以先把p加到堆尾，然后对p做上浮操作。\n\n虽然堆是一个树结构，但由于堆可以用数组实现，那我们只要用O(1)的时间就可以找到堆尾。而如上面所述上浮操作最多交换到根节点 。由于用数组实现的堆是完全二叉树，交换到根节点时间复杂度为O(logn)。因此我们可得入堆的最坏时间复杂度为O(logn)。\n\n## 删除堆顶元素——出堆\n\n我们从堆中删除元素时，一般只会删除堆顶元素。\n\n删除堆顶元素时，我们可以摘出堆尾元素p填到堆顶的空缺中，再对p做下沉操作。找到堆尾元素需要O(1)时间，下沉操作最多交换到叶子节点，时间复杂度为O(logn)。因此出堆最坏时间复杂度为O(logn)。\n\n这里加点餐：出堆时把堆尾元素p放到堆顶后下沉，而p原先在堆中的最下层，一般在整个堆中都算较小的元素。因此下沉p时有较大概率需要一直把p下沉到最下层或是倒数第二层，即出堆时最坏情况出现概率较高。\n\n## 堆的初始化——建堆\n\n建立一个堆，我们有两种思路：\n\n1. 将元素一个一个插入，即对每个元素都做一次入堆操作。\n2. 当节点p左子树和右子树都各自为一个堆时，只要把p下沉就可以把左右两个堆合并成一个更大的堆。即不断地进行堆合并操作。\n\n下面我们来分析这两种建堆策略。\n\n### 元素逐个入堆\n\n上面说到，入堆就是把元素加到堆尾，再做上浮操作。把元素逐个入堆，就是把元素逐个上浮。\n\n插入第i个元素时，堆的大小为$i$（在不影响计算情况下的近似，下同），则有堆的高度为，则上浮时间复杂度为：\n\n$$T(i) = logi$$\n\n那么把所有元素上浮，则总时间复杂度为：\n\n$$\n\\begin{aligned}\nT(n) &= \\sum_{i=1}^{n}logi\\\\\n&= 1\\times0 + 2\\times1 + ... + 2^{logn}\\times{logn} \\\\\n&=O(nlogn)\n\\end{aligned}\n$$\n\n通过把元素逐个入堆来建堆时，元素的时间复杂度可以用下图直观显示：\n\n![](/img/in-post/2021-03-21-Handly-heap-cheat-sheet/insert-length.png)\n\n（每条红线的长度就是插入该元素所需的时间，红线的总长度就是建堆所需的总时间复杂度）\n\n### 堆合并\n\n我们就可以从树结构的最底层出发不断进行堆合并，小堆合并成大堆，最后合并到根节点就建成整个堆结构。\n\n当节点的左右两个子树都是堆时，只需要对该节点进行下沉操作就可以合并左右两个堆。 不断进行堆合并，就是从下层开始把元素逐个下沉。\n\n下沉第i个元素（从顶到底数）时，以其为顶点的树高度约为$logn-logi$，则有下沉时间复杂度为：\n\n$$\nT(i) = logn-logi\n$$\n\n那么把所有元素下沉，则总时间复杂度为：\n\n$$\n\\begin{aligned}\nT(n) &= \\sum_{i=1}^{n}logn-logi \\\\\n&= \\frac{n}{2^{logn}}\\times{logn}+ ... + \\frac{n}{4}\\times2+\\frac{n}{2}\\times1 \\\\\n&= O(n)\n\\end{aligned}\n$$\n\n同样的，我们也可以把逐个元素下沉所耗费的时间用下图来示意：\n\n![](/img/in-post/2021-03-21-Handly-heap-cheat-sheet/merge-length.png)\n\n### 两种策略的比较与理解\n\n逐个元素入堆的策略时间复杂度为$O(logn)$，堆合并策略的时间复杂度为$O(n)$，为什么会出现差异呢？我们可以从两个角度来理解：\n\n1. 从元素移动路径的角度\n\n    我们从前一小节的两幅图中可发现，元素入堆策略的图中根节点附近红线十分密集。而堆合并策略的红线则整体来说比较稀疏。\n\n    这说明元素入堆策略中，在根节点附近元素做了较多重复无效的移动——也就是说插入一个元素时上浮到了根节点附近，然后又被其他后来的元素顶替下来。一上一下自然消耗了多余的时间，而这种消耗在元素入堆策略中出现频率高，无可忽视。\n\n2. 从元素移动数量与移动距离的角度\n\n    我们知道一般来说树的越下层节点数量越多。特别是用数组实现的堆是个完全二叉树，最下层节点数量占了总数的一半。 因此**建堆的时间复杂度主要取决于底层元素**的移动距离。\n\n    用元素入堆策略需要每个元素进行上浮操作，而偏偏元素数量最多的底层移动距离最长，$O(n)$个元素需要移动$O(logn)$的距离，因此时间复杂度较高。\n\n    而堆合并策略则反过来，需要每个元素进行下沉操作。移动距离最长的只有一个根元素，底层元素几乎不需要移动，因此时间复杂度加起来只有$O(n)$。\n\n    如图所示，颜色越深代表移动距离越长。颜色深度对面积的积分即为建堆时间复杂度。\n\n    ![](/img/in-post/2021-03-21-Handly-heap-cheat-sheet/move-length.png)\n\n综上分析我们可以得出，通过堆合并策略建堆较优，时间复杂度只需$O(n)$。因此我们建堆一般采用堆合并策略，从下往上逐个元素下沉。\n\n# 代码实现\n\n其实理解了上面这些，要写一个堆出来也已经是水到渠成了。但正如Linus所说，Talk is cheap, show me the code。我们还是要亲手写一段，才能知道堆到底长啥样。\n\n```python\nT = TypeVar(\"T\")\nclass Heap(Generic[T]):\n    '''堆结构\n\n    有两个成员：\n    self.A: List[T] # 堆内元素集合，元素类型为T，储存为数组\n    self.fCompare: Callable[[T,T],bool] # 比较函数\n    \n    下面假设堆为大顶堆\n    即有self.fCompare = lambda a,b: a>b\n    '''\n```\n\n## 实现树结构\n\n堆可以实现为基于数组的完全二叉树，以下标为零的节点为树根节点。\n\n对于下标为i的节点，其左子节点、右子节点、父节点的下标分别如下所示：\n\n```python\ndef lfChildOf(i:int):\n    return (i + 1) << 1 - 1\n\ndef rtChildOf(i:int):\n    return (i + 1) << 1\n\ndef parentOf(i:int):\n    return (i - 1) >> 1\n```\n\n至于为什么是这样，是因为完全二叉树与数组的对应规则如下图所示。这三个函数也没必要记住，到时候纸上画一画就记起来了。\n\n![](/img/in-post/2021-03-21-Handly-heap-cheat-sheet/tree-struct-function.png)\n\n## 实现基本操作——上浮与下沉\n\n### 上浮\n\n上浮就是递归地进行向上交换，下沉就是递归地进行向下交换。\n\n```python\ndef floatUp(self, i:int):\n    '''上浮操作\n\n    对下标为i的元素递归地进行上浮操作\n    直到该元素小于其父节点或该元素上浮到根节点\n    '''\n    # 元素i上浮到根节点时结束递归\n    if i <= 0:\n        return\n    \n    # 当元素i小于其父节点时符合堆结构，结束递归\n    pr = parentOf(i)\n    if self.fCompare(self.A[pr], self.A[i]):\n        return\n    \n    # 元素i大于其父节点，交换i与其父节点并继续上浮\n    self.A[pr], self.A[i] = self.A[i], self.A[pr]\n    self.floatUp(pr)\n```\n\n### 下沉\n\n而下沉要稍微比上浮复杂。向下交换时，需要先找出较大的子节点，再跟较大的子节点进行交互。还要考虑左右子节点不存在的情况：当子节点下标超出堆大小时，子节点不存在。\n\n```python\ndef size(self):\n    '''返回堆大小\n    '''\n    return len(self.A)\n\ndef sinkDown(self, i:int):\n    '''下沉操作\n\n    对下标为i的元素递归地进行下沉操作\n    直到该元素大于其两个子节点或该元素下沉到叶子节点\n    '''\n    lc = lfChildOf(i)\n    rc = rtChildOf(i)\n\n    # 比较元素i与其两个子节点，获取三个元素中存在且最大的元素\n    larger = i\n    if lc < self.size() and self.fCompare(self.A[lc], self.A[larger]):\n        larger = lc\n    if rc < self.size() and self.fCompare(self.A[rc], self.A[larger]):\n        larger = rc\n    \n    # 当元素i大于其两个子节点时符合堆结构，结束递归\n    # 当元素i下沉到叶子节点时，左右子节点不存在，也会在此结束递归\n    if larger == i:\n        return\n    \n    # 元素i小于其中一个子节点，交换i与较大子节点并继续下沉\n    self.A[larger], self.A[i] = self.A[i], self.A[larger]\n    self.sinkDown(larger)\n```\n\n注意这里上浮和下沉操作使用了递归，会占用递归栈空间，因此额外空间复杂度并不是$O(1)$。\n\n但上浮和下沉都可以改为循环迭代实现，迭代实现时额外空间复杂度为$O(1)$。要改成迭代实现并不困难，还请大家尝试自己实现。\n\n## 实现各种借口——读、增、删、初始化\n\n### 读取堆顶\n\n堆一般只允许读取堆顶，即全堆最大元素。\n\n```python\ndef top(self):\n    '''返回堆顶\n    '''\n    return self.A[0]\n```\n\n### 入堆\n\n入堆时，把元素加到堆尾，再做上浮操作。\n\n```python\ndef insert(self, v:T):\n    '''入堆\n    '''\n    # 将元素加到堆尾并做上浮操作\n    self.A.append(v)\n    self.floatUp(len(self.A) - 1)\n```\n\n### 出堆\n\n出堆时，取出堆顶，把堆尾元素填到堆顶后，再做下沉操作。\n\n```python\ndef pop(self)->T:\n    '''出堆\n    '''\n    # 取出堆顶元素\n    res = self.A[0]\n\n    # 将堆尾元素填到堆顶并做下沉操作\n    self.A[0] = self.A[len(self.A) - 1]\n    self.A.pop()\n    self.sinkDown(0)\n\n    return res\n```\n\n注意入堆与出堆操作都要保证堆的大小会相应变化。\n\n### 堆初始化\n\n堆的初始化采用堆合并策略，从堆尾到堆顶逐个元素做下沉操作。\n\n```python\ndef __init__(self, A:List[T]=[], \n             fCompare:Callable[[T,T],bool]=lambda a,b:a>b\n             ) -> None:\n    '''堆初始化\n\n    :param A: 在数组A上进行初始化\n    :param fCompare: 比较函数，对堆中节点p与子节点c，有fCompare(p,c)==True\n    '''\n    self.A = A\n    self.fCompare = fCompare\n    for i in reversed(range(len(A))):\n        self.sinkDown(i)\n```\n\n## 整体代码\n\n### 堆的整体实现\n\n综上，堆的整体代码实现如下：\n\n```python\nfrom typing import Any, Callable, Generic, List, TypeVar\n\nT = TypeVar(\"T\")\n\ndef lfChildOf(i:int):\n    return (i + 1) << 1 - 1\n\ndef rtChildOf(i:int):\n    return (i + 1) << 1\n\ndef parentOf(i:int):\n    return (i - 1) >> 1\n\nclass Heap(Generic[T]):\n    '''堆结构\n\n    有两个成员：\n    self.A: List[T] # 堆内元素集合，元素类型为T，储存为数组\n    self.fCompare: Callable[[T,T],bool] # 比较函数\n    \n    下面假设堆为大顶堆\n    即有self.fCompare = lambda a,b: a>b\n    '''\n    def __init__(self, A:List[T]=[], \n                 fCompare:Callable[[T,T],bool]=lambda a,b:a>b\n                 ) -> None:\n        '''堆初始化\n\n        :param A: 在数组A上进行初始化\n        :param fCompare: 比较函数，对堆中节点p与子节点c，有fCompare(p,c)==True\n        '''\n        self.A = A\n        self.fCompare = fCompare\n        for i in reversed(range(len(A))):\n            self.sinkDown(i)\n    \n    def size(self):\n        '''返回堆大小\n        '''\n        return len(self.A)\n    \n    def top(self):\n        '''返回堆顶\n        '''\n        return self.A[0]\n    \n    def sinkDown(self, i:int):\n        '''下沉操作\n\n        对下标为i的元素递归地进行下沉操作\n        直到该元素大于其两个子节点或该元素下沉到叶子节点\n        '''\n        lc = lfChildOf(i)\n        rc = rtChildOf(i)\n\n        # 比较元素i与其两个子节点，获取三个元素中存在且最大的元素\n        larger = i\n        if lc < self.size() and self.fCompare(self.A[lc], self.A[larger]):\n            larger = lc\n        if rc < self.size() and self.fCompare(self.A[rc], self.A[larger]):\n            larger = rc\n        \n        # 当元素i大于其两个子节点时符合堆结构，结束递归\n        # 当元素i下沉到叶子节点时，左右子节点不存在，也会在此结束递归\n        if larger == i:\n            return\n        \n        # 元素i小于其中一个子节点，交换i与较大子节点并继续下沉\n        self.A[larger], self.A[i] = self.A[i], self.A[larger]\n        self.sinkDown(larger)\n\n    def floatUp(self, i:int):\n        '''上浮操作\n\n        对下标为i的元素递归地进行上浮操作\n        直到该元素小于其父节点或该元素上浮到根节点\n        '''\n        # 元素i上浮到根节点时结束递归\n        if i <= 0:\n            return\n        \n        # 当元素i小于其父节点时符合堆结构，结束递归\n        pr = parentOf(i)\n        if self.fCompare(self.A[pr], self.A[i]):\n            return\n        \n        # 元素i大于其父节点，交换i与其父节点并继续上浮\n        self.A[pr], self.A[i] = self.A[i], self.A[pr]\n        self.floatUp(pr)\n    \n    def insert(self, v:T):\n        '''入堆\n        '''\n        # 将元素加到堆尾并做上浮操作\n        self.A.append(v)\n        self.floatUp(len(self.A) - 1)\n\n    def pop(self)->T:\n        '''出堆\n        '''\n        # 取出堆顶元素\n        res = self.A[0]\n\n        # 将堆尾元素填到堆顶并做下沉操作\n        self.A[0] = self.A[len(self.A) - 1]\n        self.A.pop()\n        self.sinkDown(0)\n\n        return res\n```\n\n### 单元测试\n\n入堆、出堆等操作的简单单元测试如下：\n\n```python\nimport pytest\nimport heap\n\n@pytest.fixture\ndef initHeap():\n    return heap.Heap([1,3,4,7,2,6,5,9,0,8], \n                     lambda a,b:a>b)\n\nclass Test_TestHeap:\n    def test_init_notNull(self, initHeap:heap.Heap):\n        assert initHeap.size() == 10\n        assert initHeap.top() == 9\n    \n    def test_insert_notTop(self, initHeap:heap.Heap):\n        initHeap.insert(6)\n        assert initHeap.size() == 11\n        assert initHeap.top() == 9\n    \n    def test_insert_top(self, initHeap:heap.Heap):\n        initHeap.insert(10)\n        assert initHeap.size() == 11\n        assert initHeap.top() == 10\n    \n    def test_pop(self, initHeap:heap.Heap):\n        p = initHeap.pop()\n        assert p == 9\n        assert initHeap.size() == 9\n        assert initHeap.top() == 8\n```\n\n# 关于堆排序\n\n算法竞赛中除了原生使用堆结构以外，还有一个使用到堆的地方——堆排序。堆排序有原地排序、最坏时间复杂度为$O(nlogn)$等优秀的性质，是比较常用的一个排序算法。\n\n然而，手写堆排序要注意的地方与手写堆结构有比较大的不同。堆排序时要注意的点如下：\n\n1. 堆排序时一般要求在给入数组上原地排序，不需要内部维护一个数组结构，反之，需要记录堆结构的大小。\n2. 堆结构一般占用数组前端，因此从小到大排序时，有序部分从数组末尾开始扩张，建立的堆为大顶堆。\n3. 堆排序只需要建堆与出堆操作，因此只需要实现下沉操作。\n\n关于堆排序的具体讨论，有机会的话我会另外写一篇来讲解。",
        "title": "如何手撕一个堆",
        "abstract": "在参加如AtCoder等算法竞技，或是刷Leetcode等算法题时，我们总是不可避免地遇到堆这种数据结构。\n当然，一般来说我们只要理解堆，知道堆的性质，知道怎么样用堆就足够了。在做题时只需要调用系统类库即可——在参加AtCoder时你甚至不会有时间去自己实现一个堆。\n但是，如果哪一天你把编程语言的类库全忘光了，又遇到一题需要频繁求最值的题目——你明知这里要用堆，却又忘记该调用的类名了，咋办？我还真遇到过这问题：三年没刷算法，只能对着一道自己明显会的题干着急，愣是想不起PriorityQueue的名字。这时候，只能自己实现一个堆出来了。",
        "length": 483,
        "created_at": "2021-08-28T23:09:14.000Z",
        "updated_at": "2024-04-14T13:30:33.000Z",
        "tags": [
          "数据结构",
          "算法",
          "算法竞赛"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2021-08-21-The-beauty-of-design-parten.md",
        "pagePath": "/articles/The-beauty-of-design-parten",
        "slug": "The-beauty-of-design-parten"
      },
      "meta": {
        "content": "\n# 导读\n\n## 02：如何评价代码好坏？\n\n从7个方面评价代码的好坏：\n\n1. 易维护性：根本\n2. 可读性：最重要\n3. 易扩展性：对修改关闭，对扩展开放\n4. 灵活性\n5. 简洁性：KISS\n6. 可复用性：DRY\n7. 可测试性：TDD，单元测试，控制反转与依赖注入\n\n## 03：编程方法论\n\n设计模式之美这一课程不单止讲设计模式，而会讲包括设计模式在内的指导我们进行代码设计的方法论。包括以下5个方面：\n\n1. 面向对象：封装，抽象，继承，多态\n2. 设计原则：SOLID（单一职责，开闭原则，里氏替换，接口隔离，依赖倒置），DRY，KISS，YAGNI，LOD\n3. 设计模式\n4. 编程规范：可读性，命名规范\n5. 重构技巧：（目的，对象，时机，方法），保证手段（单元测试与可测性），两种规模\n\n整个课程会以编程方法论为纵轴，以代码好坏的评价为横轴，来讲提高代码质量的方法以及采用这种方法的原因。\n\n\n# 面向对象\n\n使用封装，抽象，继承，多态，作为代码设计和实现的基石。\n\n1. 面向对象分析（做什么），设计（怎么做），编程\n\n## 05：封装，抽象，继承，多态\n\n|      | 是什么                 | 怎么做                 | 为什么                                                         |\n| ---- | ---------------------- | ---------------------- | -------------------------------------------------------------- |\n| 封装 | 信息隐藏、数据访问保护 | 访问控制关键字         | 减少不可控因素、统一修改方式、保证可读性与可维护性、提高易用性 |\n| 抽象 | 隐藏实现方法           | 函数、接口类、抽象类   | 提高可扩展性与维护性、过滤非必要信息                           |\n| 继承 | is-a关系               | 继承机制               | 代码复用、反映真实世界关系                                     |\n| 多态 | 子类替代父类           | 继承、接口类、鸭子类型 | 提高扩展性与复用性                                             |\n\n- 继承不应过度使用，会导致层次过深，导致低可读性与低可维护性\n- 在我看来，多态的本质与其说是子类替代父类，更应说是用同一个过程方法能适应多种不同类型的对象。\n- 有些观点认为，多态除了表中这三种实现方式以外，还有泛型的实现方式，被称为连接时多态。\n\n## 06，07：面向过程与面向对象\n\n1. 面向过程是：数据与方法分离\n2. 面向对象优势：适应大规模开发，代码组织更清晰；易复用、易扩展、易维护；人性化；\n3. 看似面向对象实际面向过程：滥用getter、setter破坏封装；滥用全局变量与全局方法，Constants类与Utils类；数据与方法分离，贫血模型；\n4. 为什么容易面向过程：略\n5. 面向过程的用处：略\n\n## 08：接口与抽象类\n\n1. 接口类与抽象类语法特性：略\n2. 抽象类表示is-a，为了解决代码复用。接口表示能做什么，为了解耦，隔离接口与实现。\n3. 应用场景区别：\n    - 抽象类：代表is-a关系，解决代码复用问题\n    - 接口类：解决抽象、解耦问题",
        "title": "设计模式之美读书笔记",
        "abstract": "从7个方面评价代码的好坏：\n1. 易维护性：根本\n2. 可读性：最重要",
        "length": 62,
        "created_at": "2021-08-21T08:53:27.000Z",
        "updated_at": "2022-03-27T13:30:33.000Z",
        "tags": [
          "设计模式",
          "笔记"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2021-12-12-init-a-new-hexo-project.md",
        "pagePath": "/articles/init-a-new-hexo-project",
        "slug": "init-a-new-hexo-project"
      },
      "meta": {
        "content": "\n## 使用 hexo 搭建博客\n\n最近使用 hexo 搭建了一个博客，并打算挂载在 github page 上。\n对之前的那个博客进行替代，并将之前的文章逐渐搬移过来。\n\n使用的[这个主题](https://github.com/Yue-plus/hexo-theme-arknights)功能还是比较完善的。\n\n我们可以尝试一下代码块高亮：\n\n```python\ndef func_echo(s: str):\n    print(s)\n\n\nclass HelloPrinter:\n    printer: Callable[[str]]\n\n    def __init__(self, printer: Callable[[str]]):\n        self.printer = printer\n    \n    def call(self, s: str):\n        self.printer(s)\n\n\np = HelloPrinter(func_echo)\np.call(\"hello world!\")\n```\n\n试试下标语法吧：\n\n这是一句话。[^sub]\n\n没想到还支持下标语法，还是比较惊艳的。\n\n来几句 mermaid 吧\n\n```mermaid\ngraph LR\n\nohmy-->coll\n\n```\n\n原本是不能渲染的， 这个主题渲染代码块时把 mermaid 代码当作普通代码，往里面里插换行符号了。\n使用了 hexo-filter-mermaid-diagrams 插件，添加 mermaid 过滤器，解决问题。\n\n\n来几句 LaTeX:\n\n$$\n\\begin{aligned}\nf(x) &= \\sum_{i=2}^{\\infty}{\\Join} \\\\\n&= \\sum_{i=2}^{\\infty}{\\frac{1}{i}}\n\\end{aligned}\n$$\n\n原本是不能渲染的，因为与 hexo 的渲染器有冲突，需要转义。\n我因为需要从以前的博客把文章转移过来觉得比较麻烦...\n于是魔改了一下主题，用上 mathjax 插件，能渲染了，感觉挺不错的。\n再改善一下推个 PR 吧。\n\n\n\n\n[^sub]: 这是脚注",
        "title": "init-a-new-hexo-project",
        "abstract": "最近使用 hexo 搭建了一个博客，并打算挂载在 github page 上。\n对之前的那个博客进行替代，并将之前的文章逐渐搬移过来。\n使用的[这个主题](https://github.com/Yue-plus/hexo-theme-arknights)功能还是比较完善的。",
        "length": 66,
        "created_at": "2021-12-12T20:09:13.000Z",
        "updated_at": "2022-03-27T13:30:33.000Z",
        "tags": [
          "Blog"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-03-26-create-blog-cicd-by-github.md",
        "pagePath": "/articles/create-blog-cicd-by-github",
        "slug": "create-blog-cicd-by-github"
      },
      "meta": {
        "content": "\nGitHub Action 自动化构建发布到 GitHub Pages 大家都见得多了，甚至 Hexo 官方自己都有相关的文档。\n但我今天要做的不是发布到 GitHub 这么简单，而是要同时发布到 GitHub 和自己的域名下。\n\n# 这篇文章的目标\n\n我们需要构建一个 CI/CD 过程。这个过程需要做到以下目标：\n1. 将文章 push 到 GitHub 的 master branch 后，自动触发。\n2. 我们博客使用 Hexo 引擎，需要先构建静态文件。\n3. 需要将静态文件部署到 GitHub Page 。\n4. 需要将静态文件部署到自己域名下。\n    这里我们使用 AWS 的 S3 服务与 CloudFront 服务直接部署到 CDN 上。 CloudFront 直接通过 OAI 访问 S3 ，不允许用户直接通过 S3 访问。\n5. 博客在 GitHub Page 与 S3 需要处于不同的路径下。\n    为了延续以往的情况，博客在 GitHub Page 需要部署在 `/blog/` 下。\n    而在 AWS 上我则希望直接部署在根目录下，这就导致需要两份配置文件。\n    当然弄两份配置文件我是不乐意的，于是就需要从模板自动生成配置文件...\n\n其中，一二三点都很好解决，而第四点会是一个比较难又比较坑爹的地方。\n\n# 先做简单的 —— CI/CD 构建并发布到 GitHub Pages\n\n这一步其实没什么难的， Hexo 官网上就有[这篇文章](https://hexo.io/docs/github-pages.html)写的十分详细了，可以作为参考。\n\n```yaml\nname: Pages\n\non:\n  push:\n    branches:\n      - master  # default branch\n\njobs:\n  pages:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Use Node.js 16.x\n        uses: actions/setup-node@v2\n        with:\n          node-version: '16'\n      - name: Cache NPM dependencies\n        uses: actions/cache@v2\n        with:\n          path: node_modules\n          key: ${{ runner.OS }}-npm-cache\n          restore-keys: |\n            ${{ runner.OS }}-npm-cache\n      - name: Install Dependencies\n        run: npm install\n      - name: Build\n        run: npm run build\n      - name: Deploy\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./public\n          publish_branch: gh-pages  # deploying branch\n```\n\n这个 yaml 就是 GitHub Action 的 workflow 文件，在这个 workflow 里：\n1. 先用 `npm run build` 把静态文件生成到 `./public` 下\n2. 用 `peaceiris/actions-gh-pages@v3` 这个 action 把 `./public` 的文件放到 `gh-pages` 分支下。\n\n把上面这个 yaml 文件复制到 `.github/workflows/build.yml` 中，这样 master 分支上发生任何提交都会触发构建流程了。按照 Hexo 官网上的文档跑一边就能成功发布到 GitHub Pages 上了。\n\n不过我需要部署到 `/blog/` 下，这叫 Project Page ，因此我走的是 Hexo 文档的 Project Page 这一小节的流程，需要把 `_config.yml` 里做如下设置：\n\n```yaml\nurl: https://ryojerryyu.github.io/blog # 这个其实不是很重要，现在用的主题没有用到这个字段\nroot: /blog/ # 这个比较重要，这个不设定好，整个页面的超链接都会歪掉\n```\n\n当然， “没什么难” 的前提是你首先要对 Hexo 和 GitHub Action 有一个了解...\n\n# 难一点 —— 搭建 AWS 基础设施\n\n我为什么不止用 GitHub Pages 还要配一套 AWS 呢？其实主要还是想以后可能会做一下 Backend ，而且放 AWS 上还能利用 AWS 的服务做一下流量分析之类的。没这么些需求的小伙伴可以不用继续看了...\n\n我们打算使用 AWS 的 S3 与 CloudFront 服务， CloudFront 直接通过 OAI 访问 S3 。\n\n## S3\n\nS3 是 AWS 的对象储存服务，简单来说就是可以当网盘用，往里面放文件。\nS3 有静态网站托管服务，把静态文件放到 S3 里，配置一番就直接可以通过 HTTP 访问了，还能用自己的域名。\n但我们不打算使用 S3 的静态网站托管，因为我打算直接上 CDN ，又不想用户可以直接通过 S3 来访问我们的静态文件。\n\n## CloudFront\n\nCloudFront 是 AWS 的内容分发服务，简单来说就是 CDN 。其实它不只有 CDN 的功能，它还能加速动态调用，还能通过 CloudFront 连接 Web Socket ... 不过我们这次主要是用 CDN 功能。\nCloudFront 访问 S3 的方式还是有好几种的。中文教程最常见的是让你先打开 S3 静态网站托管，然后将 CloudFront 的源设为 S3 的域名。\n这个方法是最早支持的，因此推广的也比较开。但其实我觉得这个方法有些问题：\n\n1. S3 不做另外配置的话是可以直接访问的，比较 low\n2. S3 自己的 HTTP Endpoint 不能上 TLS ，所以 CloudFront 到 S3 这一段是裸奔的\n\n因此我打算使用 AWS 最近推荐的 OAI 方式访问 S3 。这种方式不走 HTTP Endpoint 而是 S3 自己的 S3 Endpoint ，可以通过 AWS 的 IAM 机制统一管理。\nOAI 是 Origin Access Identity ，简单来说就是给 CloudFront 一个 AWS IAM Policy 的 Principal 身份， S3 可以通过如下 Bucket Policy 限制外部只能通过这个 Principal 访问：\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::cloudfront:user/<CloudFront Origin Access Identity ID>\"\n            },\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::<bucket name>/*\"\n        }\n    ]\n}\n```\n上面这一段看不懂的同学，可以去补习一下 AWS IAM 权限管理机制，关键就是 Principal —— 主体 、 Action —— 动词 、 Resource —— 受体 的一个主谓宾模式。\n\n## 其他 AWS 服务\n\n当然，仅有 S3 和 CloudFront 是不足以实现全部功能的，我们还需要 Route53 来管理路由， ACM 来获取免费证书。\n但这些我都不打算细讲，因为内容真的很多-_-，而且大部分都是 AWS 的细节，搬到别的云上不一定适用...而且手动操作麻烦死了...\n\n## Pulumi\n\n综上嘛，我们需要：\n1. 建一个 Route53 Hosted Zone ，把域名交给 Route53 管理\n2. 用 ACM 给域名申请一个 us-east-1 Region 的免费证书（CloudFront 的证书必须在 us-east-1 ）\n3. 建一个 S3 储存桶，把 Public Access Block 配置一下\n4. 建一个 CloudFront Distribution ，通过 OAI 来访问 S3 ，还要指定一下证书\n5. 给 S3 配一个 Bucket Policy ，允许 CloudFront 访问\n6. 把 Route53 里的域名弄个 DNS 记录指向 CloudFront\n\n手动操作麻烦死了，于是我打算用 IaC (Infrastructure-as-Code) 来解决。我把这些基础设施定义用 Pulumi 写成的代码放在[这里](https://github.com/RyoJerryYu/aws-blog-infra/tree/c97f0fe41b5c0306d5343ddfc22f4a3775d79b88/website)了，大家可以参考一下（做了模块化，跟我其他基础设施放一起了）。\n\n当然，用 Pulumi 没什么特别原因，纯粹是因为我最近在写 Pulumi... 你完全可以用其他 IaC 工具（Ansible、Terraform、CloudFormation）来做。而且 Pulumi 太新了，用起来挺多 Bug 的...（也许是我不会用）\n\n## 测试一下\n\nS3 桶啥的都建好之后，本地把文件 build 一下，用 `aws s3 cp ./public/ s3://<bucket>/ --recursive` 之类的命令上传到 S3 ，给 CloudFront 创建一个 Invalidation 刷新一下 CloudFront 缓存，访问域名看看，有返回个 HTML 我们的基础设施就算是跑通了。此时可能会出现以下情况，都属正常：\n1. 访问返回 307 ：\n    是 S3 储存桶 Region 不在 us-east-1 导致的。\n    CloudFront 是通过 s3 的 global endpoint 访问 s3 的，但不在 us-east-1 的 s3 刚新建时还不能通过 global endpoint 访问。\n    参考 so 的[这个问题](https://stackoverflow.com/questions/38706424/aws-cloudfront-returns-http-307-when-origin-is-s3-bucket)：\n\n    > All buckets have at least two REST endpoint hostnames. In eu-west-1, they are example-bucket.s3-eu-west-1.amazonaws.com and example-bucket.s3.amazonaws.com. The first one will be immedately valid when the bucket is created. The second one -- sometimes referred to as the \"global endpoint\" -- which is the one CloudFront uses -- will not, unless the bucket is in us-east-1. Over a period of seconds to minutes, variable by location and other factors, it becomes globally accesible as well. Before that, the 307 redirect is returned. Hence, the bucket was not ready.\n    \n    这时候只要等个十几分钟就好了。\n2. 本地 build 的时候没配置好的话，js 之类的静态文件可能返回不了，但问题不大，我们接下来再处理。\n\n\n# 搭建 S3 的 workflow\n\n基础设施搭好了，我们就要像 deploy 到 GitHub Pages 一样，造一个自动管线发布到 S3 了。\n整理一下，我们的 workflow 里要包括：\n\n1. 从模板生成配置文件\n    别忘了，我需要的是静态文件部署在 GitHub Pages 和自己域名下的不同路径上。 Hexo 生成静态文件前配置文件必须要改的。\n2. 把原先 s3 上的文件删除，并上传新的文件到 s3\n3. 给 CloudFront 创建一个 Invalidation 刷新缓存\n\n## 生成配置文件\n\n这一步其实方案很多，甚至 bash 直接全文替换都可以...\n不过怕以后要改的东西变多，这里还是选择一些模板生成工具。有如下选择：\n\n1. 屠龙刀 Ansible\n2. Python Jinja2\n3. Go Template\n\n这里用 Ansible 确实是大材小用了，而且 Ansible 不能在 Windows 下用还是有点不方便，只能弃选。而 Python 和 Go 里我选了 Go Template ，原因是... 不想写 Python...\n这里其实确实是装逼了，这种小型脚本应该 Python 比 Go 合适的多。不过还好 Go run 可以不先 go mod 就能运行，不算是个太差的选择。不过以后还是大概率要改回 Python 。\n\n写 golang 脚本没有难度，大致如下：\n\ngolang template 的 name 要是 file name\n```golang\nname := path.Base(*tmpl)\nt := template.Must(template.New(name).ParseFiles(*tmpl))\nerr = t.Execute(os.Stdout, config)\nif err != nil {\n    log.Fatal(err)\n}\n```\ngithub workflow 如下\n```yaml            \n- name: Use Go 1.16\n    uses: actions/setup-go@v1\n    with:\n    go-version: '1.16.1'\n\n- name: generate config\n    run: go run ./genconfig/main.go --env=gh-pages > _config.yml\n\n```\nwindows 玩家可能要注意一下，windows 下编码有问题， `go run ./genconfig/main.go --env=gh-pages > _config.yml` 这段命令直接在 PowerShell 下跑生成出来的文件不能被 Hexo 识别。不过没什么关系，反正这段到时候是在 GitHub Action Runner 上跑的，只不过是不能本地生成用来测试而已。\n\n[参考代码](https://raw.githubusercontent.com/RyoJerryYu/blog/2f407cb6ee723d0e17c97af1289bd2231bb265ab/genconfig/main.go)\n\n## 上传 s3 与刷新 CloudFront\n\n后两步搜一下发现其实有很多现成的 GitHub Action 可以用。\n不过我没有采用，原因是——真的没必要啊...就几个命令的事，又不是不会敲...\n\nworkflows yaml 如下：\n```yaml\n- name: Configure AWS\n    uses: aws-actions/configure-aws-credentials@v1\n    with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: ap-northeast-1\n- name: Deploy\n    env:\n        S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}\n        DISTRIBUTION_ID: ${{ secrets.AWS_CLOUDFRONT_DISTRIBUTION_ID }}\n    run: |\n        aws s3 rm s3://$S3_BUCKET/* --recursive\n        aws s3 cp ./public s3://$S3_BUCKET/ --recursive\n        aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID --paths '/*' --region=us-east-1\n```\n\n[完整 yaml 参考代码](https://raw.githubusercontent.com/RyoJerryYu/blog/f0affb812f2de437943d9cf2a4f8a5fe690d1efd/.github/workflows/clouds.yml)\n\n由于改为了生成配置文件， deploy 到 Github Pages 的 yaml 也要做相应改动，这里就不多说。\n\n# CloudFront 的一点小问题（不太小）\n\n这样我们的整个流程是不是跑完了？我们的博客已经部署到自己的域名下了？\n浏览器打开自己的域名看看，完美显示！\n\n等等，别高兴的太早，点进去一篇文章... 403 了...\n\n403 的原因：\n1. hexo 生成出来的 page 连接是 `/` 结尾的，如 `/2022/03/26/create-blog-cicd-by-github/` ，然后通过 HTTP 服务器的自动转义指向 `/2022/03/26/create-blog-cicd-by-github/index.html` 文件。\n2. CloudFront 可以定义默认根对象，没有为每个子路径都自动转义的功能。\n3. S3 的 HTTP endpoint 可以配置索引文档，为每个子路径自动转义，但 CloudFront 通过 OAI 访问 S3 时通过 REST endpoint 访问，不会触发自动转义。\n\n一大波参考阅读：\n\n[Specifying a default root object](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DefaultRootObject.html)\n> Here's an example of how a default root object works. Suppose the following request points to the object image.jpg:\n> ```\n> https://d111111abcdef8.cloudfront.net/image.jpg\n> ```\n> In contrast, the following request points to the root URL of the same distribution instead of to a specific object, as in the first example:\n> ```\n> https://d111111abcdef8.cloudfront.net/\n> ```\n> When you define a default root object, an end-user request that calls the root of your distribution returns the default root object. For example, if you designate the file index.html as your default root object, a request for:\n> ```\n> https://d111111abcdef8.cloudfront.net/\n> ```\n> Returns:\n> ```\n> https://d111111abcdef8.cloudfront.net/index.html\n> ```\n> However, if you define a default root object, an end-user request for a subdirectory of your distribution does not return the default root object. For example, suppose index.html is your default root object and that CloudFront receives an end-user request for the install directory under your CloudFront distribution:\n> ```\n> https://d111111abcdef8.cloudfront.net/install/\n> ```\n> CloudFront does not return the default root object even if a copy of index.html appears in the install directory.\n> \n> If you configure your distribution to allow all of the HTTP methods that CloudFront supports, the default root object applies to all methods. For example, if your default root object is index.php and you write your application to submit a POST request to the root of your domain (http://example.com), CloudFront sends the request to http://example.com/index.php.\n> \n> The behavior of CloudFront default root objects is different from the behavior of Amazon S3 index documents. When you configure an Amazon S3 bucket as a website and specify the index document, Amazon S3 returns the index document even if a user requests a subdirectory in the bucket. (A copy of the index document must appear in every subdirectory.) For more information about configuring Amazon S3 buckets as websites and about index documents, see the Hosting Websites on Amazon S3 chapter in the Amazon Simple Storage Service User Guide.\n\n[Configuring an index document](https://docs.aws.amazon.com/AmazonS3/latest/userguide/IndexDocumentSupport.html)\n> In Amazon S3, a bucket is a flat container of objects. It does not provide any hierarchical organization as the file system on your computer does. However, you can create a logical hierarchy by using object key names that imply a folder structure.\n> \n> For example, consider a bucket with three objects that have the following key names. Although these are stored with no physical hierarchical organization, you can infer the following logical folder structure from the key names:\n> - sample1.jpg — Object is at the root of the bucket.\n> - photos/2006/Jan/sample2.jpg — Object is in the photos/2006/Jan subfolder.\n> - photos/2006/Feb/sample3.jpg — Object is in the photos/2006/Feb subfolder.\n> \n> In the Amazon S3 console, you can also create a folder in a bucket. For example, you can create a folder named photos. You can upload objects to the bucket or to the photos folder within the bucket. If you add the object sample.jpg to the bucket, the key name is sample.jpg. If you upload the object to the photos folder, the object key name is photos/sample.jpg.\n> \n> If you create a folder structure in your bucket, you must have an index document at each level. In each folder, the index document must have the same name, for example, index.html. When a user specifies a URL that resembles a folder lookup, the presence or absence of a trailing slash determines the behavior of the website. For example, the following URL, with a trailing slash, returns the photos/index.html index document.\n> ```\n> http://bucket-name.s3-website.Region.amazonaws.com/photos/\n> ```\n> \n> However, if you exclude the trailing slash from the preceding URL, Amazon S3 first looks for an object photos in the bucket. If the photos object is not found, it searches for an index document, photos/index.html. If that document is found, Amazon S3 returns a 302 Found message and points to the photos/ key. For subsequent requests to photos/, Amazon S3 returns photos/index.html. If the index document is not found, Amazon S3 returns an error.\n\n[Implementing Default Directory Indexes in Amazon S3-backed Amazon CloudFront Origins Using Lambda@Edge](https://aws.amazon.com/blogs/compute/implementing-default-directory-indexes-in-amazon-s3-backed-amazon-cloudfront-origins-using-lambdaedge/)\n> If you implement CloudFront in front of S3, you can achieve this by using an OAI. However, in order to do this, you cannot use the HTTP endpoint that is exposed by S3’s static website hosting feature. Instead, CloudFront must use the S3 REST endpoint to fetch content from your origin so that the request can be authenticated using the OAI. This presents some challenges in that the REST endpoint does not support redirection to a default index page.\n\n> CloudFront does allow you to specify a default root object (index.html), but it only works on the root of the website (such as http://www.example.com > http://www.example.com/index.html). It does not work on any subdirectory (such as http://www.example.com/about/). If you were to attempt to request this URL through CloudFront, CloudFront would do a S3 GetObject API call against a key that does not exist.\n\n\n\n那么，我们要怎么解决这个问题呢？我觉得，这个问题有三种解决方法：\n\n1. 不使用 OAI ，让 CloudFront 直接指向 S3 的域名，让 CloudFront 使用 S3 HTTP Endpoint 的特性\n2. 调整 Hexo 配置，更改生成文件路径或连接路径\n3. 使用 AWS 推荐的 Lambda@Edge 功能，在 CloudFront 上修改路径\n\n其中第二种方案是最下策，我们不能在还有其他方案的情况下，因为基础设施的一个性质就去修改我们的产品。况且我们的产品在大多数场景下都是适用的。\n第一种方案是中策，也许实行起来也是最简单的。但我不想用，原因上面也说过了。\n第三种方案是实施起来难度最大的，我们要引入 Lambda 这一新概念。但反正折腾嘛，试试就试试，反正失败了再变回第一种方案就是。\n\n## 创建 Lambda\n\n[Implementing Default Directory Indexes in Amazon S3-backed Amazon CloudFront Origins Using Lambda@Edge](https://aws.amazon.com/blogs/compute/implementing-default-directory-indexes-in-amazon-s3-backed-amazon-cloudfront-origins-using-lambdaedge/)\n\n参考上面的文档，我们直接在 Console 创建一个 Lambda 函数，内容如下：\n\n```javascript\n'use strict';\nexports.handler = (event, context, callback) => {\n    \n    // Extract the request from the CloudFront event that is sent to Lambda@Edge \n    var request = event.Records[0].cf.request;\n\n    // Extract the URI from the request\n    var olduri = request.uri;\n\n    // Match any '/' that occurs at the end of a URI. Replace it with a default index\n    var newuri = olduri.replace(/\\/$/, '\\/index.html');\n    \n    // Log the URI as received by CloudFront and the new URI to be used to fetch from origin\n    console.log(\"Old URI: \" + olduri);\n    console.log(\"New URI: \" + newuri);\n    \n    // Replace the received URI with the URI that includes the index page\n    request.uri = newuri;\n    \n    // Return to CloudFront\n    return callback(null, request);\n\n};\n```\n这一段代码主要作用是把接收到每个以 `/` 结尾的请求，都转换为以 `/index.html` 结尾的请求。\n\nDeploy 之后，为 Lambda 添加 Trigger ，选择 CloudFront 作为 Trigger ， Event 选择 On Request 。按照界面的提示为 Lambda 创建专用的 Role 。\n提交后，我们就可以通过 Url 访问，发现 `/` 结尾的 URL 也会正常显示了。\n\n# 之后的事\n\n这个过程仍有以下问题：\n- 对 Lambda 的认识仍有不足，今后需继续学习运用\n- Lambda@Edge 还没有结合到 IaC 中\n- 配置文件生成过程仍有改进空间\n\n留下这些问题，今后再修改。\n",
        "title": "用 GitHub Action 自动化构建 Hexo 并发布到 S3",
        "abstract": "GitHub Action 自动化构建发布到 GitHub Pages 大家都见得多了，甚至 Hexo 官方自己都有相关的文档。\n但我今天要做的不是发布到 GitHub 这么简单，而是要同时发布到 GitHub 和自己的域名下。\n我们需要构建一个 CI/CD 过程。这个过程需要做到以下目标：",
        "length": 342,
        "created_at": "2022-03-26T23:55:08.000Z",
        "updated_at": "2022-03-27T13:31:04.000Z",
        "tags": [
          "Blog",
          "GitHub",
          "AWS",
          "CI/CD",
          "IaC",
          "DevOps"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-04-03-use-paste-image-and-vscode-memo.md",
        "pagePath": "/articles/use-paste-image-and-vscode-memo",
        "slug": "use-paste-image-and-vscode-memo"
      },
      "meta": {
        "content": "\n我平时使用 [vscode-memo](https://github.com/svsool/vscode-memo) 插件写笔记，其中插入图片使用 `![[]]` 语法，显示简短，也有较好的预览支持，体验极佳。希望这种特性也能在写 hexo 博客的时候使用。\n\n# 关于 vscode-memo\n\n可能有很多人不熟悉 vscode-memo 这个插件，我先来简单介绍一下。\n\nvscode-memo 定位是一个 knowledge base ，对标的是 [Obsidian.md](https://obsidian.md/) 等软件。其功能包括且不限于：\n\n1. 使用独有的短链接语法 `[[]]` 连接到其他文档与图片。\n2. 修改文件名时自动同步更新链接，反向查找当前文档被那些文档链接。\n3. 鼠标悬停时能预览链接与图片。\n\n同时，由于 vscode-memo 是个 vscode 插件，可以跟 vscode 的其他众多插件合作使用。比如 [vscode-memo 官方文档](https://github.com/svsool/vscode-memo/blob/master/help/How%20to/Pasting%20images%20from%20clipboard.md)里就推荐将 vscode-memo 与 vscode-past-image 插件配合，粘贴图片。\n\n这篇文章主要的目的，也是利用这两个插件，达到把图片粘贴为短链接，并被 Hexo 正常渲染为网页。\n\n# Image Paste 与 Hexo 的配置\n\n这一步其实很简单。\n\n在 Hexo 的文章中，一般需要使用从根目录起的相对链接。如有文件结构：\n\n```tree\nsource\n├───img\n│   └───in-post\n│       ├───heap-cheat-sheet.jpg\n│       └───post-js-version.jpg\n├───playground\n└───_posts\n    ├───2022-03-26-create-blog-cicd-by-github.md\n    └───2022-04-03-use-paste-image-and-vscode-memo.md\n```\n\n在 `2022-03-26-create-blog-cicd-by-github.md` 中引用 `heap-cheat-sheet.jpg` 这个图片，就需要 `![](/img/in-post/heap-cheat-sheet.jpg)` 这样的链接。\n\n但如果在配置里把 `post_asset_folder` 设为 `true` ，就可以在 Markdown 文件的同级位置的同名目录中直接找到图片。如：\n\n```tree\nsource\n├───img\n├───playground\n└───_posts\n    ├───2022-03-26-create-blog-cicd-by-github\n    │    ├───heap-cheat-sheet.jpg\n    │    └───post-js-version.jpg\n    ├───2022-03-26-create-blog-cicd-by-github.md\n    └───2022-04-03-use-paste-image-and-vscode-memo.md\n```\n\n然后在 `2022-03-26-create-blog-cicd-by-github.md` 中可以直接 `![](heap-cheat-sheet.jpg)` 引用图片。为了图片文件管理方便，我们打开这个配置项。\n\n为了能让 Image Paste 粘贴的图片能放到这个同名文件夹下，我们需要修改 Image Paste 配置，在 VSCode 的 Workspace Setting 中，添加如下设置：\n\n```json\n{\n    \"pasteImage.path\": \"${currentFileDir}/${currentFileNameWithoutExt}/\"\n}\n```\n\n# Image Paste 粘贴为 vscode-memo 短链接格式\n\n这一步也很简单。 Image Paste 可以设定粘贴后的格式。我们在 Workspace Setting 中添加如下设置即可：\n\n```json\n{\n    \"pasteImage.insertPattern\": \"![[${imageFileName}]]\",\n}\n```\n\n这样我们粘贴后的图片就能有预览功能了。\n\n# 让 Hexo 正确渲染 vscode-memo 的短链接\n\n这一步其实是最难的。 Hexo 当然不认识 vscode-memo 的短链接，而经过调查，现在还没有现成的方案让 Hexo 与 vscode-memo 集成。虽然我们提倡尽量不要重复造轮子，但这里我们也是除了造轮子没有其他办法了。\n\n我们采用的方案是让 Hexo 在渲染 Markdown 前，先把 Markdown 中形如 `![[]]` 的短链接，替换为 `![]()` 的正常 Markdown 图片链接。\n\n假设我们项目 `source` 文件夹如下：\n\n```tree\nsource\n├───playground\n└───_posts\n    ├───2022-03-26-create-blog-cicd-by-github\n    │    ├───heap-cheat-sheet.jpg\n    │    └───post-js-version.jpg\n    ├───2022-03-26-create-blog-cicd-by-github.md\n    └───2022-04-03-use-paste-image-and-vscode-memo.md\n```\n\n如在渲染 `2022-03-26-create-blog-cicd-by-github.md` 前，需要将其中的 `![[heat-cheat-sheet.jpg]]` 替换为 `![](heap-cheat-sheet.jpg)` 。我们知道 Hexo 在生成静态文件前会先把项目根目录下 `scripts` 目录下的所有脚本执行一遍。我们可以在这里注册一个 filter ，专门做这个替换。代码如下：\n\n```js\n'use-strict';\n\nhexo.extend.filter.register('before_post_render', function (data) {\n    const isToHandle = (data) => {\n        var source = data.source;\n        var ext = source.substring(source.lastIndexOf('.') + 1, source.length).toLowerCase();\n        return ['md'].indexOf(ext) > -1;\n    }\n\n    if (!isToHandle(data)) {\n        return data;\n    }\n\n    const reg = /(\\s+)\\!\\[\\[(.+)\\]\\](\\s+)/g;\n\n    data.content = data.content\n        .replace(reg, function (raw, start, content, end) {\n            var nameAndTitle = content.split('|');\n            if (nameAndTitle.length == 1) {\n                return `${start}![](${content})${end}`;\n            }\n            return `${start}![${nameAndTitle[1]}](${nameAndTitle[0]})${end}`;\n        });\n    return data;\n\n})\n```\n\n# 测试一下\n\n文章中如下内容：\n\n![[这部分内容会被转换为图片.png]]\n\n\n而你看到上面的内容是一张图片，表示这个转换已经成功了。\n\n# 不足之处\n\n这一段代码仍有以下待改进的地方：\n1. 如果图片短链接的内容写在 Code Block 里，也一样会被转换。实际上我们一般不希望 Code Block 里的内容被转换，需要过滤一下。\n2. 形如 `![[文件|图片描述]]` 的内容会正常转换为 `![图片描述](文件)` 。然而我现在用的这个主题不支持图片描述。以后可能需要更换主题。\n\n# 补充\n\n如果希望网站图片放在 `img` 之类的文件夹下统一管理，不把 `post_asset_folder` 设为 `true` ，也是没问题的，可以通过修改代码，在返回 `${content}` 前添加统一前缀。\n\n而如果希望图片放在 `img` 下，又要按文章分文件夹管理，如下情况：\n\n```tree\nsource\n├───img\n│   ├───2022-03-26-create-blog-cicd-by-github\n│   │    ├───heap-cheat-sheet.jpg\n│   │    └───post-js-version.jpg\n│   └───2022-04-03-use-paste-image-and-vscode-memo\n├───playground\n└───_posts\n    ├───2022-03-26-create-blog-cicd-by-github.md\n    └───2022-04-03-use-paste-image-and-vscode-memo.md\n```\n\n可以通过在代码中引用 `data.source` 解决。",
        "title": "完善 Hexo 编写环境，改善文章中使用图片的体验",
        "abstract": "我平时使用 [vscode-memo](https://github.com/svsool/vscode-memo) 插件写笔记，其中插入图片使用 `![[]]` 语法，显示简短，也有较好的预览支持，体验极佳。希望这种特性也能在写 hexo 博客的时候使用。\n可能有很多人不熟悉 vscode-memo 这个插件，我先来简单介绍一下。\nvscode-memo 定位是一个 knowledge base ，对标的是 [Obsidian.md](https://obsidian.md/) 等软件。其功能包括且不限于：",
        "length": 158,
        "created_at": "2022-04-03T21:03:03.000Z",
        "updated_at": "2022-04-03T17:47:52.000Z",
        "tags": [
          "Blog",
          "VSCode",
          "Hexo",
          "JavaScript"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-06-28-graph-for-economics-1.md",
        "pagePath": "/articles/graph-for-economics-1",
        "slug": "graph-for-economics-1"
      },
      "meta": {
        "content": "\n> 1. 这篇文章参考了曼昆的《经济学原理》与北京大学王辉老师的《微观经济学》课程，内容上会有部分相似。\n> 2. 这篇文章中的图使用 3Blue1Brown 的动画生成工具 manim 的 Community Edition 制作，源代码之后会上传到 GitHub 。\n\n我们先不讲课，先来带个货。\n\n德国和加拿大有一种特别的葡萄酒，叫冰酒。这种葡萄酒制作工艺比较特殊，必须要在严冬葡萄被霜冻在藤曼上时采摘下来，再经过发酵、压榨酿造而成。\n在冰酒压榨过程中，大量的冰被去除，使得葡萄的成分得到浓缩，因此冰酒口感偏甜。但加工过程对温度要求十分苛刻，温度过高、过低、变化太过剧烈等都会对口感产生影响。由于冰酒工艺特别，主要只有德国、加拿大等少数地区生产。\n\n好了，现在来考虑一个消费场景：\n假如你到加拿大去旅游，回国时在机场看到有礼品店在卖冰酒，考虑买一瓶冰酒回国后自己消费饮用。这瓶冰酒容量大概为 400ml ，并且这瓶冰酒只是一个普通牌子，不是奢侈品或高档品牌。\n在这个场景下，请大家考虑两个问题：\n1. 假设商店中这瓶冰酒标价换算为人民币是 ￥100 ，你是否愿意以这个价格购买一瓶冰酒？\n2. 假设你还不知道这瓶冰酒的价格，你会选择购买的最高价格是一瓶冰酒多少元？\n\n在问题 2 中，你愿意支付的最高价格就是你的心理价位，如果商店价格高于心理价位，你就不会购买这件商品。而如果价格低于心理价位，你就会购买这件商品。\n\n# 意愿支付\n\n上面场景中所说的心理价位，在经济学中又叫**意愿支付**。\n\n> **意愿支付**（ willingness to pay ）：是消费者愿意为获得某种物品所支付的最高代价。 —— 曼昆《经济学原理：微观经济学分册》: Page 134\n\n如果这瓶冰酒定价是 100 元，你会觉得太贵了不买，就说明你的意愿支付价格低于 100 元。\n然后我们再假设这瓶冰酒 50 元，你觉得很好很便宜，选择买了，就说明你的意愿支付价格在 100 元到 50 元之间。\n再把范围收窄一点， 80 元选择不买， 60 元选择买。范围逐渐收敛，买与不买之间的意向会变得越来越模糊。我们假设最终收敛到 70 元，你变得非常犹豫，感觉买与不买没有差别，那 70 元就是你的意愿支付价格。\n\n[ [图片：意愿支付价格逼近。价格100时，不买，价格50时，买，80，60，最后70时会犹豫]]\n\n## 意愿支付价格的经济学解释\n\n经济学上有理性人这一概念，实际上是在假设你在决定是否做决策时，会将成本与收益做比较：\n- if 收益 > 成本 : 做出决策\n- if 收益 < 成本 : 不做出决策\n\n在购买冰酒的场景中，做决策就是指“买冰酒”这个行为。而成本就是冰酒的价格，收益就是你喝下冰酒感觉“爽到”。\n而我们不是机器人，我们“爽到”的感觉是很难与价格这种数字相比较的。因此我们要找一个办法把我们的爽到量化为价格。\n而意愿支付价格就是这个办法。在上面的例子中，买与不买的价格范围不断逼近，最终到 70 元时你觉得买还是不买都没什么区别。也就是说你喝冰酒爽到，就相当于得到了 70 元。\n\n> **意愿支付价格**：商品消费行为给消费者带来的效用的货币度量。\n\n\n有了意愿支付价格，买冰酒这件事就很容易模型化了。我们可以直接套回理性人决策的模型：\n- 做决策 = 买冰酒\n- 收益 = 意愿支付价格\n- 成本 = 冰酒价格\n\n则有：\n- if 意愿支付价格 > 冰酒价格 : 买冰酒\n- if 意愿支付价格 < 冰酒价格 : 不买冰酒\n\n[ [图片：意愿支付价格=收益柱=70元，价格=成本=线，线高于柱=不决策，线低于柱=决策]]\n\n在买冰酒这一决策中，你的收益就是 70 元（喝冰酒爽到）。要你花 100 元（冰酒价格）来换 70 元，你肯定是不干的。而要你花 50 元来换 70 元，你就会爽快答应了。\n\n## 消费者剩余\n\n在上面模型中，如果你的意愿支付价格是 70 元，而冰酒只卖 50 元，你就一定会买买买，因为只要花 50 元就能买到 70 元的“爽到”呀！买到就是赚到。\n\n70 元的“爽到”只要花 50 元就能买到，这中间就差了 20 元呢，你就会觉得买冰酒的这笔钱花得真值，赚到 20 元。经济学上就称这是得到了 20 元的消费者剩余。\n\n> **消费者剩余**（ consumer surplus ）：买者原意为一种物品支付的量减去其为此实际支付的量。 —— 曼昆《经济学原理：微观经济学分册》: Page 135\n\n计算上：\n$$\n消费者剩余 = 意愿支付价格 - 商品价格 \n$$\n而实际上，消费者剩余是你买商品时赚到的感觉，是这种感觉的量化。\n你感觉买这瓶冰酒赚飞了，量化后表现为这次交易你获得的消费者剩余多；你感觉这次交易一般般，有点小贵（但还是愿意买），量化后就是这次交易你获得的消费者剩余少。你获得多少消费者剩余，就代表你在这场交易中赚到了多少（感觉上）。\n\n现在考虑另一种情况：你的意愿支付价格为 70 元，而冰酒价格为 100 元时。你没有选择交易，因此在这种情况，你没有获得消费者剩余，当然也没有失去消费者剩余。\n从另一个角度来说，你觉得交易成立后你会得到负的消费者剩余，因此机制的你决定不交易，防止了这次损失。\n\n话又说回来，实际情况中人的决策是不可能这么理性地去比较成本与收益，甚至有可能根本得不出一个意愿支付价格。因此上述讨论都是建立在假设上的——假设理性人模型成立。\n在实际情况中，这一假设可能根本不成立，因此这些讨论在现实中可能根本不适用。可这又有什么关系呢？就算相对论是正确的，牛顿定理仍然有他价值不是吗？\n\n# 需求曲线\n\n好了，上面说了一大堆，其实都是单个消费者（你）进行消费的情况。可实际上，这冰酒总不可能只有一个人买呀！\n\n而实际上，每个人对冰酒的爱好、口感要求、奢侈品需求等都是不同的。这就导致了每个人对冰酒这一商品的意愿支付价格可能都不一样！\n\n## 意愿支付价格统计\n\n我们假设，今天其实有包括你在内的 100 个客人都来过这家冰酒店。我把这 100 个客人全部逮住，按顺序每个人都审问了一遍意愿支付价格。于是得到了这样一幅意愿支付价格统计的图：\n\n[ [图：意愿支付价格柱状图，乱序，横坐标是到店时间，纵坐标价格，横线为冰酒价格，上下浮动，意愿支付价格超过冰酒价格就会购买]]\n\n如果冰酒价格为 80 元，那所有意愿支付价格超过 80 元的客人都会选择买冰酒，而意愿支付价格低于 80 元的人都不会选择买。而如果冰酒价格为 60 元，那意愿支付价格超过 60 元的那部分客人也会开始选择买。冰酒价格越低，选择买冰酒的客人就越多。\n\n可是这图有点乱：\n1. 看不出客人意愿支付价格的分布\n2. 如果有 200 个客人到店，对应价格的冰酒又会有多少人买？\n\n## 需求曲线\n\n为了处理上面提出的两个问题，我把客人按照意愿支付价格从高到低来了个快速排序，然后把柱状图连成了一条曲线：\n\n[ [图：快速排序，意愿支付价格从高到低，然后连成曲线，最后还是有冰酒价格横线]]\n\n我们能看到，代表冰酒价格（市场价格）的横线与曲线形成了一个交点。交点左边的客人都会选择买冰酒，而右边的人都会选择不买。\n冰酒价格下降，交点右移，选择购买冰酒的客人就会变多；冰酒价格上升，交点左移，购买的人就会变少。因此交点的横坐标就是购买冰酒的人数，也就是冰酒交易量。\n\n假设每个客人只会买一瓶冰酒，那么交点的横坐标同时也就是冰酒的需求量（实际上有客人不止买一瓶冰酒也没关系，我们可以当是来了两个客人）。而交点的纵坐标当然就是冰酒的价格。\n冰酒价格变化，交点位置也会变化，对应需求量也跟随发生变化。这条曲线描绘的就是冰酒需求量随冰酒价格变化的关系。\n\n[ [图：需求曲线与价格的交点，纵坐标横坐标解释，价格变动后，纵坐标与横坐标变化解释，交点连续变为曲线]]\n\n我们称这条曲线为冰酒的需求曲线。\n\n> **需求曲线**（ Demand Curve ）：表示一种物品价格与需求量之间关系的图形 —— 曼昆《经济学原理：微观经济学分册》: Page 68\n\n像这样用需求曲线表示价格与需求量的关系，可以解决上面的两个问题：\n1. 客人意愿支付价格的分布就是需求曲线的形状（虽然我们为了简化只画直线，但其实曲线形状也是可以上凸下凹，甚至是S形的）\n2. 如果客人数量翻倍，我们一般认为新来的 100 人意愿支付价格分布跟原先 100 人的分布几乎相同，因此需求曲线形状不变，横坐标轴缩短一半（或者说图形横向拉伸一倍）就是我们要的结果了。\n\n值得一提的是，数学中我们常把横坐标当作自变量，而纵坐标表示因变量。但需求曲线中正相反，纵坐标的价格是自变量，需求量才是因变量。\n我记得高中老师一般都会说这是因为经济学家不懂数学，然后草草带过。但实际上，消费者意愿支付多少钱容易统计，而不同价格下到底有多少人会想买难以统计。通过统计意愿支付价格并排序生成需求曲线时，将价格放在纵轴是一种很合理的选择。马歇尔当初也是在这一框架下推导出需求曲线的，曼昆也在他的[这篇博客](http://gregmankiw.blogspot.com/2006/09/who-invented-supply-and-demand.html)中对此有过讨论。\n（实际上，马歇尔是从效用理论推导出需求曲线的，与我们上面推导的过程不一样，但总的来说还是在同一框架下。说马歇尔不懂数学，就像是在说薛定谔不懂数学——怎么可能嘛。）\n\n\n\n## 市场上所有的消费者剩余\n\n店里来了这么多人，每个人意愿支付价格都不一样，那每个人买到同样价格的冰酒，感觉赚到的程度肯定是不一样的。\n\n[ [图：柱状图，展示个人的消费者剩余，然后到线图，展示面积，即市场中的消费者剩余]]\n\n对于单个人来说，他的消费者剩余就是意愿支付价格减去商品价格，也就是柱子在价格线以上标橙色的部分。\n\n可现在到店里的不止一个人啊，我要算所有消费者一共感觉赚到了多少。那我就要把所有橙色部分加起来，也就是做了一个积分，积分的结果就是到店里所有人通过买冰酒这件事一共能赚多少了。大家别看到积分就怕，其实意思就是求需求曲线以下，价格线以上这一三角形的面积。\n（每个人买冰酒价格肯定是固定的。总不能对不同的人以不同的价格出售吧）\n\n值得一提的是，在价格线与需求曲线交点右边的这些人，是不算消费者剩余的，也不会使总的消费者剩余减少。因为他们嫌冰酒太贵（高于意愿支付价格），根本就没有买冰酒（达成交易）。\n\n\n# 总结一下\n\n",
        "title": "图解经济学原理(1)",
        "abstract": "> 1. 这篇文章参考了曼昆的《经济学原理》与北京大学王辉老师的《微观经济学》课程，内容上会有部分相似。\n> 2. 这篇文章中的图使用 3Blue1Brown 的动画生成工具 manim 的 Community Edition 制作，源代码之后会上传到 GitHub 。\n我们先不讲课，先来带个货。",
        "length": 139,
        "created_at": "2022-06-28T00:59:41.000Z",
        "updated_at": "2022-06-28T14:24:43.000Z",
        "tags": [],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-07-19-graph-for-economics-2.md",
        "pagePath": "/articles/graph-for-economics-2",
        "slug": "graph-for-economics-2"
      },
      "meta": {
        "content": "\n> 1. 这篇文章参考了曼昆的《经济学原理》与北京大学王辉老师的《微观经济学》课程，内容上会有部分相似。\n> 2. 这篇文章中的图使用 3Blue1Brown 的动画生成工具 manim 的 Community Edition 制作，源代码之后会上传到 GitHub 。\n\n上一篇讲供给，这一篇讲需求。\n其实供给与需求有很多相似的地方，有时只需要套一下上一篇中给出的模型就能求解。因此各位如果还没有看过上一篇，可以先看完再回来看这一篇内容也不迟。\n\n讲需求曲线的时候，我们会先假设消费者是一个理性人，做决策时会将成本与收益作比较。比如在买冰酒这个场景，我们就会比较冰酒的价格与我们喝冰酒爽到的满足感（意愿支付价格），如果冰酒价格低于意愿支付价格我们就会去买这瓶冰酒。\n\n而假如我们是葡萄庄园主，我们当然也会去比较各种收益与成本，来决定是否制作冰酒拿出去卖。\n\n当我们选择制作冰酒拿出去卖，那收益自然就是卖冰酒所能拿到的钱，也就是冰酒的价格。\n\n但选择制作冰酒拿出去卖的成本呢？\n\n# 机会成本\n\n有一般会计常识的人可能会很快答出：成本不就是买种子、种葡萄、酿酒等过程中花掉的钱嘛！\n\n但实际上问题没有这么简单。因为我们的决策是“制作冰酒拿出去卖”，所以我们必须计算做出决策和不做出决策两种情况之间的差别。\n如果决定不制作冰酒拿出去卖，那我们可以省下一大笔时间与资金。我们可以拿着这些时间与资金去做其他事情，比如可以去种西瓜卖，可以去投资，甚至可以去打工当码农。这些活动都可以获得收益。\n而如果决定去制作冰酒拿出去卖，就代表你为了获得卖冰酒的收益，要选择放弃上面这些活动中获得的收益。“放弃获得这些收益的机会”也是你卖冰酒的成本。\n\n为了与常识中所说的成本做区别，我们把这种算上放弃收益机会的成本称为**机会成本**。经济学中常说的成本的也是机会成本。\n\n> **机会成本**（ Opportunity cost ）：是指为了得到某种东西所必须放弃的东西。 _ 曼昆《经济学原理：微观经济学分册》：Page 52\n\n## 生产者的理性人决策模型\n\n还记得理性人决策的模型吗？不记得的话可以先看看上一篇。\n引入了机会成本这一概念，我们套模型的三个要素就都准备好了：\n- 做决策 = 生产冰酒拿出去卖\n- 收益 = 冰酒价格（卖冰酒拿到的钱）\n- 成本 = 机会成本\n\n那么我们就有：\n- if 冰酒价格 > 机会成本 : 生产冰酒拿出去卖\n- if 冰酒价格 < 机会成本 : 不生产冰酒\n\n[ [图片：机会成本柱，价格线，线高于柱做决策，线低于柱不做决策]]\n\n假设我们是葡萄庄园主，而且已经能预知冰酒市场价稳定在每瓶 50 元。如果我们荒废掉庄园拿钱去投资，就算减去掉投资风险，赚的钱都比卖冰酒赚的钱多得多，那我们就会毫不犹豫地荒废掉庄园选择躺着赚钱。这种其实就是机会成本高于交易收益的情况。\n\n## 生产者剩余\n\n如果收益高于机会成本，那我们就会毫不犹豫地选择生产冰酒拿出去卖。为啥？因为能赚钱呀！赚钱嘛，不寒掺。\n\n“赚钱”，可能就是生产者剩余最贴切地解释了。因为卖冰酒获得的收益（冰酒价格），比制作冰酒的成本（制作冰酒所放弃的其他收益加上制作冰酒耗费的金钱，也就是机会成本）更多，我们称在卖出冰酒的过程中我们获得了生产者剩余。\n\n> **生产者剩余**（ producer surplus ）：卖者出售一种商品得到的量减去其生产成本。Page 140\n\n与消费者剩余类似，生产者剩余计算上表示为：\n$$\n生产者剩余 = 卖出商品得到的量 - 卖出商品所支付的金钱\n$$\n表示为图的话就是商品价格与成本之间的部分：\n[ [图片：生产者剩余柱，价格线，线与柱之间的部分]]\n\n## 供给曲线\n\n与上一篇里消费者情况类似，市场中的生产者也不会只有我们一个。我们把市场中（可能）卖冰酒的人全部抓过来审问一遍，统计一下他们卖冰酒的机会成本，从低到高排个序后就得到下面的图：\n\n[ [图：机会成本柱状图，从低到高连成曲线，冰酒价格横线，线高于柱做决策，线低于柱不做决策]]\n\n与上一篇需求曲线过程类似，把各人机会成本连成曲线，我们就得到了冰酒市场中的供给曲线。\n\n供给曲线与冰酒价格交点的左边，由于这些生产者机会成本小于商品价格，能获得生产者剩余，他们就会选择制作并卖出冰酒（进入市场）。假设他们全部都能卖出冰酒，那他们卖出冰酒的量就是并就的交易量，他们的生产者剩余总和，也就是需求曲线以上价格线以下的部分，就是冰酒市场中总的生产者剩余。\n\n[ [图：需求曲线与价格的交点，纵坐标横坐标解释，价格变动后，纵坐标与横坐标变化解释，交点连续变为曲线]]\n\n与上一篇同理，由于机会成本比较高，处于供给曲线与价格线交点右方的那些人不会选择制作冰酒，因此他们并不会在冰酒市场获得或失去生产者剩余。\n\n# 均衡\n\n上面分析供给曲线，包括上一篇中分析需求曲线时，我们都是先假设先有一个价格，然后再分析如果价格高了会怎么样，如果价格低了会怎么样。\n\n可是这个价格是谁来定的？\n\n## 完全竞争市场\n\n为了分析这个问题，我们需要引入除理性人假设外第二个假设：完全竞争市场假设：\n\n我们假设市场是完全竞争的，这样的市场必须具有三个特征：\n1. 消费者能自由选择购买任一生产者的商品\n2. 市场中的商品都是完全相同的\n3. 买卖双方都人数众多\n\n在这种假设下，市场中所有商品价格都相等，且没有任何一个消费者或生产者能够影响市场价格。因为如果有一个生产者的商品价格高于市场价，消费者们就会到别的地方购买；而由于他们都是理性人，没有生产者会打算以低于市场价的价格出售商品。消费者角度也同理：没有理由用高价买商品，而低价将买不到商品。\n\n这时，我们就可以把需求曲线与供给曲线放在一起分析，由于完全竞争市场中：\n$$\n任意消费者购买商品的价格 = 任意生产者出售商品的价格\n$$\n因此市场中商品价格是固定值，是水平于供给量/需求量的横线。\n[ [图：需求曲线与供给曲线，价格横线只有一条，高于交点时与低于交点时]]\n\n另外，我们看到供给曲线与需求曲线之间有一个交点。接下来我们就要针对这个交点，解决完全竞争市场中价格由谁来定的问题。\n\n## 市场趋向于均衡\n\n首先我们考虑如果价格高于交点时的情况。\n\n前面我们说过，只有价格线与需求曲线交点左边的消费者会购买商品，而只有价格线与供给曲线交点左边的生产者会生产并出售商品。因此我们可以直观地知道，这时商品供给量比需求量要多。那多出来的那一部分商品一定会卖不出去。\n\n卖不出去咋办呀？那就只能降价。\n之前我们说完全竞争市场中生产者没有低价出售商品的理由，那是建立在商品都能卖出去前提下的。商品都能卖出去时没有道理自损利益，但现在商品卖不出去就只能降价吸引客流了。（其实提升商品质量也增加售出量是一种好方法，但我们这里假设了是完全竞争市场，所有商品都完全相同）\n\n消费者们也都是理性人，既然商品完全相同，自然就会选择购买更低价的商品。原本还凑合着能卖出去的那部分商品反而因为未降价变得卖不出去了，自然他们也会选择降价，最终市场中商品的价格整体降低。\n\n市场价降低，使得一部分生产者的机会成本高过了收益，这一部分生产者就会选择离开，使得供给量下降。另一方面，降价使得价格低过了一部分潜在消费者的意愿支付价格，这一部分人就会选择购买商品，使得需求量上升。\n\n[ [图：均衡P81(Eng P77)，价格高于均衡的情况]]\n\n而另一种情况，也相类似。如果价格低于交点，市场中商品的需求量就会大于供给量。这时必然会有一部分人想买但是买不到商品，他们就会逐渐选择用更高的价格来购买，最终拉高整个市场中的商品价格。市场价升高，使得供给量上升，需求量下降。\n\n[ [图：价格低于均衡的情况]]\n\n价格高于交点时会趋于降价，而价格低于交点时会趋于涨价。在充分选择的情况下，最终市场价会等于交点处商品价格。这时市场达到均衡，生产者生产出的所有商品都能卖出，所有消费者都能购买到他们所需的商品。\n\n## 市场效率与福利\n\n之前我们提过消费者剩余与生产者剩余的概念。\n\n消费者剩余 = 意愿支付价格 - 商品价格，如果一个消费者在一场交易中消费者剩余越大，他就感觉越赚，他就对这场交易越满意。对于市场中所有消费者都是如此，因此市场中所有的消费者剩余，也就是需求曲线以下价格线以上的面积，代表了市场中所有消费者对市场交易的满意度。\n\n同样的，生产者剩余 = 商品价格 - 机会成本。市场中所有的生产者剩余，就是价格曲线以下供给曲线以上的部分，代表了市场中所有生产者对市场交易的满意度。\n\n因此，市场中总的生产者剩余加上总的消费者剩余，代表了市场中所有人对市场交易的满意度。我们称这就是市场中的总剩余。\n\n在市场均衡情况下，我们很容易地就能知道总剩余是多少。由于市场均衡时市场中所有的生产者与所有的消费者都能达成交易，而交易价格就是均衡价格。因此我们很容易地就能在图中找到代表生产者剩余、消费者剩余与总剩余的面积。\n\n[ [图]]\n\n而如果市场没有达到均衡，会出现有的消费者没能买到商品、或是生产者的商品没能卖出去的情况。这些时候，没能买到商品的消费者与没能卖出商品的生产者自然不会对交易满意（因为没能达成交易），自然也不计算剩余。而市场中商品的交易量取决于需求量与供给量中更小的一方。无论如何，总剩余总会小于均衡时的总剩余。\n\n[ [图]]\n\n由此可以看出，只有当市场达到均衡的时候，加入市场的所有人对市场交易的满意度最大。而市场均衡是完全竞争的自由市场中会自发达到的状态。\n\n因此，从经济学的观点来看，在所有人都是理性人、市场是完全竞争市场的假设下，不需要任何外加的制度或政策，市场就会自发地达到人们满意度最高的状态。也正因如此，亚当·斯密才会说市场是一个看不见的手。\n\n# 总结一下\n\n\n\n\n\n\n\nneeded:\n- [ ] svg character\n- [ ] bar graph\n- [ ] function graph\n- [ ] manim command for output path and input path https://docs.manim.community/en/stable/tutorials/configuration.html\n- [ ] want mp4 as picture\n- [ ] interactive manim https://github.com/3b1b/3Blue1Brown.com/tree/main/public/content/lessons/2021/newtons-fractal\n\n\n# 税收，污染权与外部性，国际贸易\n\n※\n# 比较优势\n# 弹性与均衡移动，收益分析 \n# 生产成本，垄断，寡头\n# 生产要素市场\n\n宏观：\n重点：\n1. 三个指标：GDP，价格水平，就业 =》 促使政府调节经济\n2. 两种政策：\n    1. 货币政策 =》 名义利率，量化宽松，前瞻指引，汇率决定制度等\n    2. 财政政策 =》 加息、采购\n3. 两种政策对经济影响（总供给总需求模型），以及国际经济与贸易影响\n\n# 三个指标与两种政策\n## 三个指标\n## 经济增长的原因 —— 全要素生产率\n\n## 经济波动\n## 凯恩斯主义 —— 促进政府调节经济\n## 调节经济两种政策\n# 财政政策\n# 货币政策\n\n## 两种政策对经济影响 —— 总供给总需求模型\n## 国际经济\n\n",
        "title": "图解经济学原理(2)",
        "abstract": "> 1. 这篇文章参考了曼昆的《经济学原理》与北京大学王辉老师的《微观经济学》课程，内容上会有部分相似。\n> 2. 这篇文章中的图使用 3Blue1Brown 的动画生成工具 manim 的 Community Edition 制作，源代码之后会上传到 GitHub 。\n上一篇讲供给，这一篇讲需求。",
        "length": 188,
        "created_at": "2022-07-19T23:12:48.000Z",
        "updated_at": "2022-08-13T09:53:03.000Z",
        "tags": [],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-07-31-why-homogeneous.md",
        "pagePath": "/articles/why-homogeneous",
        "slug": "why-homogeneous"
      },
      "meta": {
        "content": "## 首先，什么是线性变换？\n\n简化了一万倍来说，线性变换主要是在描述符合这两种性质的变换：一是要可加，二是要能数乘。\n也就是说，对于空间中所有向量 $$\\vec{v_1}, \\vec{v_2}$$ ，以及任意数量 $$k_1, k_2$$ ，如果有：\n$$\nA(k_1 \\vec{v_1} + k_2 \\vec{v_2}) = k_1 A(\\vec{v_1}) + k_2 A(\\vec{v_2})\n$$\n符合这种规律的 A 就叫线性变换。而一次矩阵乘法正好可以代表一次线性变换。\n\n为什么叫“线性”变换呢？感性地来说，因为它很“线”。\n\n我们可以直观地从下面这张图看出原因：\n\n![[OnOneLineWillStillOneLine_ManimCE_v0.16.0.post0.gif]]\n\n我们可以看到，在同一直线上的点，经过同一线性变换后还在同一直线上。所以它很“线”。\n\n另一方面，我们可以找一找最简单的线性变换：\n\n考虑函数：\n$$\nf(x) = k_0 x\n$$\n我们都知道这是一条过原点的直线。\n\n而从另一方面想，其实这个函数对于任意一维向量（实数） $$x_1, x_2$$ , 与任意数量（实数） $$k_1, k_2$$ , 都有：\n$$\nf(k_1 x_1 + k_2 x_2) = k_1 k_0 x_1 + k_2 k_0 x_2 = k_1 f(x_1) + k_2 f(x_2) \\\\\n$$\n\n即， xy 平面上过原点的直线（正比例函数）本身就是一种从 x 轴到 y 轴的线性变换。\n\n关于线性变换， [3blue1Brown](https://www.3blue1brown.com/topics/linear-algebra) 上有更详细更感性的介绍，大家感兴趣可以前往观看。\n\n## 为什么普通的线性变换不能表示点平移？\n\n从上面的感性介绍来看，我们知道线性变换的性质就是可加和数乘，写成等式就是：\n\n$$\nA(k_1 \\vec{v_1} + k_2 \\vec{v_2}) = k_1 A(\\vec{v_1}) + k_2 A(\\vec{v_2})\n$$\n\n而当两个向量都为零向量时，等式就会简化成：\n\n$$\nA(\\vec{0}) = A(\\vec{0}) + A(\\vec{0})\n$$\n\n解一下方程，就可以知道，对任意线性变换 A，都会有：\n\n$$\nA(\\vec{0}) = 0\n$$\n\n也就是说，不管是哪个线性变换 A ，原点经过变换后都必须只能是在原点不变。如果变换后原点的位置变了，那它就一定不是线性变换。\n\n我们从下图也可以看出，对于切变 $$\\begin{pmatrix}1 & 1 \\\\ 0 & 1\\end{pmatrix}$$ 、伸缩 $$ \\begin{pmatrix}2 & 0 \\\\ 0 & \\frac{1}{2}\\end{pmatrix} $$、旋转 $$ \\begin{pmatrix}\n    \\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{\\sqrt{3}}{2}\n\\end{pmatrix} $$ 这些经典的线性变换，变换后原点都不会变。\n\n![[SliceScaleRotateForOrigin_ManimCE_v0.16.0.post0.gif]]\n\n但是平移这种变换不一样。原点经过平移后，是一定不会还留在原点的。因此平移不是一种线性变换，自然也不能用矩阵来表示。\n\n## 为什么基于齐次坐标下的线性变换就可以表示平移？\n\n我们先来看一下齐次坐标做了些什么。\n\n在上面传统的线性变换中，我们不会考虑向量与点的区别。一个二维坐标 $$(x, y)$$ 既能代表那个坐标上的点，也能代表从原点到 $$(x, y)$$ 的向量。这时，点与向量是一一对应的。\n\n但如果要考虑平移，点与向量就不能再一一对应了，因为对向量平移没有意义（不考虑物理中力矩的场景）。\n所以在齐次坐标下，我们需要区分这个坐标代表的是点还是向量。\n\n以二维空间为例，齐次坐标就是在二维空间上加了第三个维度 w 轴，二维空间里的点在 w 轴上的值为 1 ，而二维向量在 w 轴上的值对应为 0 ：\n\n$$\n\\begin{align}\n    P &= \\begin{pmatrix}x & y & 1\\end{pmatrix} \\\\\n    \\vec{v} &= \\begin{pmatrix}v_x & v_y & 0\\end{pmatrix}\n\\end{align}\n$$\n\n从字面上看可能还是不太明显，让我们试着把二维空间齐次坐标强行转化为三维空间坐标看看：\n\n![[HomogeneousTransform_ManimCE_v0.16.0.post0.gif]]\n\n我们发现，原来二维空间中的点，被投射到三维空间中 w = 1 的平面上了！\n\n这样一来，二维空间齐次坐标下的平移矩阵也很好理解了：\n\n$$\n将平面沿向量 (x, y) 平移：\n\\begin{pmatrix}\n    1 & 0 & x \\\\\n    0 & 1 & y \\\\\n    0 & 0 & 1\n\\end{pmatrix}\n$$\n\n这不就是三维空间中在 w 轴上做切变时的变换矩阵嘛！\n\n我们可以重点关注一下 $$\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}$$ 这个向量。\n从齐次坐标的定义来看，这个向量对应着二维空间中的原点 $$P_{Origin} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$ 。而由矩阵乘法计算可知，经过 $$ A = \\begin{pmatrix} 1 & 0 & x \\\\ 0 & 1 & y \\\\ 0 & 0 & 1 \\end{pmatrix} $$ 对应的线性变换后， $$ \\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix} $$ 这个向量会被映射到 $$ \\begin{pmatrix}x\\\\y\\\\1\\end{pmatrix} $$ 上。也就是说，二维空间原点 $$ P_{Origin} = \\begin{pmatrix}0\\\\0\\end{pmatrix}$$ 经过变换后会变为 $$ P_{Origin}' = A(P_{Origin}) = \\begin{pmatrix}x\\\\y\\end{pmatrix}$$ 。\n\n而对于二维空间中的向量 $$\\vec{v}=\\begin{pmatrix}v_x\\\\v_y\\end{pmatrix}$$ ，其齐次坐标下 w 轴方向分量为 0 ，因此 w 轴方向上的切变并不会影响二维空间中的向量。即 $$ \\vec{v'} = A(\\vec{v}) = \\vec{v} $$ 。\n\n而对于原来二维空间中的其他点的坐标：\n$$\nP = \\begin{pmatrix}x_0\\\\y_0\\end{pmatrix}\n$$ \n其实可以理解为原点坐标再加上一个偏移向量：\n$$\nP = \\begin{pmatrix}0\\\\0\\end{pmatrix} + \\begin{pmatrix}x_0\\\\y_0\\end{pmatrix} = P_{Origin} + \\vec{v}_{x,y}\n$$\n\n而在齐次坐标下，点坐标 = 原点坐标 + 偏移向量 这一等式仍然成立：\n$$\nP =  \\begin{pmatrix}x_0\\\\y_0\\\\1\\end{pmatrix} = \\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix} + \\begin{pmatrix}x_0\\\\y_0\\\\0\\end{pmatrix} = P_{Origin} + \\vec{v}_{x,y}\n$$\n\n而由于切变是线性变换，因此有：\n\n$$\n\\begin{align}\nP' &= A(P) \\\\\n&= A(P_{Origin} + \\vec{v}_{x,y}) \\\\\n&= A(P_{Origin}) + A(\\vec{v}_{x,y}) \\\\\n&= P_{Origin}' + \\vec{v}_{x,y} \\\\\n\\end{align}\n$$\n\n因为切变前后偏移向量没有发生变化，因此二维空间上的点经变换后相对于原点的方向、距离都没有发生变化。由此也可得出，原先由二维空间中的点组成的图案，经齐次坐标下 w 轴的切变后，其大小、形状、方向都不会发生变化。\n\n![[SliceOnHomogeneousWithGraph_ManimCE_v0.16.0.post0.gif]]\n\n而这种大小、形状、方向都不变化，只有整体位置发生了变化的变换，正是我们一般所说的“平移”。因此在齐次坐标下，我们能通过线性变换（aka 矩阵乘法）表示平移。\n\n> 其实 $$\\begin{pmatrix}1 & 0 & x \\\\0 & 1 & y \\\\0 & 0 & 1\\end{pmatrix}$$ 对应切变作用后各点坐标如何变化这个过程， 3Blue1Brown 的[这个视频](https://www.3blue1brown.com/lessons/matrix-multiplication) 有更直观明了的解释，大家可以参考。\n\n## 总结一下\n\nQ: 为什么普通的矩阵乘法不能表示平移？\nA: 因为矩阵乘法只能表示线性变换。平移不是线性变换。\n\nQ: 为什么在齐次坐标下的矩阵乘法又能表示平移？\nA: 因为齐次坐标增加了一个维度。平移变换矩阵其实是在新增的这个维度上做切变（一种线性变换）。切变后的结果正好就是原坐标中的平移变换。\n\n\n",
        "title": "为什么使用在齐次坐标下矩阵乘法能表示点平移？",
        "abstract": "简化了一万倍来说，线性变换主要是在描述符合这两种性质的变换：一是要可加，二是要能数乘。\n也就是说，对于空间中所有向量 $$\\vec{v_1}, \\vec{v_2}$$ ，以及任意数量 $$k_1, k_2$$ ，如果有：\n$$",
        "length": 149,
        "created_at": "2022-07-31T15:35:17.000Z",
        "updated_at": "2022-08-05T17:45:09.000Z",
        "tags": [],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-08-13-introduction-for-k8s.md",
        "pagePath": "/articles/introduction-for-k8s",
        "slug": "introduction-for-k8s"
      },
      "meta": {
        "content": "\n# 容器， Docker 与 K8s\n\n我们知道 K8s 利用了容器虚拟化技术。而说到容器虚拟化就要说 Docker 。可是，容器到底是什么？ Docker 又为我们做了些什么？我们又为什么要用 K8s ？\n\n### 关于容器虚拟化\n\n> 要把一个不知道打过多少个升级补丁，不知道经历了多少任管理员的系统迁移到其他机器上，毫无疑问会是一场灾难。 —— Chad Fowler 《Trash Your Servers and Burn Your Code》\n\n\"Write once, run anywhere\" 是 Java 曾经的口号。 Java 企图通过 JVM 虚拟机来实现一个可执行程序在多平台间的移植性。但我们现在知道， Java 语言并没能实现他的目标，会在操作系统调用、第三方依赖丢失、两个程序间依赖的冲突等各方面出现问题。\n\n要保证程序拉下来就能跑，最好的方法就是把程序和依赖打包到一起，然后将外部环境隔离起来。容器虚拟化技术就是为了解决这个。\n\n与常说的虚拟机不同， Docker 等各类容器是用隔离名称空间的方式进行资源隔离的。 Linux 系统的内核直接提供了名称空间隔离的能力，是针对进程设计的访问隔离机制，可以进行一些资源封装。\n\n| 名称空间     | 隔离内容                      | 内核版本 |\n| :----------- | :---------------------------- | :------- |\n| Mount        | 文件系统与路径等              | 2.4.19   |\n| UTS          | 主机的Hostname、Domain names  | 2.6.19   |\n| IPC          | 进程间通信管道                | 2.6.19   |\n| PID          | 独立的进程编号空间            | 2.6.24   |\n| Network      | 网卡、IP 地址、端口等网络资源 | 2.6.29   |\n| User         | 进程独立的用户和用户组        | 3.8      |\n| Cgroup       | CPU 时间片，内存分页等        | 4.6      |\n| Time \\<- New! | 进程独立的系统时间            | 5.6      |\n\n值得注目的是， Linux 系统提供了 Cgroup 名称空间隔离的支持。通过隔离 Cgroup ，可以给单独一个进程分配 CPU 占用比率、内存大小、外设 I/O 访问权限等。再配合 IPC 、 PID 等的隔离，可以让被隔离的进程看不到同一实体机中其他进程的信息，就像是独享一整台机器一样。\n\n由于容器虚拟化技术直接利用了宿主机操作系统内核，因此远远要比虚拟机更轻量，也更适合用来给单个程序进行隔离。但也同样由于依赖了宿主机内核，在不同的架构、不同种类的操作系统间容器可能不能移植。\n\n### 关于 Docker\n\n在介绍 K8s 之前，我们要先搞清楚 Docker 是什么。或者说，我们平时说的“ Docker ”是什么？\n\n我们平时说的 Docker ，可能是以下几个东西：\n\n- Docker Engine: 在宿主机上跑的一个进程，专门用来管理各个容器的生命周期、网络连接等，还暴露出一些 API 供外部调用。有时会被称为 Docker Daemon 或是 dockerd 。\n- Docker Client: 命令行中的 `docker` 命令，其实只会跟 Docker Server 通信，不会直接创建销毁一个容器进程。\n- Docker Container: 宿主机上运行的一组被资源隔离的进程，在容器中看来像是独占了一台虚拟的机器，不需要考虑外部依赖。\n- Docker Image: 是一个打包好的文件系统，可以从一个 Image 运行出复数个 Container 。 Image 内部包含了程序运行所需的所有文件、库依赖，以及运行时的环境变量等。\n- Docker 容器运行时: 是 Docker Engine 中专门管理容器状态、生命周期等的那个组件，原来名为 libcontainer 。[《开放容器交互标准》](https://en.wikipedia.org/wiki/Open_Container_Initiative)制定后， Docker 公司将此部分重构为 [runC 项目](https://github.com/opencontainers/runc)，交给 Linux 基金会管理。而 Docker Engine 中与运行时进行交互的部分则抽象出来成为 [containerd 项目](https://containerd.io/)，捐献给了 CNCF 。\n\n我们平时在 linux 机上运行 `yum install docker` 之类的命令，安装的其实是 Docker Engine + Docker Client 。（而在 Windows 或 MacOS 上安装的 Docker Desktop 其实是一个定制过的 linux 虚拟机。）下面说的 Docker 的功能其实都是指 Docker Engine 的功能。\n\n而 Docker 提供给我们的功能，除了最基础的运行和销毁容器外，还包括了一些容器网络编排、重启策略、文件路径映射、端口映射等功能。\n\n而我认为 Docker 最大的贡献，还是容器的镜像与镜像仓库。有了镜像与镜像仓库，人们就可以把自己的程序与执行环境直接打包成镜像发布，也可以直接拿打包好的镜像来运行容器进行部署，而不需要额外下载或是安装一些东西，也不需要担心程序会与已经跑起来的其他程序冲突。\n\n### 为什么要用 K8s ？\n\n其实 Docker 有一个很强大的工具叫 docker-compose ，可以通过一个 manifest 对多个容器组成的网络进行编排。那为什么我们还需要 K8s 呢？换句话说，有什么事是 Docker 不能做的？而 K8s 设计出来的目标是为了解决什么问题？\n\n首先， Docker 做不到以下的功能：\n\n1. **Docker 不能做跨多主机的容器编排。** docker-compose 再方便，他也只能编排单台主机上的容器。对跨主机的集群编排无能为力。（实际上，用了 Docker-Swarm 后是可以多主机编排的，但一来 Docker-Swarm 出现的比 K8s 晚，而来 Docker-Swarm 功能不如 K8s ，因此用的人很少，我们下面就默认 Docker-Swarm 不存在了。）\n2. **Docker 提供的容器部署管理功能不够丰富。** Docker 有一些简单的容器重启策略，但也只是简单的失败后重启之类的，没有完整的应用状态检查等功能。同时，版本升级、缩扩容等策略选择的余地也不多。\n3. **Docker 缺乏高级网络功能。** 要让 Docker 的容器间进行网络通信，也只能是说把容器放到同一个网络下，然后再通过各自的 Hostname 来找到对方。但实际上，我们更会想要一些负载均衡、自定义域名、选择某些容器端口不暴露之类的功能。\n\nand more...\n\n总的来说， Docker 更关注单台主机上容器怎么跑，而对部署管理的功能则支持不多。而最大的痛点，就是 Docker 对多主机的集群部署支持的实再很差。然而，为了实现多区可用、负载均衡等功能，多主机集群的容器编排又是必不可少的。\n\nK8s 的出现，主要就是为了解决多主机集群上的容器编排问题。\n\n1. **K8s 可以进行多主机调度。** 用户只需要描述自己需要运行怎样的应用， K8s 就可以自己选择一个合适的节点进行部署，用户不需要关心自己的应用部署到哪个节点上。\n2. **K8s 中一切皆资源。** K8s 有完善的抽象资源机制，用户几乎不需要知道磁盘、网络等任何硬件信息，只需要对着统一的抽象资源进行操作。\n3. **K8s 能保证较强的可用性。** 除了能跨多主机调度实现多区可用外， K8s 还提供了很完善的缩扩容机制、健康检查机制以及自动恢复机制。\n\n可以说， K8s 是容器编排工具的主流选择。\n\n### K8s 与 Docker 的关系\n\nK8s 与 Docker 关系很复杂，是一个逐渐变化的过程。\n\n一开始 K8s 是完全依赖于 Docker Engine 进行容器启动与销毁的。后来[容器运行时接口（CRI）](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)、 [CRI-O 标准](https://github.com/cri-o/cri-o)、开放容器交互标准（OCI）等标准逐渐建立，可替代 Docker Engine 的工具越来越多， K8s 中已经完全可以不使用 Docker Engine 了。\n\n[《凤凰架构》](http://icyfenix.cn/)一书中有下面这样一张图来描述 K8s 与 Docker Engine 的关系：\n\n![K8s 与 Docker Engine 的关系](http://icyfenix.cn/assets/img/kubernetes.495f9eae.png)\n\n《凤凰架构》书中[这一章节](http://icyfenix.cn/immutable-infrastructure/container/history.html#%E5%B0%81%E8%A3%85%E9%9B%86%E7%BE%A4%EF%BC%9Akubernetes)详细介绍了 K8s 与 Docker 的历史，我这里就不再赘述。\n\n# 部署一个 Pod\n\n上面说了一堆概念，我们接下来实际上会怎样应用 K8s 。\n\n### Pod 示例\n\n> Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。\n> Pod 是一组容器；Pod 中的内容总是一同调度，在共享的上下文中运行。 Pod 中包含一个或多个应用容器，这些容器相对紧密地耦合在一起。在非云环境中，在相同的物理机或虚拟机上运行的应用类似于在同一逻辑主机上运行的云应用。\n> —— Kubernetes 官方文档\n\nPod 是 K8s 的最小部署单位。\n\n因为 K8s 将硬件资源都抽象化了，用户不需要知道自己的应用部署到哪台机上。但是有些场景下两个主进程之间又必须相互协作才能完成任务，如果两个进程不确定会不会部署到同一个节点上会变得很麻烦。因此才需要 Pod 这种资源。\n\n下面是一个 Nginx Pod 的示例（这是 K8s manifest 文件，可以用 `kubectl apply -f <filepath>` 进行部署）：\n\n```yaml\nmetadata:\n  name: simple-webapp\nspec:\n  containers:\n    - name: main-application\n      image: nginx\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n    - name: sidecar-container\n      image: busybox\n      command: [\"sh\",\"-c\",\"while true; do cat /var/log/nginx/access.log; sleep 30; done\"]\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n  volumes:\n    - name: shared-logs\n      emptyDir: {}\n```\n\n可以看到， Pod 中可以包含多个容器，这组容器总是以一定的逻辑一起部署，且总是部署在同一个节点。对 K8s 操作时，不能说只部署 Pod 中一个特定的容器，也不能说把 Pod 中一个容器部署在这个节点，另一个容器部署在另一个节点上。\n\n在上面这个例子中，我们看到 Pod 中除了 Nginx 容器以外还有一个 Sidecar 容器负责将 Nginx 的 access.log 日志输出到控制台。两个容器可以通过 mount 同一个路径来实现文件共享。这种场景下，单独跑一个 Sidecar 容器没有意义，而我们也不会希望两个容器部署在不同的节点上。 **两个容器同生共死** ，这样的模式被称为 **Sidecar 模式** 。 Jaeger Agent ，或是 Service Mesh 中常见的 Envoy Sidecar 都可以通过这种模式部署，这样业务容器中就可以不考虑 tracing 或是流量控制相关的问题。\n\n此外，由于同一个 Pod 中的容器默认共享了相同的 network 和 UTS 名称空间，不管是在 Pod 的内部还是外部来看，他们一定程度上就像是真的部署在同一主机上一样，有相同的 Hostname 与 ip 地址，在一个容器中也可以通过 localhost 来访问零一个容器的端口。\n\n另外 Pod 中可以定义若干个 initContainer ，这些容器会比 `spec.containers` 中的容器先运行，并且是顺序运行。下面是通过安装 bitnami 的 Kafka Helm Chart 得到的一个 Kafka Broker Pod （有所简化）:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kafka-0\n  namespace: kafka\nspec:\n  containers:\n  - name: kafka\n    image: docker.io/bitnami/kafka:3.1.0-debian-10-r52\n    command:\n    - /scripts/setup.sh\n    volumeMounts:\n    - name: scripts\n      mountPath: /scripts/setup.sh\n      subPath: setup.sh\n    - name: shared\n      mountPath: /shared\n  initContainers:\n  - name: auto-discovery\n    image: docker.io/bitnami/kubectl:1.23.5-debian-10-r1\n    command:\n    - /scripts/auto-discovery.sh\n    volumeMounts:\n    - name: shared\n      mountPath: /shared\n    - name: scripts\n      mountPath: /scripts/auto-discovery.sh\n      subPath: auto-discovery.sh\n  volumes:\n  - name: scripts\n    configMap:\n      defaultMode: 493\n      name: kafka-scripts\n  - name: shared\n    emptyDir: {}\n```\n\n可以看到，在 `kafka` pod 启动前会先启动一个名为 `auto-discovery` 的 initContainer ，负责获得集群信息等准备工作。准备工作完成后，会将信息写入 `/shared` 目录下，然后再启动 `kafka` 容器 Mount 同一目录，就可以获取准备好的信息。\n\n**这样运行容器进行 Pod 初始化就叫 initContainer 模式** 。每个 initContainer 会运行到成功退出为止，如果有一个 initContainer 启动失败，则整个 Pod 启动失败，触发 K8s 的 Pod 重启策略。\n\n\n# 部署更多 Pod\n\n### Replica Set\n\n可是上面说了这么多，还只是单个 Pod 的部署，但我们希望能做多副本部署。\n\n其实，只要把 Pod 的 manifest 改一下 `metadata.name` 再部署一次，就能得到一模一样的两个 Pod ，就是一个简单的多副本部署了。（必须改 `metadata.name` ，不然 K8s 会以为你是想修改同一个 Pod ）\n\n可是这样做会有很多问题：\n\n- 要复制一下还要改名字多麻烦啊，我想用同一份模板，只定义一下副本数就能得到对应数量的 Pod 。\n- 缩容扩容还要对着 Pod 操作很危险，我想直接修改副本数就能缩容扩容。\n- 如果其中一些 Pod 挂掉了不能重启，现在是什么都不会做。我希望能自动建一些新的 Pod 顶上，来保证副本数不变。\n\n为了实现这些需求，就出现了 Replica Set 这种资源。下面是实际应用中一个 Replica Set 的例子：\n\n```yaml\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: gateway\n  name: gateway-9dc546658\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n      name: gateway\n    spec:\n      containers:\n        name: gateway\n        image: xxxxxxxx.amazonaws.com/gateway:xxxxxxx\n        ports:\n        - containerPort: 50051\n          protocol: TCP\n        readinessProbe:\n          initialDelaySeconds: 5\n          tcpSocket:\n            port: 50051\n        startupProbe:\n          failureThreshold: 60\n          tcpSocket:\n            port: 50051\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: gateway\n              topologyKey: topology.kubernetes.io/zone\n            weight: 80\n```\n\n我们可以看到， `spec.template` 中就是我们要的 Pod 的模板，在 metadata 里带上了 `app:gateway` 标签。而在 `spec.replicas` 中定义了我们需要的 Pod 数量， `spec.selector` 中描述了我们要对带 `app:gateway` 标签的 Pod 进行控制。把这份 manifest 部署后，我们就会得到除名字以外几乎一摸一样的两个 Pod ：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  generateName: gateway-9dc546658-\n  labels:\n    app: gateway\n    pod-template-hash: 9dc546658\n  name: gateway-9dc546658-6c9qs\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: true\n    controller: true\n    kind: ReplicaSet\n    name: gateway-9dc546658\n    uid: 6633f89c-377c-4c90-bd08-3be5bc7b21bd\n  resourceVersion: \"49793842\"\n  uid: f927db88-a39a-4623-852d-4f150a6d853b\nspec:\n  containers:\n    name: gateway\n    image: xxxxxxxx.amazonaws.com/gateway:xxxxxxx\n    ports:\n    # 后续省略\n\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubernetes.io/psp: eks.privileged\n  creationTimestamp: \"2022-08-09T08:51:25Z\"\n  generateName: gateway-9dc546658-\n  labels:\n    app: gateway\n    pod-template-hash: 9dc546658\n  name: gateway-9dc546658-8trcs\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: true\n    controller: true\n    kind: ReplicaSet\n    name: gateway-9dc546658\n    uid: 6633f89c-377c-4c90-bd08-3be5bc7b21bd\n  resourceVersion: \"49793745\"\n  uid: 0918e3ed-2965-4237-8828-421a7831c9ed\nspec:\n  containers:\n    image: xxxxxxxx.amazonaws.com/gateway:xxxxxxx\n    name: gateway\n    ports:\n    # 后续省略\n```\n\n可以看到，创建出来的 Pod 自动生成了两个后缀（ `6c9qs` 与 `8trcs` ），带上了 Replica Set 的信息（在 `metadata.ownerReferences` ），其他部分基本一模一样。如果其中一个 Pod 挂掉了， K8s 会帮我们从模板中重新创建一个 Pod 。而且由于我们在 Pod 模板定义了 affinity ， K8s 还会按照我们的要求自动筛选合适的节点。例如在上面 Replica Set 的例子中，创建出来的 Pod 就会尽量部署在不同的节点上。\n\n> **K8s 中对 Pod 的生存状态检查机制**\n> \n> 除了线程直接错误退出以外，还有出现死锁等等各种可能性使得容器中的应用不能正常工作。这些情况下虽然是不健康状态，但容器却不一定会挂掉。因此 K8s 提供了一些探针检查的机制来判断 Pod 是否健康。\n> K8s 主要提供了三种探针：\n> 1. **存活探针（ liveness probe ）** : Pod 运行时 K8s 会循环执行 liveness probe 检查容器是否健康。如果检查失败， K8s 会认为这个容器不健康，就会尝试重启容器。\n> 2. **就绪探针（ readiness probe ）** : 程序可能会有一段时间不能提供服务（比如正在加载数据等）。这时可能既不想杀死应用，也不想给它发送请求，这时就需要 readiness probe 。如果 readiness probe 检查失败， K8s 就会将这个 Pod 从 Service 上摘下来，直到 readiness probe 成功重新加入 Service 。\n> 3. **启动探针（ startup probe ）** : 有些程序会有非常长的启动时间，会有较长时间不能提供服务。这时如果 liveness probe 失败了导致重启毫无必要，此时就需要 startup probe 。 startup probe 只会在容器启动时检查直到第一次成功。直到 startup probe 成功为止， liveness probe 与 readiness probe 都不会开始执行检查。\n> \n> 而检测方式主要有：\n> 1. httpGet: 对指定的端口路径执行 HTTP GET 请求，如果返回 2xx 或 3xx 就是成功。\n> 2. tcpSocket: 尝试与容器的端口建立连接，如果不能成功建立连接就是失败。\n> 3. exec: 在容器内执行一段命令，如果退出时状态码不为 0 就是失败。\n> 4. grpc (New!): K8s 1.24 新出的检查方式，直接用 [GRPC Health Checking Protocol](https://github.com/grpc/grpc/blob/master/doc/health-checking.md) 对 GRPC Server 进行检查。\n\n此外， Replica Set 还提供了简易的缩容扩容功能。 kubectl 中提供了 scale 命令：\n\n```bash\nkubectl scale replicaset gateway --replicas=10\n```\n\n执行上述命令，就可以将名为 gateway 的 Replica Set 对应的副本数扩容到 10 份。当然，你也可以直接修改 Replica Set 的 `spec.replicas` 字段来实现缩容扩容。\n\n然而， Replica Set 的功能还是有限的。实际上， Replica Set 只关心跟它的 selector 匹配的 Pod 的数量。而至于匹配的 Pod 是否真的是跟 template 字段中描述的一样， Replica Set 就不关心了。因此如果单用 Replica Set ，更新 Pod 就会变得究极麻烦。\n\n### Deployment\n\n为了解决 Pod 的更新问题，我们需要有 Deployment 这种资源。实际上， Replica Set 的主要用途是提供给 Deployment 作为控制 Pod 数量，以及创建、删除 Pod 的一种机制。我们一般不会直接使用 Replica Set 。\n\n下面是实际应用中一个 Deployment 的例子：\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: gateway\n  name: gateway\nspec:\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: gateway\n  template:\n    metadata:\n      labels:\n        app: gateway\n      name: gateway\n    spec:\n      containers:\n        name: gateway\n        image: xxxxxxxx.amazonaws.com/gateway:xxxxxxx\n        ports:\n        # 下略\n```\n\n可以看到 Deployment 的 manifest 跟 Replica Set 很像。但实际上， Deployment 不会直接创建 Pod ，而是创建出一个 Replica Set ，再由 Replica Set 来创建 Pod ：\n\n\n```mermaid\nflowchart TB\n\nDeployment1[Deployment]\nReplicaSet11[Replica Set]\nPod11[Pod1]\nPod12[Pod2]\nDeployment1 --> ReplicaSet11\nReplicaSet11 --> Pod11\nReplicaSet11 --> Pod12\n```\n\n比如在上面的例子中，名为 gateway 的 Deployment 创建后，就会有如下 ReplicaSet 和 Pod ：\n\n```sh\n# Replica Set:\n$ kubectl get rc -l app=gateway\nNAME                 DESIRED   CURRENT   READY   AGE\ngateway-9dc546658    2         2         2       5d3h\n\n# Pod:\n$ kubectl get po -l app=gateway\nNAME                      READY   STATUS    RESTARTS   AGE\ngateway-9dc546658-6c9qs   1/1     Running   0          5d3h\ngateway-9dc546658-8trcs   1/1     Running   0          5d3h\n```\n\n可以看到，gateway Deployment 创建了一个 Replica Set ，然后随机给了它一个 `9dc546658` 后缀。然后 gateway-9dc546658 这个 Replica Set 又根据 template 中创建了两个 Pod ，再在自己名字的基础上加上两个后缀 `6c9qs` 与 `8trcs` 。\n\n接下来就是 Deployment 的重点了： Replica Set 只会根据 template 创建出 Pod ，而不管匹配的 Pod 到底是不是跟 template 中描述的一样。而 **Deployment 则会专门关注 template 的内容变更。**\n\n假如我们现在更新了 Deployment 的 template 中的内容提交给 K8s ， Deployment 就会感知到 template 被修改了， Pod 需要更新。\n感知到更新之后， Deployment 就会创建一个新的 Replica Set 。然后逐渐将旧的 Replica Set 缩容到 0 ，并同时将新的 Replica Set 扩容到目标值。最后，所有旧版本的 Pod 将会被更新成新版本的 Pod 。如下图所示：\n\n```mermaid\nflowchart TB\n\nsubgraph A\ndirection TB\nDeployment1[Deployment]\nReplicaSet11[Replica Set]\nReplicaSet12[New Replica Set]\nPod11[Pod1]\nPod12[Pod2]\nDeployment1 --> ReplicaSet11\nDeployment1 --> ReplicaSet12\nReplicaSet11 --> Pod11\nReplicaSet11 --> Pod12\nend\n\nsubgraph B\ndirection TB\nDeployment2[Deployment]\nReplicaSet21[Replica Set]\nReplicaSet22[New Replica Set]\nPod21[New Pod1]\nPod22[Pod2]\nDeployment2 --> ReplicaSet21\nDeployment2 --> ReplicaSet22\nReplicaSet21 --> Pod22\nReplicaSet22 --> Pod21\nend\n\nsubgraph C\ndirection TB\nDeployment3[Deployment]\nReplicaSet31[Replica Set]\nReplicaSet32[New Replica Set]\nPod31[New Pod1]\nPod32[New Pod2]\nDeployment3 --> ReplicaSet31\nDeployment3 --> ReplicaSet32\nReplicaSet32 --> Pod31\nReplicaSet32 --> Pod32\nend\n\nA --> B --> C\n```\n\n整个过程完成后， Deployment 还不会将旧的 Replica Set 删除掉。我们注意到 Deployment 的声明中有这么一个字段： `revisionHistoryLimit: 10` ，表示 Deployment 会保留历史中 最近的 10 个 Replica Set ，这样在必要的时候可以立刻将 Deployment 回滚到上个版本。而超出 10 个的 Replica Set 才会被从 K8s 中删除。\n\n```sh\n# 实际中被 scale 到 0 但还没被删除的 Replica Set\n$ kubectl get rs -l app=gateway\nNAME                 DESIRED   CURRENT   READY   AGE\ngateway-5c4cdf957d   0         0         0       5d4h\ngateway-5c56f6d487   0         0         0       17d\ngateway-65857cfc78   0         0         0       10d\ngateway-6bddbdd85f   0         0         0       16d\ngateway-6cc9bb5b4c   0         0         0       13d\ngateway-6f4664bc65   0         0         0       17d\ngateway-7bd667cb79   0         0         0       9d\ngateway-7d658d57f5   0         0         0       13d\ngateway-84df97d4c8   0         0         0       6d4h\ngateway-9998f4689    0         0         0       13d\ngateway-9dc546658    2         2         2       5d4h\n```\n\n### Stateful Set\n\nDeployment 中默认了我们不关心自己访问的是哪个 Pod ，因为各个 Pod 的功能是一样的，访问哪个没有差别。\n\n实际上这也符合大多数情况：试想一个 HTTP Server ，如果其所有数据都存放到同一个的数据库中，那这个 HTTP Server 不管部署在哪台主机、不管有多少个实例、不管你访问的是哪个实例，都察觉不出有什么差别。而有了这种默认，我们就能更放心地对 Pod 进行负载均衡、缩扩容等操作。\n\n但实际上我们总会遇到需要保存自己状态的 Pod 。比如我们在 K8s 里部署一个 Kafka 集群，每个 Kafka broker 都需要保存自己的分区数据，而且还要往 Zookeeper 里写入自己的名字来实现选举等功能。如果简单地用 Deployment 来部署， broker 之间可能就会分不清到底哪块是自己的分区，而且由 Deployment 生成出来的 Pod 名字是随机的，升级后 Pod 的名字会变，导致 Kafka 升级后名字与 Zookeeper 里的名字不一致，被以为是一个新的 broker 。\n\nStateful Set 就是为了解决有状态应用的部署而出现的。下面是 用 bitnami 的 Kafka Helm Chart 部署的一个 Kafka Stateful Set 的例子：\n\n```yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: kafka\n  name: kafka\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kafka\n  serviceName: kafka-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kafka\n    spec:\n      containers:\n      - name: kafka\n        image: docker.io/bitnami/kafka:3.1.0-debian-10-r52\n        command:\n        - /scripts/setup.sh\n        ports:\n        - containerPort: 9092\n          name: kafka-client\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /bitnami/kafka\n          name: data\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      resources:\n        requests:\n          storage: 10Gi\n      storageClassName: gp2\n```\n\n可以看到其实 Stateful Set 类似 Deployment ，也可以通过 replicas 字段定义实例数，如果更新 template 部分， Stateful Set 也会以一定的策略对 Pod 进行更新。\n\n而其创建出来的 Pod 如下所示：\n```sh\n$ kubectl get po -l app.kubernetes.io/name=kafka\nNAME      READY   STATUS    RESTARTS   AGE\nkafka-0   1/1     Running   1          26d\nkafka-1   1/1     Running   3          26d\nkafka-2   1/1     Running   3          26d\n```\n\n与 Replica Set 创建出来的 Pod 相比名字上会有很大差别。 Stateful Set 创建出来的 Pod 会固定的以 `-0` 、 `-1` 、 `-2` 结尾而不是随机生成：\n\n```mermaid\nflowchart TB\nrs[Replica Set A]\nrs --> A-qwert\nrs --> A-asdfg\nrs --> A-zxcvb\n\nss[Stateful Set A]\nss --> A-0\nss --> A-1\nss --> A-2\n```\n\n这样一来，更新时将 Pod 更换之后，新的 Pod 仍能够跟旧的 Pod 保持相同的名字。此外，与 Deployment 相比， Stateful Set 更新后同名的 Pod 仍能保持原来的 IP ，拿到同一个持久化卷，而且不同的 Pod 还能通过独立的 DNS 记录相互区分。这些内容后面还会详细介绍。\n\n> **宠物与牛（ Cattle vs Pets ）的比喻**\n> \n> Deployment 更倾向于将 Pod 看作是牛：我们不会去关心每一个 Pod 个体，如果有一个 Pod 出现了问题，我们只需要把他杀掉并替换成新的 Pod 就好。\n> \n> 但 Stateful Set 更倾向于将 Pod 看作是宠物：弄来一直完全一模一样的宠物并不是容易的事，我们对待这些宠物必须小心翼翼。我们要给他们各自一个专属的名字，替换掉一只宠物时，必须要保证它的花色、名字、行为举止都与之前那只宠物一模一样。\n\n### Daemon Set\n\n不管是 Deployment 还是 Stateful Set ，一般都不会在意自己的 Pod 部署到哪个节点。而假如你不在意自己 Pod 的数量，但需要保证每个节点上都运行一个 Pod 时，就需要 Daemon Set 了。\n\n需要保证每个节点上有且只有一个 Pod 在运行这种情况，经常会在基础结构相关的操作中出现。比如我需要在集群中部署 fluentd 采集 log ，一般来说需要在 Pod 里直接挂载节点磁盘上的文件路径。这种时候如果有一个节点上没有运行 Pod ，那个节点的 log 就采集不到；另一方面，一个节点上运行多个 Pod 毫无意义，而且可能还会导致 log 重复等冲突。\n\n这种需求下简单地使用 Replica Set 或是 Stateful Set 都是不能达到要求的，这两种资源都只能通过亲和性达到“尽量不部署在同一个节点”，做不到绝对。而且当节点数有变更时还需要手动更改设置。\n\n下面是一个用 fluent-bit helm chart 部署的 fluent-bit Daemon Set 的例子：\n\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/instance: fluent-bit\n    app.kubernetes.io/name: fluent-bit\n  name: fluent-bit\n  namespace: fluent-bit\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: fluent-bit\n      app.kubernetes.io/name: fluent-bit\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: fluent-bit\n        app.kubernetes.io/name: fluent-bit\n    spec:\n      containers:\n      - image: cr.fluentbit.io/fluent/fluent-bit:1.9.5\n        volumeMounts:\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: etcmachineid\n          mountPath: /etc/machine-id\n          readOnly: true\n      volumes:\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n          type: \"\"\n      - name: etcmachineid\n        hostPath:\n          path: /etc/machine-id\n          type: File\n```\n\nSelector 之类的都是一样的了，而 Daemon Set 不能指定 replicas 。另外可以看到一个比较刺激的地方： Volume 里使用了 `hostPath` 这种 Volume ，在 Pod 里直接指定了宿主机磁盘上的路径。\n\nK8s 认为经过抽象后， Pod 不应该去关心自己在哪台宿主机上，一般来说是不推荐在 Pod 里直接访问宿主机路径的（不过也没有强制禁止）。不过 Daemon Set 是个特例，由于 Daemon Set 生成的 Pod 与节点强相关， K8s 十分推荐在且仅在 Daemon Set 的 Pod 中访问宿主机路径。\n\n### Job 与 CronJob\n\nReplica Set ， Stateful Set ， Daemon Set 的 Pod 中运行的一般是持续运行的程序，因此这些 Pod 运行终止后会有相应的机制重启这些 Pod 。而 Job 与 Cron Job 这两种资源则专门负责调度不会持续运行的程序。\n\n下面是 《Kubernetes in Action》 书中的一个例子：\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  completions: 5\n  parallelism: 2\n  template:\n    spec:\n      containers:\n      - name: pi\n        image: perl:5.34.0\n        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n```\n\n可以看到，这个 Job 描述了一个会输出 PI 小数点后 2000 位的 Pod 模板。这个 Job 部署后，一共会以这个模板跑完 5 个 Pod ，其中最多并行跑 2 个，并在其中一个成功终止后再跑剩下的 Pod 。可以通过调整 `completions` 与 `parallelism` 字段调整并行与穿行数量。\n\n顺带一提，在 Job 定义中一般不会出现 selector ，但其实 Job 有 selector 字段，一般会由 K8s 为每个 Job 生成一个 uuid 作为 selector 。\n\n另外，可以通过部署 CronJob 这种资源来定时执行 Job 。下面是 《Kubernetes in Action》 书中关于 CronJob 的例子：\n\n```yaml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pi\nspec:\n  schedule: \"0 0 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: pi\n            image: perl:5.34.0\n            command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n          restartPolicy: Never\n```\n\n这个例子中， CronJob 会在每天的 0 点创建一个只运行一个 Pod 的 Job 。 CronJob 不会直接创建 Pod ，而是创建一个 Job ，再由 Job 创建 Pod （就像 Deployment 与 Replica Set 的关系）。另外， CronJob 创建的 Job 会限制 `completions` 与 `parallelism` 都只能等于 1 。\n\n> 关于资源的名称空间\n> \n> 在 K8s 中，各资源都是不能重名的。不能部署两个都叫 `gateway` 的 Pod ，资源之间有可能因为名字冲突而导致部署不成功。（部署一个叫 `gateway` 的 Pod 和一个叫 `gateway` 的 Deployment 倒是可以，因为 `gateway` 不是他们两个的全名，他们的全名分别叫 `pod/gateway` 及 `deployment/gateway` 。）\n> 另外我们已经知道 Deployment 等资源一般会通过标签等来管理自己创建的资源，那两份不相关的应用完全有可能会撞标签，这时候部署逻辑就有可能会出问题。\n> \n> K8s 中提供了名称空间这种资源，用于进行资源隔离。K8s 中大部分资源都从属于一个且仅从属于一个名称空间， Deployment 等资源一般只能控制在同一名称空间下的资源，而不会影响其他名称空间。\n> \n> 另外，也有一些资源是名称空间无关的，比如节点 `Node` 。\n\n\n",
        "title": "Kubernetes 入门 （1）",
        "abstract": "我们知道 K8s 利用了容器虚拟化技术。而说到容器虚拟化就要说 Docker 。可是，容器到底是什么？ Docker 又为我们做了些什么？我们又为什么要用 K8s ？\n> 要把一个不知道打过多少个升级补丁，不知道经历了多少任管理员的系统迁移到其他机器上，毫无疑问会是一场灾难。 —— Chad Fowler 《Trash Your Servers and Burn Your Code》\n\"Write once, run anywhere\" 是 Java 曾经的口号。 Java 企图通过 JVM 虚拟机来实现一个可执行程序在多平台间的移植性。但我们现在知道， Java 语言并没能实现他的目标，会在操作系统调用、第三方依赖丢失、两个程序间依赖的冲突等各方面出现问题。",
        "length": 644,
        "created_at": "2022-08-13T17:45:31.000Z",
        "updated_at": "2022-08-20T14:02:18.000Z",
        "tags": [
          "Kubernetes",
          "DevOps",
          "Docker",
          "Cloud Native"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2022-08-20-introduction-for-k8s-2.md",
        "pagePath": "/articles/introduction-for-k8s-2",
        "slug": "introduction-for-k8s-2"
      },
      "meta": {
        "content": "\n我们之前说的都是用于部署 Pod 的资源，我们接下来介绍与创建 Pod 不相关的资源：储存与网络。\n\n# 储存\n\n其实我们之前已经接触过储存相关的内容了：在讲 Stateful Set 时我们提过 Stateful Set 创建出来的 Pod 都会有相互独立的储存；而讲 Daemon Set 时我们提到 K8s 推荐只在 Daemon Set 的 Pod 中访问宿主机磁盘。但独立的储存具体指什么？除了访问宿主机磁盘以外还有什么其他的储存？\n\n在 Docker 中，我们可以把宿主机磁盘上的一个路径作为一个 Volume 来给容器绑定，或者直接使用 Docker Engine 管理的 Volume 来提供持久化存储或是容器间共享文件。在 K8s 里面也沿用了 Volume 这个概念，可以通过 Mount 绑定到容器内的路径，并通过实现 CSI 的各种引擎来提供更多样的存储。\n\n> CSI: Container Storage Interface ，容器储存接口标准，是 K8s 提出的一种规范。不管是哪种储存引擎，只要编写一个对应的插件实现 CSI ，都可以在 K8s 中使用。\n\n### K8s 中使用 Volume 与可用的 Volume 类型\n\n其实 K8s 中使用 Volume 的例子我们一开始就已经接触过了。还记得一开始介绍 Pod 时的 Nginx 例子吗？\n\n```yaml\nmetadata:\n  name: simple-webapp\nspec:\n  containers:\n    - name: main-application\n      image: nginx\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n    - name: sidecar-container\n      image: busybox\n      command: [\"sh\",\"-c\",\"while true; do cat /var/log/nginx/access.log; sleep 30; done\"]\n      volumeMounts:\n        - name: shared-logs\n          mountPath: /var/log/nginx\n  volumes:\n    - name: shared-logs\n      emptyDir: {}\n```\n\n这个 Pod 描述中声明了一个种类为 `emptyDir` 的，名为 `shared-logs` 的 Volume ，然后 Pod 中的两个容器都分别 Mount 了这个 Volume 。\n\nK8s 中默认提供了几种 Volume ，比如：\n\n- emptyDir ：一个简单的空目录，一般用于储存临时数据或是 Pod 的容器之间共享数据。\n- hostPath ：绑定到节点宿主机文件系统上的路径，一般在 Daemon Set 中使用。\n- gitRepo ：这种 Volume 其实相当于 emptyDir ，不过在 Pod 启动时会从 Git 仓库 clone 一份内容作为默认数据。\n- configMap 、 secret ：一般用于配置文件加载，需要与 configMap 、 secret 这两种资源一同使用。会将 configMap 、 secret 中对应的内容拷贝一份作为 Volume 绑到容器。（下一节中会展开讨论）\n- nfs 、 glusterfs 、 ……：可以通过各种网络存储协议直接挂载一个网络存储\n- (deprecated!) gcePersistentDisk 、 awsElasticBlockStore ……：可以调用各个云平台的 API ，创建一个块储存硬件挂载到宿主机上，再将那个硬件挂载到容器中。\n- persistentVolumeClaim ：持久卷声明，用于把实际储存方式抽象化，使得 Pod 不需要关心具体的储存类型。这种类型会在下面详细介绍。\n\n我们可以注意到， Volume 的声明是 Pod 的一个属性，而不是一种单独的资源。 Volume 是 Pod 的一部分，因此不同的 Pod 之间永远不可能共享同一个 Volume 。\n\n> 但是 Volume 所指向的位置可以相同，比如 HostPath 类型的 Volume 就可以两个 Pod 可以绑定到宿主机上同一个路径，因此 Volume 里的数据还是能通过一定方式在 Pod 间共享。但当然 K8s 不推荐这么做。\n\n另外，由于 Volume 是 Pod 的一部分， Volume 的生命周期也是跟随 Pod 的，当一个 Pod 被销毁时， Volume 也会被销毁，因此最主要还是用于 Pod 内容器间的文件共享。如果需要持久化储存，需要使用 Persistent Volume 。\n\n> Volume 会被销毁不代表 Volume 指向的内容会被销毁。比如 hostPath 、 NFS 等类型 Volume 中的内容就会继续保留在宿主机或是 NAS 上。下面提到的 Presistent Volume Claim 也是，拥有 `persistentVolumeClaim` 类型 Volume 的 Pod 被删除后对应的 PVC 不一定会被删除。\n\n### Presistent Volume 、 Presistent Volume Claim 、 Storage Class\n\n如果需要在 Pod 声明中直接指定 NFS 、 awsElasticBlockStore 之类的信息，就需要应用的开发人员对真实可用的储存结构有所理解，违背了 K8s 的理念。因此 K8s 就弄出了小标题中的三种资源来将储存抽象化。\n\n一个 Persistent Volume (PV) 对应云平台提供的一个块存储，或是 NAS 上的一个路径。可以简单地理解为 **PV 直接描述了一块可用的物理存储** 。因为 PV 直接对应到硬件，因此 PV 跟节点一样，是名称空间无关的。\n\n而一个 **Persistent Volume Claim (PVC) 则是描述了怎样去使用储存** ：使用多少空间、只读还是读写等。一个 PVC 被创建后会且只会对应到一个 PV 。 PVC 从属于一个名称空间，并能被该名称空间下的 Pod 指定为一个 Volume 。\n\nPV 与 PVC 这两种抽象是很必要的。试想一下用自己的物理机搭建一个 K8s 集群的场景。你会提前给物理机插上许多个储存硬件，这时你就需要用 PV 来描述这些硬件，之后才能在 K8s 里利用这些硬件的储存。而实际将应用部署到 K8s 中时，你才需要用 PVC 来描述 Pod 中需要怎么样的储存卷，然后 K8s 就会自动挑一个合适 PV 给这个 PVC 绑定上。这样实际部署应用的时候就不用再特意跑去机房给物理机插硬件了。\n\n但是现在都云原生时代了，各供应商都有提供 API 可以直接创建一个块储存，还要想办法提前准备 PV 实在是太蠢了。于是便需要 Storage Class 这种资源。\n\n使用 Storage Class 前需要先安装各种云供应商提供的插件（当然使用云服务提供的 K8s 的话一般已经准备好了），然后再创建一个 Storage Class 类型的资源（当然一般也已经准备好了）。下面是 AWS 上的 EKS 服务中默认自带的 Storage Class ：\n\n```yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\n  name: gp2\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  fsType: ext4\n  type: gp2\n# 当 PVC 被删除时会同时删除 PV\nreclaimPolicy: Delete\n# 只有当 PVC 被绑定为一个 Pod 的 Volume 时才会创建一个 PV\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n可以看到 EKS 自带的 gp2 提供了一些默认的选项，我们也可以类似地去定义自己的 Storage Class 。有了 gp2 这个 Storage Class ，我们创建一个 PVC 后 K8s 就会调用 AWS 的 API ，创建一个块储存接到我们的节点上，然后 K8s 再自动创建一个 PV 并绑定到 PVC 上。\n\n例如，我们部署 Kafka 时会创建一个这样的 PVC ：\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-kafka-0\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: gp2\n```\n\nK8s 就会自动为我们创建出一个对应的 PV ：\n\n```sh\n# `pvc-` 开头这个是 AWS 自动给我们起的名字。它虽然是 `pvc-` 开头，但他其实是一个 PV 。\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                STORAGECLASS   REASON   AGE\npvc-3614c15f-5697-4d66-a13c-6ddf7eb89998   10Gi       RWO            Delete           Bound    kafka/data-kafka-0   gp2                     152d\n```\n\n要是打开 AWS Console 还会发现， K8s 调用了 AWS 的 API ，自动为我们创建了一个 EBS 块储存并绑定到了我们对应的宿主机上。\n\n可以用下面这张图来表示 Pod 中的 Volume 、 PVC 、 PV 之间的关系：\n\n```mermaid\nflowchart TD\n\nsubgraph Pod[Pod: Kafka-0]\nsubgraph Container[Container: docker.io/bitnami/kafka:3.1.0]\nvm[VolumeMount: /bitnami/kafka]\nend\nvolume[(Volume: data)]\nvm --> volume\nend\n\npvc[pvc: data-kafka-0]\npv[pv: pvc-3614c15f-5697-4d66-a13c-6ddf7eb89998]\nebs[ebs: AWS 为我们创建的块储存硬件]\n\nvolume --> pvc\npvc --> pv\npv --> ebs\n```\n\n而 Storage Class 在上图中则负责读取我们提交的 PVC ，然后创建 PV 与 EBS 。\n\n### 再说回 Stateful Set\n\n之前我们提到 Stateful Set 时说到 Stateful Set 创建的 Pod 拥有固定的储存，到底是什么意思呢？跟 Deployment 的储存又有什么区别呢？\n\n我们先来看看，如果要给 Deployment 创建出来的 Pod 挂载 PVC 需要怎么做。下面是一个部署 Nginx 的 Deployment 清单，其中 html 目录下的静态文件存放在 NFS 里，通过 PVC 挂载到 Pod 中：\n\n```yaml\n# 这里省略了 Service 相关的内容\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-dpl-with-nfs-pvc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts: #挂载容器中的目录到 pvc nfs 中的目录\n        - name: www\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: www\n        persistentVolumeClaim: #指定pvc\n          claimName: nfs-pvc-for-nginx\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc-for-nginx\n  namespace: default\nspec:\n  storageclassname: \"\" # 指定使用现有 PV ，不使用 StorageClass 创建 PV\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\n# 这个例子中需要挂载 NFS 上的特定路径，所以手动定义了一个 PV\n# 一般情况下我们不会手动创建 PV，而是使用 StorageClass 自动创建\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv-for-nginx\nspec:\n  capacity: \n    storage: 1Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /nfs/sharefolder/nginx\n    server: 81.70.4.171\n```\n\n这份清单我们主要关注前两个资源，我们可以看到除了一个 Deployment 资源以外我们还单独定义了一个 PVC 资源。然后在 Deployment 的 Pod 模板中声明并绑定了这个 PVC 。\n\n可这样 apply 了之后会发生什么情况呢？因为我们只声明了一份 PVC ，当然我们只会拥有一个 PVC 资源。但这个 Deployment 的副本数是 3 ，因此我们会有 3 个相同的 Pod 去绑定同一个 PVC 。也就是最终会在 3 个容器里访问同一个 NFS 的同一个目录。如果我们在其中一个容器里对这个目录作修改，也会影响到另外两个容器。\n\n> 注：这一现象不一定在任何情况下都适用。比如 AWS 的 EBS 卷只支持单个 AZ 内的绑定。如果 Pod 因为 Node Affinity 等设定被部署到了多个区，没法绑定同一个 EBS 卷，就会在 Scedule 的阶段报错。\n\n很多时候我们都不希望多个 Pod 绑定到同一 PVC 。比如我们部署一个 DB 集群的时候，如果好不容易部署出来的多个实例居然用的是同一份储存，就会显得很呆。 Stateful Set 就是为了解决这种情况，会为其管理下的每个 Pod 都部署一个专用的 PVC 。\n\n下面是给 Stateful Set 创建出来的 Pod 挂载 PVC 的一份清单：\n\n```yaml\n# 这里省略了 Service 相关的内容\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"nginx\"\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n```\n\n我们可以看到，部署 Stateful Set 时我们不能另外单独定义一份 PVC 了，只能作为 Stateful Set 定义的一部分，在 volumeClaimTemplates 字段中定义 PVC 的模板。这样一来， Stateful Set 会根据这个模板，为每个 Pod 创建一个对应的 PVC ，并作为 Pod 的 Volume 绑定上：\n\n```bash\n# Stateful Set 创建出来的 Pod ，名字都是按顺序的\n$ kubectl get pods -l app=nginx\nNAME      READY     STATUS    RESTARTS   AGE\nweb-0     1/1       Running   0          1m\nweb-1     1/1       Running   0          1m\n\n# Stateful Set 创建出来的 PVC ，名字与 Pod 的名字一一对应\n$ kubectl get pvc -l app=nginx\nNAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nwww-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s\nwww-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s\n```\n\n这样， Stateful Set 的多个 Pod 就会拥有自己的储存，不会相互打架了。另外，如果我们事先定义了 StorageClass ，还能根据 Stateful Set 的副本数动态配置 PV 。\n\n### ConfigMap 与 Secret 挂载作为特殊的卷\n\n有时候我们需要使用配置文件来配置应用（比如 Nginx 的配置文件），而且我们有时候会需要不重启 Pod 就热更新配置。如果用 PVC 来加载配置文件略微麻烦，这时候可以使用 Config Map 。\n\n下面是 K8s 官网上 Config Map 的一个例子：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-demo\ndata:\n  # 一个 Key 可以对应一个值\n  player_initial_lives: \"3\"\n  ui_properties_file_name: \"user-interface.properties\"\n\n  # 一个 Key 也可以对应一个文件的内容\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5    \n  user-interface.properties: |\n    color.good=purple\n    color.bad=yellow\n    allow.textmode=true    \n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-demo-pod\nspec:\n  containers:\n    - name: demo\n      image: alpine\n      command: [\"sleep\", \"3600\"]\n      env:\n        # ConfigMap 的 Key 可以作为环境变量引用\n        - name: PLAYER_INITIAL_LIVES\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo           # 从这个 Config Map 里\n              key: player_initial_lives # 拿到这个 key 的值\n        - name: UI_PROPERTIES_FILE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo\n              key: ui_properties_file_name\n      volumeMounts:\n      - name: config\n        mountPath: \"/config\"\n        readOnly: true\n  volumes:\n    # 定义 Pod 的 Volume ，种类为 configMap\n    - name: config\n      configMap:\n        name: game-demo # ConfigMap的名字\n        # 需要作为文件放入 Volume 的 Key\n        items:\n        - key: \"game.properties\"\n          path: \"game.properties\"\n        - key: \"user-interface.properties\"\n          path: \"user-interface.properties\"\n```\n\n我们可以看到 ConfigMap 里的 Key 可以作为文件或是环境变量加载到 Pod 中。另外，作为环境变量加载后其实还能作为命令行参数传入应用，实现各种配置方式。如果修改 Config map 的内容，也可以自动更新 Pod 中的文件。\n\n然而， Config Map 的热更新有一些不太灵活的地方：\n\n1. 作为环境变量加载的 Config Map 数据不会被热更新。想要更新这一部分数据需要重启 Pod。（当然，命令行参数也不能热更新）\n2. 由于 Kubelet 会先将 Config Map 内容加载到本地作为缓存，因此修改 Config Map 后新的内容不会第一时间加载到 Pod 中。而且在旧版本的 K8s 中， Config Map 被更新直到缓存被刷新的时间间隔还会很长，新版本的 K8s 这一部分有了优化，可以设定刷新时间，但会导致 API Server 的负担加重。（这其实是一个 Known Issue ，被诟病多年： https://github.com/kubernetes/kubernetes/issues/22368 ）\n\n除 Config Map 以外， K8s 还提供了一种叫 Secret 的资源，用法和 Config Map 几乎一样。对比 Config Map ，Secret 有以下几个特点：\n\n1. 在 Pod 里， Secret 只会被加载到内存中，而永远不会被写到磁盘上。\n2. 用 `kubectl get` 之类的命令显示的 Secret 内容会被用 base64 编码。（不过， well ，众所周知 base64 可不算是什么加密）\n3. 可以通过 K8s 的 Service Account 等 RBAC 相关的资源来控制 Secret 的访问权限。\n\n不过，由于 Secret 也是以明文的形式被存储在 K8s 的主节点中的，因此需要保证 K8s 主节点的安全。\n\n> **Downward API 挂载作为特殊的卷**\n> \n> 还有另外一种叫 Downward API 的东西，可以作为 Volume 或是环境变量被加载到 Pod 中。有一些参数我们很难事先在 Manifest 中定义（ e.g. Deployment 生成的 Pod 的名字），因此可以通过 Downward API 来实现。\n> \n> ```yaml\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>     name: test-volume-pod\n>     namespace: kube-system\n>     labels:\n>         k8s-app: test-volume\n>         node-env: test\n> spec:\n>     containers:\n>     - name: test-volume-pod-container\n>       image: busybox:latest\n>       env:\n>       - name: POD_NAME # 将 Pod 的名字作为环境变量 POD_NAME 加载到 Pod 中\n>         valueFrom:\n>           fieldRef:\n>             fieldPath: metadata.name\n>       command: [\"sh\", \"-c\"]\n>       args:\n>       - while true; do\n>           cat /etc/podinfo/labels | echo;\n>           env | sort | echo;\n>           sleep 3600;\n>         done;\n>       volumeMounts:\n>       - name: podinfo\n>         mountPath: /etc/podinfo\n>     volumes:\n>     - name: podinfo\n>       downwardAPI: # Downward API 类型的卷\n>         items:\n>         - path: \"labels\" # 将 Pod 的标签作为  labels 文件挂载到 Pod 中\n>           fieldRef:\n>             fieldPath: metadata.labels\n> ```\n\n\n\n# 网络\n\n其实 Pod 只要部署好了，就会被分配到一个集群内部的 IP 地址，流量就可以通过 IP 地址来访问 Pod 了。然而通过可能会有很大问题： **Pod 随时会被杀死。** 虽然通过用 Deployment 等资源可以在挂掉后重新创建一个 Pod ，但那毕竟是不同的 Pod ， IP 已经改变。\n\n另外， Deployment 等资源的就是为了能更方便的做到多副本部署及任意缩容扩容而存在的。如果在 K8s 中访问 Pod 还需要小心翼翼地去找到 Pod 的 IP 地址，或是去寻找 Pod 是否部署了新副本， Deployment 等资源就几乎没有存在价值了。\n\n> 其实 Pod 部署好后不止会被分配 IP 地址，还会被分配到一个类似 `<pod-ip>.<namespace>.pod.cluster.local` 的 DNS 记录。例如一个位于 default 名字空间，IP 地址为 172.17.0.3 的 Pod ，对应 DNS 记录为 `172-17-0-3.default.pod.cluster.local` 。\n\n### Service\n\n在古代，人们是通过注册中心、服务发现、负载均衡等中间件来解决上面这些问题的，但这样很不云原生。于是 K8s 引入了 Service 这种资源，来实现简易的服务发现、 DNS 功能。\n\n下面是一个经典的例子，部署了一个 Service 和一个 Deployment：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: auth-service\n  labels:\n    app: auth\nspec:\n  type: ClusterIP\n  selector:\n    app: auth # 指向 Deployment 创建的 Pod\n  ports:\n  - port: 80 # Service 暴露的端口\n    targetPort: 8080 # Pod 的端口\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name:  auth\n  labels:\n    app: auth\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: auth\n  template:\n    metadata:\n      name: auth\n      labels:\n        app: auth\n    spec:\n      containers:\n      - name: auth\n        image: xxxxx.dkr.ecr.ap-northeast-1.amazonaws.com/auth:xxxxx\n        ports:\n        - containerPort: 8080\n```\n\n根据前面的知识我们知道，这份文件会部署 Deployment 会创建 2 个相同的 Pod 副本。另外还会部署一个名为 auth-service 的 Service 资源。这个 Service 暴露了一个 80 端口，并且指向那两个 Pod 的 8080 端口。\n\n而这份文件部署后， Service 资源就会在集群中注册一个 DNS A 记录（或 AAAA 记录），集群内其他 Pod （为了辨别我们叫它 Client ）就可以通过相同的 DNS 名称来访问 Deployment 部署的这 2 个 Pod ：\n\n```sh\ncurl http://auth-service.<namespace>.svc.cluster.local:80\n# 或者省略掉后面的一大串\ncurl http://auth-service.<namespace>:80\n# 如果 Client 和 Service 在同一个 Namespace 中，还可以：\ncurl http://auth-service:80\n```\n\n像这样 Client 通过 Service 来访问时，会随机访问到其中一个 Pod ，这样一来无论 Deployment 到底创建了多少个副本，只要副本的标签相同，就能通过同一个 DNS 名称来访问，还能自动实现一些简单的负载均衡。\n\n> **为什么 DNS 名称可以简化？**\n> \n> Pod 被部署时， kubelet 会为每个 Pod 注入一个类似如下的 `/etc/resolv.conf` 文件：\n> \n> ```\n> nameserver 10.32.0.10\n> search <namespace>.svc.cluster.local svc.cluster.local cluster.local\n> options ndots:5\n> ```\n> \n> Pod 中进行 DNS 查询时，默认会先读取这个文件，然后按照 `search` 选项中的内容展开 DNS 。例如，在 test 名称空间中的 Pod ，访问 data 时的查询可能被展开为 data.test.svc.cluster.local 。\n> 更多关于 `/etc/resolv.conf` 文件的内容可参考 https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html\n\n### Service 的种类\n\n我们上面的例子中，可以看到 Service 资源有个字段 `type:ClusterIP` 。其实 Service 资源有以下几个种类：\n\n| 种类           | 作用                                                                                                                                                                                                                                                                                             |\n| :------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `ClusterIP`    | 这个类型的 Service 会在集群内创建一条 DNS A 记录并通过一定方法将流量代理到其指向的 Pod 上。这种 Service 不会暴露到集群外。这是最基础的 Service 种类。                                                                                                                                            |\n| `NodePort`     | 这种 Service 会在 ClusterIP 的基础上，在所有节点上各暴露一个端口，并把端口的流量也代理到指向的 Pod 上。可以通过这种方法从集群外访问集群内的资源。                                                                                                                                                |\n| `LoadBalancer` | 这种 Service 会在 ClusterIP 的基础上，在所有节点上各暴露一个端口，并在集群外创建一个负载均衡器来将外部流量路由到暴露的端口，再把流量代理到指向的 Pod 上。这种 Service 一般需要调用云服务提供的 API 或是额外安装的插件。如果什么插件都没安装的话，这种 Service 部署后会与 `NodePort` 的表现一样。 |\n| `ExternalName` | 这种 Service 不需要 selector 字段指定后端，而是用 externalName 字段指定一个外部 DNS 记录，然后将流量全部指向外部服务。如果打算将集群内的服务迁移到集群外、或是集群外迁移到集群内，这种类型的 Service 可以实现无缝迁移。                                                                          |\n\n### 虚拟 IP 与 Headless Service\n\n如果你在集群内尝试对 Service 对应的 DNS 记录进行域名解析，会发现返回来的 IP 地址与 Service 指向的任何一个 Pod 对应的 IP 地址都不相同。如果你还尝试了去 Ping 这个 IP 地址，会发现不能 Ping 通。为什么会这样呢？\n\n原来，每个 Service 被部署后， K8s 都会给他分配一个集群内部的 IP 地址，也就是 Cluster IP （这也是最基础的 Service 种类会起名叫 Cluster IP 的原因）。\n\n但是这个 Cluster IP 不会绑定任何的网卡，是一个虚拟 IP 。然后 K8s 中有一个叫 kube-proxy 的组件（这里叫他做组件，是因为 kube-proxy 与 Service 、 Deployment 等不一样，不是一种资源而是 K8s 的一部分）， kube-proxy 通过修改 iptables ，将虚拟 IP 的流量经过一定的负载均衡规则后代理到 Pod 上。\n\n![K8s 官网上的虚拟 IP 图](https://d33wubrfki0l68.cloudfront.net/27b2978647a8d7bdc2a96b213f0c0d3242ef9ce0/e8c9b/images/docs/services-iptables-overview.svg)\n\n> **为什么不使用 DNS 轮询？**\n> \n> 为什么 K8s 不配置多条 DNS A 记录，然后通过轮询名称来解析？为什么需要搞出虚拟 IP 这么复杂的东西？这个问题 K8s 官网上也有特别提到原因：\n> \n> - DNS 实现的历史由来已久，它不遵守记录 TTL，并且在名称查找结果到期后对其进行缓存。\n> - 有些应用程序仅执行一次 DNS 查找，并无限期地缓存结果。\n> - 即使应用和库进行了适当的重新解析，DNS 记录上的 TTL 值低或为零也可能会给 DNS 带来高负载，从而使管理变得困难。\n\n有些时候（比如想使用自己的服务发现机制或是自己的负载均衡机制时）我们确实也会想越过虚拟 IP ，直接获取背后 Pod 的 IP 地址。这时候我们可以将 Service 的 `spec.clusterIP` 字段指定为 `None` ，这样 K8s 就不会给这个 Service 分配一个 Cluster IP 。这样的 Service 被称为 **Headless Service** 。\n\nHeadless Service 资源会创建一组 A 记录直接指向背后的 Pod ，可以通过 DNS 轮询等方式直接获得其中一个 Pod 的 IP 地址。另外更重要的一点， Headless Service 还会创建一组 SRV 记录，包含了指向各个 Pod 的 DNS 记录，可以通过 SRV 记录来发现所有 Pod 。\n\n我们可以在集群里用 nsloopup 或 dig 命令去验证一下：\n\n```sh\n# 在集群的 Pod 内部运行\n$ nslookup kafka-headless.kafka.svc.cluster.local\nServer:     10.96.0.10\nAddress:    10.96.0.10#53\n\nName:   kafka-headless.kafka.svc.cluster.local\nAddress: 172.17.0.6\nName:   kafka-headless.kafka.svc.cluster.local\nAddress: 172.17.0.5\nName:   kafka-headless.kafka.svc.cluster.local\nAddress: 172.17.0.4\n\n$ dig SRV kafka-headless.kafka.svc.cluster.local\n# .....\n;; ANSWER SECTION:\nkafka-headless.kafka.svc.cluster.local.      30      IN      SRV     0 20 9092 kafka-0.kafka-headless.kafka.svc.cluster.local.\nkafka-headless.kafka.svc.cluster.local.      30      IN      SRV     0 20 9092 kafka-1.kafka-headless.kafka.svc.cluster.local.\nkakfa-headless.kafka.svc.cluster.local.      30      IN      SRV     0 20 9092 kafka-2.kafka-headless.kafka.svc.cluster.local.\n\n;; ADDITIONAL SECTION:\nkafka-0.kafka-headless.kafka.svc.cluster.local. 30 IN A  172.17.0.6\nkafka-1.kafka-headless.kafka.svc.cluster.local. 30 IN A  172.17.0.5\nkafka-2.kafka-headless.kafka.svc.cluster.local. 30 IN A  172.17.0.4\n```\n\n> 拥有 Cluster IP 的 Service 其实也有 SRV 记录。但这种情况的 SRV 记录中对应的 Target 仍为 Service 自己的 FQDN 。\n\n### 第三次回到 Stateful Set\n\n在上面 Headless Service 的例子中，我们看到，各个 Pod 对应的 DNS A 记录格式为 `<pod_name>.<svc_name>.<namespace>.svc.cluster.local` 。不对啊，之前的小知识里不是说过 Pod 被分配的 DNS A 记录格式应该是 `172-17-0-3.default.pod.cluster.local` 的吗？\n\n其实 Headless Service 还有一个众所周知的隐藏功能。 Pod 这种资源本身的参数中有 `subdomain` 字段和 `hostname` 字段，如果设置了这两个字段，这个 Pod 就拥有了形如 `<hostname>.<subdomain>.<namespace>.svc.cluster.local` 的 FQDN （全限定域名）。如果这时刚好在同一名称空间下有与 `subdomain` 同名的 Headless Service ， DNS 就会用为这个 Pod 用它的 FQDN 来创建一条 DNS A 记录。\n\n比如 Pod1 在 `kafka` 名称空间中， `hostname` 为 `kafka-1` ， `subdomain` 为 `kafka-headless` ，那么 Pod1 的 FQDN 就是 `kafka-1.kafka-headless.kakfa.svc.cluster.local` 。而同样在 `kafka` 名称空间中，刚好又有一个 `kafka-headless` 的 Headless Service ，那么 DNS 就会创建一条 A 记录，就可以通过 `kafka-1.kafka-headless.kafka.svc.cluster.local` 来访问 Pod1 了。当然，由于 DNS 展开，也可以用 `kafka-1.kafka-headless.kafka` 甚至是 `kafka-1.kafka-headless` 来访问这个 Pod 。\n\n其实这些 Pod 是用 Stateful Set 来部署的，这一部分其实是 Stateful Set 相关的功能。之前我们说到 Stateful Set 有唯一稳定的网络标识。我们现在就来详细讲讲，这“唯一稳定的网络标识”到底是在指什么。\n\n我们来看一下这个 kafka Stateful Set 到底是怎么部署的：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-headless\nspec:\n  clusterIP: None # 这是一个 headless service\n  ports:\n  - name: tcp-client\n    port: 9092\n    protocol: TCP\n    targetPort: kafka-client\n  selector:\n    select-label: kafka-label\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\nspec:\n  replicas: 3\n  serviceName: kafka-headless # 注意到这里有 serviceName 字段\n  selector:\n    matchLabels:\n      select-label: kafka-label\n  template:\n    metadata:\n      labels:\n        select-label: kafka-label\n    spec:\n      containers:\n      - name: kafka\n        image: docker.io/bitnami/kafka:3.1.0-debian-10-r52\n        # 接下来 Pod 相关部分省略\n  # 下面 Volume 相关部分也省略\n```\n\n我们看到， Stateful Set 的定义中必须要用 `spec.serviceName` 字段指定一个 Headless Service 。 Stateful Set 创建 Pod 时，会自动给 Pod 指定 `hostname` 和 `subdomain` 字段。这样一来，每个 Pod 才有了唯一固定的 hostname ，唯一固定的 FQDN ，以及通过与 Headless Service 共同部署而获得唯一固定的 A 记录。（此外，其实当 Pod 因为版本升级等原因被重新创建时，相同序号的 Pod 还会被分配到相同固定的集群内 IP 。）\n\n> **关于 Stateful Set 中 `serviceName` 字段的争议**\n> \n> Stateful Set 中的 serviceName 字段是必填字段。这个字段唯一的作用其实就是给 Pod 指定 subdomain 。其实这样会有一些问题：\n> \n> 1. Stateful Set 部署时不会检查是否真的存在这么一个 Headless Service 。如果 serviceName 乱填一个值，会导致虽然 Pod 的 `hostname` 和 `subdomain` 都指定了却没有创建 A 记录的情况。\n> 2. 有时 Stateful Set 的 Pod 不需要接收流量，也不需要相互发现，这时候还强行需要指定一个 serviceName 显得有点多余。\n> \n> 在 GitHub 上有关于这个问题的 Issue ： https://github.com/kubernetes/kubernetes/issues/69608\n\n### 从集群外部访问\n\n在 K8s 集群里把应用部署好了，可是如何让集群外部的客户端访问我们集群中的应用呢？这可能是大家最关心的问题。\n\n不过有认真听的同学估计已经有这个问题的答案了。之前我们讲过 NodePort 和 LoadBalancer 这两种 Service 类型。\n\n其中 NodePort Service 只是简单地在节点机器上各开一个端口，而如何路由、如何负载均衡等则一概不管。\n\n而 LoadBalancer Service 则是在 NodePort 的基础上再加一个一个负载均衡器，然后把节点暴露的端口注册到这个负载均衡器上。这样一来，集群外部的客户端就可以通过同一个 IP 来访问集群中的应用。但是要使用 LoadBalancer Service ，一般需要先安装云供应商提供的 Controller ，或是安装其他第三方的 Controller （比如 Nginx Controller ）。\n\n在 Service 之外还另有一种资源类型叫 Ingress ，也可以用来实现集群外部访问集群内部应用的功能。 Ingress 其实也会在集群外创建一个负载均衡器，因此也需要预先安装云供应商的 Controller 。但 Ingress 与 Service 不同的是，它还会管理一定的路由逻辑，接收流量后可以根据路由来分配给不同的 Service 。\n\n| 类型                 | OSI 模型工作层数 | 依赖于云平台或其他插件 |\n| :------------------- | :--------------- | :--------------------- |\n| NodePort Service     | 第四层           | 否                     |\n| LoadBalancer Service | 第四层           | 是                     |\n| Ingress              | 第七层           | 是                     |\n\n特别再详细说一下 Ingress 这种资源。 Ingress 本身不会在集群内的 DNS 上创建记录，一般也不会主动去路由集群内的流量（除非你在集群内强行访问 Ingress 的负载均衡器…… 不过一般也没什么理由要这样做对吧）。但 Ingress 可以根据 HTTP 的 hostname 和 path 来路由流量，把流量分发到不同的 Service 上。 Ingress 也是 K8s 的原生资源里唯一能看到 OSI 第七层的资源。\n\n下面是 AWS 的 EKS 服务中部署的一个 Ingress 的例子（集群中已安装 AWS Load Balancer Controller ）：\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/backend-protocol-version: GRPC\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/healthcheck-path: /grpc.health.v1.Health/Check\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/success-codes: 0,12\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:xxxxxxxxxx:certificate/xxxxxxxxxx\n\n    external-dns.alpha.kubernetes.io/hostname: sample.example.com\n  \n  name: gateway-ingress\nspec:\n  rules:\n  - host: sample.example.com\n    http:\n      paths:\n      - path: /grpc.health.v1.Health\n        pathType: Prefix\n        backend:\n          service:\n            name: health-service\n            port:\n              number: 50051\n      - path: /proto.sample.v1.Sample\n        pathType: Prefix\n        backend:\n          service:\n            name: sample-service\n            port:\n              number: 50051\n```\n\n可以看到， Ingress 资源可以通过 `spec.rules` 字段中定义各条规则，通过 hostname 或是 path 等第七层的信息来进行路由。 Ingress 部署下去后， AWS Load Balancer Controller 会读取会根据的配置，并在云上创建一个 AWS Application Load Balancer （ALB），而 `spec.rules` 会应用到 ALB 上，由 ALB 来负责流量的路由。\n\n我们也会注意到，怎么 `metadata.annotations` 里有这么多奇奇怪怪的字段！ Ingress 本身的功能都是 AWS Load Balancer Controller 调用 AWS 的 API 创建 ALB 来实现的。但 AWS 的 ALB 能实现的功能可不止 Ingress 字段定义的这些，比如安装 TLS 证书、 health check 等 spec 字段中描述不下的功能，就只能是通过 annotation 的形式来定义了。\n\n> 小彩蛋：可以看到例子中的 Ingress 资源 annotation 字段里还有一行 `external-dns.alpha.kubernetes.io/hostname: sample.example.com` 。其实这个 K8s 集群中还安装了 external-dns 这个应用，它可以根据 annotation 来在外部 DNS 上直接创建 DNS 记录！有了这个插件我们可不用再慢慢打开公共 DNS 管理页面，再小心翼翼地记下 IP 地址去添加 A 记录了。\n\n# 更高级的部署方式（一）\n\n一路说道这里， K8s 中最基础的资源大部分都已经介绍了。但是，这么多资源之间又需要相互配合，只部署一种资源基本没什么生产能力。\n\n比如只部署 Deployment 的话，我们确实是能在一组多副本的 Pod 里跑起可执行程序，但这组 Pod 却几乎没办法接受集群里其他 Pod 的流量（只能通过制定 Pod 的 IP 来访问，但 Pod 的 IP 是会变的）。因此一般来说一个 Deployment 都会搭配一个 Service 来使用。这还是最简单的一种搭配了。\n\n假若我们现在要在自己的 K8s 里安装一个别人提供的应用。当然由于 K8s 是基于容器的，只要别人提供了他应用的 yaml 清单，我们只用把清单用 `kubectl apply -f` 提交给 K8s ，然后让 K8s 把清单中的镜像拉下来就能跑了。可如果我们需要根据环境来改一些参数呢？\n\n如果别人提供的 yaml 文件比较简单还好说，改改对应的字段就好了。如果别人的应用比较复杂，那改 yaml 文件可就是一个大难题了。比如 AWS 的 Load Balancer Controller ，它的 yaml 清单文件可是多达 939 行！\n\n[[aws-elb-controller-lines.png]]\n\n在这种复杂的场景下，我们就需要一些更高级的部署方式了。\n\n### Helm\n\n首先来介绍的是 Helm 。 Helm 是一个包管理工具，可以类比一下 CentOS 中的 yum 工具。它可以把一组 K8s 资源发布成一个 Chart ，然后我们可以用 Helm 来安装这个 Chart ，并且可以通过参数设值来改变 Chart 中的部分资源。利用 Helm 安装 Chart 后还可以管理 Chart 的升级、回滚、卸载。\n\n使用别人提供的 Helm Chart 前，需要先 add 一下 Chart 的仓库，然后再安装仓库里提供的 Chart 。比如我们要安装 bitnami 提供的 Kafka Chart 时：\n\n```bash\n# 添加 https://charts.bitnami.com/bitnami 这个仓库，命名为 bitnami\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# 在 kafka 名称空间里安装 bitnami 仓库里的 kafka Chart ，并通过参数设置为 3 个副本，并同时安装一个 3 副本的 Zookeeper\nhelm install kafka -n kafka \\\n  --set replicaCount=3 \\\n  --set zookeeper.enabled=true \\\n  --set zookeeper.replicaCount=3 \\\n  bitnami/kafka\n```\n\n命令执行后， helm 就会根据参数与 Chart 的内容，在 K8s 里安装 StatefulSet 、 Service 、 ConfigMap 等一切所需要的资源。\n\n```sh\n$ k -n kafka get all,cm\nNAME                    READY   STATUS    RESTARTS      AGE\npod/kafka-0             1/1     Running   1             46d\npod/kafka-1             1/1     Running   3             46d\npod/kafka-2             1/1     Running   3             46d\npod/kafka-zookeeper-0   1/1     Running   0             46d\npod/kafka-zookeeper-1   1/1     Running   0             46d\npod/kafka-zookeeper-2   1/1     Running   0             46d\n\nNAME                               TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE\nservice/kafka                      ClusterIP      172.20.1.196     <none>           9092/TCP                     164d\nservice/kafka-headless             ClusterIP      None             <none>           9092/TCP,9093/TCP            164d\nservice/kafka-zookeeper            ClusterIP      172.20.227.236   <none>           2181/TCP,2888/TCP,3888/TCP   164d\nservice/kafka-zookeeper-headless   ClusterIP      None             <none>           2181/TCP,2888/TCP,3888/TCP   164d\n\nNAME                               READY   AGE\nstatefulset.apps/kafka             3/3     164d\nstatefulset.apps/kafka-zookeeper   3/3     164d\n\nNAME                                DATA   AGE\nconfigmap/kafka-scripts             2      164d\nconfigmap/kafka-zookeeper-scripts   2      164d\nconfigmap/kube-root-ca.crt          1      165d\n```\n\n甚至， Helm 可以通过模板生成的 Pod 环境变量，来预先设置好 Kafka 的配置，让他找得到 Zookeeper 服务：\n\n```yaml\napiVersion: v1\nkind: Pod\n# 略去无关信息\nspec:\n  containers:\n  - name: kafka\n    command:\n    - /scripts/setup.sh\n    env:\n    - name: KAFKA_CFG_ZOOKEEPER_CONNECT\n      value: kafka-zookeeper\n    # ...\n```\n\n通过设置 `KAFKA_CFG_ZOOKEEPER_CONNECT` 这个环境变量，指定了 Kafka Broker 可以通过访问 `kafka-zookeeper` 来找到 zookeeper 服务。（还记得 zookeeper 的 Service 名字是 `kafka-zookeeper` 吗？ zookeeper 与 kafka 部署在同一个名称空间里，因此可以直接通过 Service 名访问。）\n\n如果我们打开这个 helm chart 对应的[代码仓库](https://github.com/bitnami/charts/tree/master/bitnami/kafka)，会发现原来有一组 go template 文件，以及一个 `values.yaml` 文件和 `Chart.yaml` 文件：\n\n```sh\n.\n├── Chart.lock\n├── Chart.yaml\n├── README.md\n├── templates\n│   ├── NOTES.txt # 这里定义的是 helm 工具的命令行信息\n│   ├── _helpers.tpl # 这里面是一些定义好的 go template 代码块可以供其他模板使用\n│   ├── configmap.yaml\n│   ├── statefulset.yaml\n│   ├── svc-headless.yaml\n│   ├── svc.yaml\n│   └── # 以下省略若干模板文件\n└── values.yaml\n```\n\n- `Chart.yaml` 中定义了这个 Chart 的基本信息，包括名称、版本、描述、依赖等。\n- `values.yaml` 中定义了这个 Chart 的默认参数，包括各种资源的默认配置、副本数量、镜像版本等。其中的值都可以通过 `helm install` 命令的 `--set` 参数来覆盖。\n- `templates/` 文件夹下的都是 go template 的模板文件。\n\n`helm install` 就是通过用 `values.yaml` 中预定义的参数，渲染 `templates/` 文件夹下的 go template 文件，生成最终的 yaml 文件，然后再通过 kubectl apply -f 的方式，将 yaml 文件里的资源部署到 K8s 里。然后通过忘资源里注入一些特殊 annotation 的方式来记住自己部署了那些资源，进而提供 `update` 、 `uninstall` 等功能。\n\n关于更多 Helm 的内容，可以参考[官方文档](https://helm.sh/docs/)。\n\n### Kustomize\n\n另一个部署工具是 Kustomize 。之前提到 Config Map 时的例子中，将配置文件的内容直接写进了 yaml 清单的一个字段里：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-demo\ndata:\n  # 一个 Key 可以对应一个值\n  player_initial_lives: \"3\"\n  ui_properties_file_name: \"user-interface.properties\"\n\n  # 一个 Key 也可以对应一个文件的内容\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5    \n  user-interface.properties: |\n    color.good=purple\n    color.bad=yellow\n    allow.textmode=true    \n```\n\n其实这样很不好，先不说这样写没办法在 IDE 里用配置文件自己的语法检查，每行还需要一定的缩进，如果配置文件有好几百行，你甚至会忘了这一行到底是哪个配置文件！此时我们就会自然而然的想把每个配置文件以单独文件的形式保存。\n\nKustomize 就是这样一个工具，它可以帮助我们把每个配置文件以单独文件的形式保存，然后再通过一个 `kustomization.yaml` 文件，将这些配置文件组合起来，生成最终的 yaml 文件。\n\n```yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  # 其他资源也可以单独使用一个文件定义\n  - deployment.yaml\n\n# 用 configMapGenerator 从文件中生成 ConfigMap\nconfigMapGenerator:\n  - name: game-demo\n    literals:\n      - \"ui_properties_file_name=user-interface.properties\"\n      - \"player_initial_lives=3\"\n    # 从文件中读取内容\n    files:\n      - game.properties\n      - user-interface.properties\n# 有多个 configMap 时，可以通过统一的 generatorOptions 来设置一些通用的选项\ngeneratorOptions:\n  disableNameSuffixHash: true\n```\n\n然后两个配置文件的内容可以单独用文件定义，此时可以结合 IDE 的语法检查，以及代码补全功能，来编写配置文件。\n\n```properties\n# user-interface.properties\ncolor.good=purple\ncolor.bad=yellow\nallow.textmode=true    \n```\n\n然后将 `kustomization.yaml` 和其他所需的文件都放在同一个目录下：\n\n```bash\n.\n├── kustomization.yaml\n├── deployment.yaml\n├── game.properties\n└── user-interface.properties\n```\n\n然后就可以通过 `kubectl apply -k ./` 来将整个 kustomize 文件夹转换为 yaml 清单直接部署到 K8s 中。\n（没错，现在 Kustomize 已经成为 kubectl 中的内置功能！可以不用先 `kustomize build` 生成 yaml 文件再 `kubectl apply` 两步走了！）\n\n值得提醒的是，虽然 `kustomization.yaml` 有 `apiVersion` 和 `kind` 字段，长得很像一个资源清单，但其实 K8s 的 API server 并不认识他。 Kustomize 的工作原理其实是先根据 `kustomization.yaml` 生成 K8s 认识的 yaml 资源清单，然后再通过 `kubectl apply` 来部署。\n\n除了可以直接将 ConfigMap 与 Secret 中的文件字段内容用单独的文件定义外， Kustomize 还有其他比如为部署的资源添加统一的名称前缀、添加统一字段等功能。这些大家可以阅读 Kustomize 的[官方文档](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/)来了解。\n\n### 各种工具的优缺点\n\n我们目前已经知道有三种在 K8s 中部署资源的方式： `kubectl apply`、Helm 和 Kustomize 。\n\n其中 `kubectl apply` 的优缺点很明确，优点是最简单直接，缺点是会导致要么 yaml 清单过长，要么需要分多文件多次部署，使集群中产生中间状态。\n\n而 Helm 与 Kustomize 我们上面也分析过，其实都是基于 `kubectl apply` 的。 Helm 是通过 go template 先生成 yaml 文件再 `kubectl apply` ，而 Kustomize 是通过 `kustomization.yaml` 中的定义用自己的一套逻辑生成 yaml 文件，然后再 `kubectl apply` 。\n\nHelm 的优点是 Helm Chart 安装时可以直接使用别人 Helm 仓库中已经上传好的 Chart ，只需要设置参数就可以使用。这也是 Kustomize 的缺点：如果想要使用别人提供的 Kustomization 而只修改其中的一些配置，必须要先把放 `kustomization.yaml` 的整个文件夹下载下来才能做修改。\n\n而 Helm 的缺点也是明显的， Helm 依赖于往资源里注入特殊的 annotation 来管理 Chart 生成的资源，这可能会很难与集群中现有的一些系统（比如 Service Mesh 或是 GitOps 系统等）放一起管理。而 Kustomize 生成的 yaml 清单就是很干净的 K8s 资源，原先的 K8s 资源该是什么表现就是什么表现，与现有的系统兼容一般会比较好。\n\n而另外，由于 Helm 与 Kustomize 都是基于 `kubectl apply` 的，因此他们有共同的缺点，就是不能做 `kubectl apply` 不能做的事情。\n\n什么叫 `kubectl apply` 不能做的事情呢？比如说我们要在 K8s 中部署 Redis 集群。聪明的你可能就想到要用 Stateful Set 、 PVC 、 Headless Service 来一套组合拳。这确实可以部署一个多节点、有状态的 Redis Cluster 。可是如果我们要往 Redis Cluster 里加一个节点呢？\n\n你当然可以把 Stateful Set 中的 `Replicas` 字段加个 1 然后用 `kubectl apply` 部署，可是这实际上只能增加一个一个 Redis 实例 —— 然后什么都没发生。其他节点不认识这个新的节点，访问这个新节点也不能拿到正确的数据。要知道往 Redis Cluster 里加节点，是要先让集群发现这个新节点，然后还要迁移 slot 的！ `kubectl apply` 可不会做这些事。\n\n> Well, 其实这些也是可以通过增加 initContainer 、修改镜像增加启动脚本等方式，实现用 `kubectl apply` 部署的。可是，这会让整个 Pod 资源变得很难理解，也不好维护。而且，如果不是因为做不到，谁会想去修改别人的镜像呢？\n\n我们接下来会介绍 K8s 的核心架构，来理解我们之前讲的这些资源到底是怎么工作的。最后会引出一组新的概念： Operator 与自定义资源（ Custom Resource Definition ，简称 CRD ）。通过 Operator 与 CRD ，我们可以做到 `kubectl apply` 所不能做到的事，包括 Redis Cluster 的扩容。\n\n> DIO: `kubectl apply` 的能力是有限的……\n> 越是部署复杂的应用，就越会发现 `kubectl apply` 的能力是有极限的……除非超越 `kubectl apply` 。\n> \n> JOJO: 你到底想说什么？\n> \n> DIO: 我不用 `kubectl apply` 了！ JOJO ！\n> （其实还是要用的）\n\n",
        "title": "Kubernetes 入门 （2）",
        "abstract": "我们之前说的都是用于部署 Pod 的资源，我们接下来介绍与创建 Pod 不相关的资源：储存与网络。\n其实我们之前已经接触过储存相关的内容了：在讲 Stateful Set 时我们提过 Stateful Set 创建出来的 Pod 都会有相互独立的储存；而讲 Daemon Set 时我们提到 K8s 推荐只在 Daemon Set 的 Pod 中访问宿主机磁盘。但独立的储存具体指什么？除了访问宿主机磁盘以外还有什么其他的储存？\n在 Docker 中，我们可以把宿主机磁盘上的一个路径作为一个 Volume 来给容器绑定，或者直接使用 Docker Engine 管理的 Volume 来提供持久化存储或是容器间共享文件。在 K8s 里面也沿用了 Volume 这个概念，可以通过 Mount 绑定到容器内的路径，并通过实现 CSI 的各种引擎来提供更多样的存储。",
        "length": 875,
        "created_at": "2022-08-20T21:56:52.000Z",
        "updated_at": "2022-08-20T14:02:18.000Z",
        "tags": [
          "Kubernetes",
          "DevOps",
          "Docker",
          "Cloud Native"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/articles/2024-11-16-try-cursor-and-thinking.md",
        "pagePath": "/articles/try-cursor-and-thinking",
        "slug": "try-cursor-and-thinking"
      },
      "meta": {
        "content": "\n试着用了一下 cursor ，感觉还不错。非技术也能半小时能做一款 App 可能是真的。但替代不了技术岗也是真的。\n\n## 先说问题\n\n虽然 Cursor 基于 VSCode 二次开发，但可能为了做 AI 功能把 Editor Pooling 或者 File Watching 能力搞坏了，经常 Apply 了修改后 Explorer 和 Editor 里没有及时反馈。然后自动补全功能因为可以删内容导致手感跟 VSCode 里的 Copilot 比较不同，用起来比较 annoying 。\n\n### 文件系统监控问题\n\n作为一个基于 VSCode 二次开发的编辑器，Cursor 在文件系统监控方面存在一些问题。当对文件进行修改后，Explorer 和 Editor 经常无法及时反映这些变化，这可能是因为为了实现 AI 功能而对原有的 Editor Pooling 或 File Watching 机制进行了修改导致的。\n\n> 这简直太让人抓狂了！😫 你改了代码，编辑器却在那装傻充愣。就好像你发了消息，对方已读不回 💬。程序员最讨厌等待了，对吧？⏳\n\n### 自动补全体验差异\n\nCursor 的自动补全功能与 VSCode 中的 GitHub Copilot 有明显的使用体验差异。由于 Cursor 的补全可以删除已有内容，这种行为方式与程序员习惯的编辑模式不太相符，使用起来感觉比较突兀和烦人。\n\n> 这个功能真的让人又爱又恨 💔！AI 小助手太热情了，动不动就想帮你重写代码。冷静点，老铁！我只是想要一点提示而已！🤪\n\n### 性能影响\n\n这些技术问题不仅影响了开发体验，还可能会降低编码效率。实时的文件系统反馈对于开发工作流程来说是非常重要的，而自动补全功能的差异也会影响到日常编码的流畅度。\n\n> 性能问题真是让人头大 🤯！写代码就应该是行云流水的感觉，现在却经常要等等等...等到我都能喝完一杯咖啡了 ☕️！\n\n\n## Chat Anywhere\n\n不过 Chat Anywhere 这个做法应该是做对了。需要用 AI 代写的场景，很多时候并不是不会写而是懒得写，以前在 VSCode 里需要切到 Coplilot 的 Tab ，写 prompt 等回复，然后再将答案复制粘贴回去，多数情况下有这闲工夫还不如直接自己写🤣，在 Cursor 里可以直接原地调 AI 改写，真的巨舒服。\n\n> 这功能简直就是懒人福音啊！🎯 再也不用在各种窗口之间跳来跳去了！爽歪歪！🎊\n\n## 智能代码分析\n\n还有 Cursor 可以直接将整个项目 Indexing 掉，还能理解代码间的调用关系，一个 prompt 直接出调用关系图，再也不用挠爆头想怎么做 RAG 怎么给文章分块了，爽到。\n\n![[Pasted image 20241116200220.png]]\n\n![[Pasted image 20241116200148.png]]\n\n## Cursor 可能会比较有用的场景\n\n### 智能注释生成\n\nCursor 在生成代码注释方面表现出色。它不仅能分析当前文件的代码，还能理解整个项目的上下文。通过分析 import 关系、函数调用链、接口实现和类型定义等多个维度，它能生成更加准确和有意义的注释。这对于维护大型项目或者需要快速理解他人代码的场景特别有帮助。\n\n> 终于不用为写注释抓耳挠腮了！🎉 AI 帮你分析完所有代码关系，三下五除二就能生成一份漂亮的注释！💡\n\n### 文档和文章创作\n\n在文档和文章创作方面，Cursor 的原地 AI 改写功能特别实用。当你需要写一篇长文，需要参考多个文档源，但又不需要特别严格的逻辑推导时，这个功能简直是神器。你可以：\n\n- 让 AI 帮你规划文章结构，生成合适的目录\n- 根据已有内容快速扩充段落\n- 实时调整文章语气和风格\n- 参考相关文档自动补充内容\n\n> 写文档再也不用对着空白发呆了！✍️ AI 小助手随时待命，帮你把想法变成优美的文字！📝\n\n### 项目结构优化\n\n在项目结构维护方面，Cursor 提供了一系列强大的功能：\n\n1. **可视化项目结构**：一键生成项目依赖关系图，让项目结构一目了然。再也不用在复杂的目录结构中迷失方向。\n\n2. **智能重构建议**：基于项目分析，AI 可以提供项目结构优化建议，帮助你建立更清晰的代码组织方式。\n\n3. **自动化工具生成**：需要批量处理文件？Cursor 可以直接生成 Shell 命令或 Python 脚本，帮你完成繁琐的目录操作。\n\n> 项目管理变得如此轻松！🚀 让 AI 帮你梳理项目结构，生成工具脚本，程序员的生产力简直起飞！✨\n\n\n## 总结\n\n> 总的来说，Cursor 作为一款基于 AI 的代码编辑器有其独特的优势，尤其是 Chat Anywhere 和智能代码分析这样的创新功能确实提升了开发效率。但同时也存在一些技术问题，比如文件系统监控和自动补全体验等需要改进的地方。尽管如此，它展示了 AI 辅助编程的潜力，为未来编程工具的发展提供了新的思路。\n\n顺便一提，在引用块里的内容都是由 AI 生成的。\n",
        "title": "尝试 Cursor 的感想和一些思考",
        "abstract": "试着用了一下 cursor ，感觉还不错。非技术也能半小时能做一款 App 可能是真的。但替代不了技术岗也是真的。\n虽然 Cursor 基于 VSCode 二次开发，但可能为了做 AI 功能把 Editor Pooling 或者 File Watching 能力搞坏了，经常 Apply 了修改后 Explorer 和 Editor 里没有及时反馈。然后自动补全功能因为可以删内容导致手感跟 VSCode 里的 Copilot 比较不同，用起来比较 annoying 。\n作为一个基于 VSCode 二次开发的编辑器，Cursor 在文件系统监控方面存在一些问题。当对文件进行修改后，Explorer 和 Editor 经常无法及时反映这些变化，这可能是因为为了实现 AI 功能而对原有的 Editor Pooling 或 File Watching 机制进行了修改导致的。",
        "length": 78,
        "created_at": "2024-11-16T15:42:00.000Z",
        "updated_at": "2024-11-16T15:42:00.000Z",
        "tags": [
          "Cursor",
          "杂谈"
        ],
        "license": true
      }
    }
  ],
  "ideas": [
    {
      "pathMapping": {
        "filePath": "public/content/ideas/blog-in-next.md",
        "pagePath": "/ideas/blog-in-next",
        "slug": "blog-in-next"
      },
      "meta": {
        "content": "\n# blog todo\n\n## 一期\n\n- [x] 复写 markdown parse ，增加 latex 支持\n    - [x] remark-math\n    - [x] rehype-katex\n    - [x] rehype-slug\n    - [x] https://github.com/unifiedjs/unified\n    - [x] recma -> 修改 MDX , remark -> 修改 markdown ， rehype -> 修改 html\n    - [x] https://www.haxibami.net/blog/posts/blog-renewal\n    - [x] about img to next/Image https://zenn.dev/elpnt/articles/c17727e9d254ef00ea60\n    - [x] introduction of remark https://vivliostyle.github.io/vivliostyle_doc/ja/vivliostyle-user-group-vol2/spring-raining/index.html\n- [x] 下标语法行为 -> 修改默认不用 a 标签\n    使用 remark gfm 后自动解决\n- [x] 处理 picture 路径的问题\n- [x] 默认元素映射，处理代码高亮\n    - [x] mermaid 渲染\n    - [x] latex 渲染 弃用，思考 remark-rehype 优先\n    - [x] latex 中文渲染\n    - [x] vscode-memo 语法\n        - [x] 站内多媒体相对链接\n    - [x] latex 显示两次\n        - [x] katex 需要特殊的 css ，加载后修复\n    - [x] 没有闭合的左箭头自动转义\n        做不到， next-mdx-remote 调用 mdx-js ， createProcessor 中会在执行所有 remark 插件之前，先执行 remark-mdx ，无法通过插件修改 \n        - https://github.com/mdx-js/mdx/tree/main/packages/remark-mdx\n        - https://github.com/mdx-js/mdx/blob/main/packages/mdx/lib/core.js\n- [x] 简单的样式\n    - [x] Article 页面\n    - [x] 主页\n        - [x] 主页封面： position: static 不会反应 z-index ，需要 position: 其他值\n    - [x] about 页\n        - 先不做\n    - [x] 其他页的最小高度，处理 footer 不要太高\n    - [x] Tag 页\n    - [x] Idea 页\n        - [x] refactor move meta together\n- [x] fix post list height\n- [x] fix post list page tag box\n- [x] SEO 简单\n    - [x] site map https://github.com/iamvishnusankar/next-sitemap\n    - [x] Header， title\n    - [x] OG\n        - [x] og:site_name vs og:title?\n        - [x] og:url?\n    - [x] footer\n        - [x] icon hover 发光， bg 不要变色\n        - [x] 增加 icon ， Pixiv\n- [x] 处理旧链接跳转 https://www.viget.com/articles/host-build-and-deploy-next-js-projects-on-github-pages/\n    - [x] path prefix\n        - [x] 环境变量？配置文件？\n    - [x] 代码高亮换成静态后 mermaid 也要用 remark 处理\n        - [x] mermaid 内样式 -> 修复 global applyed 的行高\n        - [x] 为什么 haxibami 的样式不会变？为什么是 tspan 和 text ？待探究 -> 是 pie 和 sequence 默认的\n            - [x] maybe rehype-raw ？ -> 不是 rehype-raw 的问题，而且如果用 rehype-raw 会让 mdx 渲染不了显示 Cannot compile `mdxJsxFlowElement` node\n            - [x] https://github.com/mermaid-js/mermaid/issues/2688\n            - [x] turning the htmlLables to false fixed this (maybe other types of charts should be tested as well)\n    - [x] 去除 path 中的日期\n        1. get all slugs once\n        2. get post meta by slug\n    - [x] 修复 image url 跳转\n    - [x] link jump 301 -> 308\n        - 不能直接在 config 里配置，不会自动静态生成\n        - 在 getStaticProps 里处理:\n            - param 中拿到 slug 以 string[] 出现， slash 无关\n            - build 时出现 error：`redirect` can not be returned from getStaticProps during prerendering\n            - https://nextjs.org/docs/messages/gsp-redirect-during-prerender\n        - 放弃治疗\n- [x] git time versioning\n    - [x] created at\n    - [x] updated at\n- [x] 部署脚本\n    - [x] ghpage\n        - [x] gh page 环境 path 前缀设置，\n        - [x] env file 使用\n    - [x] s3 test + prod\n        - reusable wrokflow (on workflow call) https://docs.github.com/en/actions/using-workflows/reusing-workflows\n        - composite action (no details, not to use) https://docs.github.com/en/actions/creating-actions/creating-a-composite-action\n        - use artifact between workflow https://docs.github.com/en/actions/using-workflows/storing-workflow-data-as-artifacts\n        - an https://stackoverflow.com/questions/70003863/reusing-github-action-workflow-steps-inside-another-job\n        - gh-pages 先用蠢办法， build 上传一次 artifact ，deploy 时在 gh-pages action 里再上传一次\n        - [x] Github Action 复用\n        - [x] test infra stack 更新使用 s3 web hosting\n        - [x] 处理 url 结尾带 slash 问题\n            - https://github.com/keaeriksson/hugo-s3-cloudfront \\<- 最终解决方案，用 pulumi 把这个重写一遍\n            - maybe it's time to serverless https://github.com/serverless-nextjs/serverless-next.js\n            - or just use vercel! \n\n        - [x] 更新 prod （infra ， Deploy ）\n- [x] 修复 mermaid 渲染\n    - [x] 方法1 remark 时处理，变静态 svg （简单）\n    - [x] 方法2 highlight 推延到渲染时处理，用组件包裹 （希望优先）\n        - [x] 组建渲染时快速刷新可能导致 syntax error\n    - [x] 可能的处理闪动的信息： \n        - https://github.com/ant-design/ant-design/issues/16037\n        - https://stackoverflow.com/questions/72248724/components-flicker-during-server-side-render-for-nextjs-and-material-ui\n        - https://www.stackfive.io/work/nextjs/how-to-fix-styled-components-page-flicker-in-next-js-12\n        - https://stackoverflow.com/questions/65527040/using-prism-js-line-highlight-plugin-with-next-js-app-causes-text-content-did-n\n        - https://css-tricks.com/syntax-highlighting-prism-on-a-next-js-site/#aa-highlighting-lines\n        - https://www.felixmokross.dev/blog/code-samples-line-highlighting\n- [x] 修复：\n    - [x] Home Page 下方链接点击问题\n    - [x] 改进 trailing slash -> lambda 写错，要加 .html 不是 /index.html\n    - [x] 排查 s3 旧文件未删除问题 rm 后加一句 ls debug\n    - [x] tag box 增加 focus\n- [x] reuse https://docs.github.com/en/actions/using-workflows/reusing-workflows\n\n\n## 二期\n\n- [x] favicon\n- [x] Google Analytics\n    - [ ] sitemap 检查\n- [x] test workflow for dependabot\n- [x] SEO better, og:type=article read more: https://ogp.me/#types\n- [x] 文章按创建时间倒序\n- [x] 写 README 与项目简介\n- [ ] Rss etc https://github.com/jpmonette/feed\n- [x] 响应式适配\n\n- tips:\n    - md syntax -> mdx(react component) : use mdx provider component rewriting (code block to mermaid component)\n    - md syntax -> special html -> use rehype plugin rewriting (katex, highlight)\n    - special md syntax -> use remark plugin rewriting (could not to mdx, should first to html and then mdx provider rewrite)\n\n",
        "title": "用 Next.js 重构 blog ，TODO list",
        "abstract": "- [x] 复写 markdown parse ，增加 latex 支持\n    - [x] remark-math\n    - [x] rehype-katex",
        "length": 128,
        "created_at": "2023-02-12T07:25:33.000Z",
        "updated_at": "2024-11-29T15:13:20.000Z",
        "tags": [
          "Blog",
          "Nextjs"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/blog-syntax.md",
        "pagePath": "/ideas/blog-syntax",
        "slug": "blog-syntax"
      },
      "meta": {
        "content": "\n# 一级标题\n\n## 二级标题\n\n### 三级标题\n\n#### 四级标题\n\n##### 五级标题\n\n###### 六级标题\n\n**加粗**\n\n*斜体*\n\n_斜体2_\n\n***加粗斜体***\n\n**_加粗斜体2_**\n\n~~删除线~~\n\n==高亮==\n\n> 引用\n\n# 其他 MD 语法\n\n## 代码块\n\n`行内代码`\n\n代码块高亮：\n\n```python\n# 代码块\ndef func_echo(s: str):\n    print(s)\n\n\nclass HelloPrinter:\n    printer: Callable[[str]]\n\n    def __init__(self, printer: Callable[[str]]):\n        self.printer = printer\n\n    def call(self, s: str):\n        self.printer(s)\n\n\np = HelloPrinter(func_echo)\np.call(\"hello world!\")\n```\n\n大围栏\n\n````markdown\n```\ndef func_echo(s: str):\n    print(s)\n```\n````\n\n行内反引号围栏： `` ` `` 或者 ``` `` ``` 的模式\n\n```markdown\n`段落反引号内的行内反引号`\n```\n\n#### Rehype Pretty 语法：\n\n名称显示，captions 显示：\n\n```python title=\"main.py\" caption=\"这是一段描述\"\n# 代码块\ndef func_echo(s: str):\n    print(s)\n\n\nclass HelloPrinter:\n    printer: Callable[[str]]\n\n    def __init__(self, printer: Callable[[str]]):\n        self.printer = printer\n\n    def call(self, s: str):\n        self.printer(s)\n\n\np = HelloPrinter(func_echo)\np.call(\"hello world!\")\n```\n\n高亮\n\n```python {1-3,5}#a {4}#b {7} /printer/#c \"Callable\"#a /func_echo/#1\n# 代码块\ndef func_echo(s: str):\n    print(s)\n\n\nclass HelloPrinter:\n    printer: Callable[[str]]\n\n    def __init__(self, printer: Callable[[str]]):\n        self.printer = printer\n\n    def call(self, s: str):\n        self.printer(s)\n\n\np = HelloPrinter(func_echo)\np.call(\"hello world!\")\n```\n\n行号\n\n```python showLineNumbers{998}\n# 代码块\ndef func_echo(s: str):\n    print(s)\n\n```\n\n## 列表\n\n- 无序列表\n  - 无序列表\n    - 无序列表\n    - 无序列表\n\n1. 有序列表\n2. 有序列表\n3. 有序列表\n   1. 有序列表\n   2. 有序列表\n      1. 有序列表\n      2. 有序列表\n\n- [ ] 未完成\n- [x] 已完成\n  - [x] 已完成\n    - [ ] 未完成\n\n- 交叉嵌套\n  - [ ] 未完成\n  - [x] 已完成\n    1. 有序列表\n    2. 有序列表\n\n- [ ] 交叉嵌套 2\n  1. 有序列表\n  2. 有序列表\n     - 无序列表\n     - 无序列表\n\n1. 交叉嵌套\n2. 交叉嵌套\n   - 无序列表\n   - 无序列表\n     - [ ] 未完成\n     - [x] 已完成\n\n## Quote Block\n\n> 这是一个 Quote Block\n>\n> 里面可以有多行数据\n\n## 链接\n\n[链接](https://blog.ryo-okami.xyz)\n\n[站内链接](/ideas/using-chart-js)\n\n## 图片\n\n图片：\n\n![图片](https://blog.ryo-okami.xyz/content/articles/2022-07-31-why-homogeneous/OnOneLineWillStillOneLine_ManimCE_v0.16.0.post0.gif)\n\n站内图片：\n\n![站内图片](/content/articles/2022-07-31-why-homogeneous/OnOneLineWillStillOneLine_ManimCE_v0.16.0.post0.gif)\n\n## 表格\n\n| 表头     | 表头     | 表头     |\n| -------- | -------- | -------- |\n| ~~删除~~ | ==高亮== | **加粗** |\n| 单元     | 单元     | _斜体_   |\n\n## 脚注\n\n下标[^1]\n\n[^1]: 注释\n\n# 插件\n\n## Katex\n\n行间公式\n\n$$\n\\begin{aligned}\n\\dot{x} & = \\sigma(y-x) \\\\\n\\dot{y} & = \\rho x - y - xz \\\\\n\\dot{z} & = -\\beta z + xy\n\\end{aligned}\n$$\n\n行内公式 $E=mc^2$\n\n## Mermaid\n\nmermaid 流程图\n\n```mermaid\ngraph LR\n  A[方形] --> B(圆角)\n  B --> C{条件}\n  C -->|a=1| D[结果1]\n  C -->|a=2| E[结果2]\n  C -->|a=3| F[结果3]\n```\n\n另一个 mermaid 流程图，同样类型不会冲突\n\n```mermaid\ngraph TD\n  A((圆)) --> B([圆边])\n  B --> C[(DB)]\n```\n\nmermaid 时序图\n\n```mermaid\nsequenceDiagram\n  participant Alice\n  participant Bob\n  Alice->>John: Hello John, how are you?\n  loop Healthcheck\n    John->>John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts <br/>prevail...\n  John-->>Alice: Great!\n  John->>Bob: How about you?\n  Bob-->>John: Jolly good!\n```\n\n## Heading 引用\n\n点击能够跳转：\n\n[文章内标题引用](#一级标题)\n\n[跨文章标题引用](/ideas/using-chart-js#react-chartjs-2)\n\n# Obsidian\n\n## Wikilink\n\n### 站内短引用\n\n[[2022-08-13-introduction-for-k8s]]\n\n可以去掉路径中的日期 [[introduction-for-k8s]] 作为 page path 引用\n\n带 label [[2022-08-13-introduction-for-k8s|其他文章]]\n\n带路径 [[articles/2022-08-13-introduction-for-k8s|其他文章]]\n\nPage path 全路径 [[/articles/introduction-for-k8s|其他文章]]\n\nfile path 全路径 [[public/content/articles/2022-08-13-introduction-for-k8s|其他文章]]\n\n一行多个 [[2022-08-13-introduction-for-k8s|其他文章]] [[2022-08-20-introduction-for-k8s-2|其他文章2]]\n\n> 目前未实现 Obsidian Anchor 直接引用到块或标题\n\n## Rich Content 短引用\n\n### 图片短引用\n\n短引用图片，纯文件名\n\n![[test-img-show-image.png]]\n\n短引用图片，带注释\n\n![[test-img-show-image.png|这是一张图片]]\n\n短引用图片，带路径\n\n![[blog-syntax/test-img-show-image.png]]\n\n短引用图片，带路径和注释\n\n![[blog-syntax/test-img-show-image.png|这是一张图片]]\n\n短引用图片，全路径\n\n![[/content/ideas/blog-syntax/test-img-show-image.png]]\n\n### Excalidraw 短引用\n\n短引用 Excalidraw\n\n![[Drawing 2024-04-13 17.33.27.excalidraw]]\n\n## Callouts\n\n> [!note]\n>\n> Note 级\n\n> [!info]\n>\n> Callout 里是正常的 markdown 语法\n> ```markdown\n> > [!info]\n> >\n> > Callout 里是正常的 markdown 语法\n> ```\n\n> [!tip] 标题\n>\n> Tip 级带标题\n\n> [!faq]- 可折叠 Callout\n>\n> 是的，这是 faq 级 callout 。可折叠 Callout 在折叠时隐藏内容。\n\n> [!faq]+ 默认打开的可折叠 Callout\n>\n> 可折叠 Callout 也可以设置为默认打开。\n\n> [!question] 可以嵌套吗？\n>\n> > [!todo] 是的，可以。\n> > > [!example] 你可以使用多层嵌套。\n> > > 在最里面也可以使用 markdown 语法\n>\n> > [!tip]- 还可以嵌套其他 Obsidian 语法\n> > 比如==高亮==，也可以嵌套 wikilink [[introduction-for-k8s]]\n> >\n> > 或者嵌套 Embeded 图片\n> >\n> > ![[test-img-show-image.png]]\n> >\n> > 也可以嵌套 mermaid 图表\n> >\n> > ```mermaid\n> > graph TD\n> > A[Start] --> B[Process]\n> > B --> C[End]\n> > ```\n> >\n> > 也可以嵌套 Excalidraw\n> >\n> > ![[Drawing 2024-04-13 17.33.27.excalidraw]]\n> >\n> > 也可以是 Tag #Tag1 #Tag2\n> >\n> > #单行Tag\n>\n\n> [!abstract]-\n> Abstract 级\n\n> [!summary]-\n> Summary 级, 是 abstract 的 alias\n\n> [!tldr]-\n> TL;DR 级, 是 summary 的 alias\n\n> [!info]-\n> Info 级\n\n> [!todo]-\n> Todo 级\n\n> [!tip]-\n> Tip 级\n\n> [!hint]-\n> Hint 级, 是 tip 的 alias\n\n> [!important]-\n> Important 级, 是 tip 的 alias\n\n> [!success]-\n> Success 级\n\n> [!check]-\n> Check 级, 是 success 的 alias\n\n> [!done]-\n> Done 级, 是 success 的 alias\n\n> [!question]-\n> Question 级\n\n> [!help]-\n> Help 级, 是 question 的 alias\n\n> [!faq]-\n> Faq 级, 是 question 的 alias\n\n> [!warning]-\n> Warning 级\n\n> [!caution]-\n> Caution 级, 是 warning 的 alias\n\n> [!attention]-\n> Attention 级, 是 caution 的 alias\n\n> [!failure]-\n> Failure 级\n\n> [!fail]-\n> Fail 级, 是 failure 的 alias\n\n> [!missing]-\n> Missing 级, 是 failure 的 alias\n\n> [!danger]-\n> Danger 级\n\n> [!error]-\n> Error 级, 是 danger 的 alias\n\n> [!bug]-\n> Bug 级\n\n> [!example]-\n> Example 级\n\n> [!quote]-\n> Quote 级\n\n> [!cite]-\n> Cite 级, 是 quote 的 alias\n\n自定义 callout 类型\n\n> [!reasoning]\n> LLM Reasoning\n\n> [!query]\n> User Query\n\n> [!ai]\n> AI Generated Content\n\n> [!think]\n> Further thinking by writer\n\n> [!idea]\n> Idea 级, 是 think 的 alias\n\n## 标签\n\n文字里可以有 #Tag ， 会被渲染成标签。\n\n#Tag\n\n一行可以有多个 #Tag/Tag2 #Tag3 如果存在于 Tag 索引，则可点击 #Linux\n\n> [!info] Tag 可以与其他组件结合\n> 就像 #Tag 这样\n",
        "title": "博客语法渲染测试",
        "abstract": "**加粗**\n*斜体*\n_斜体2_",
        "length": 477,
        "created_at": "2024-04-14T11:41:29.000Z",
        "updated_at": "2025-03-10T15:26:43.000Z",
        "tags": [
          "Blog",
          "Nextjs"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/first-idea.md",
        "pagePath": "/ideas/first-idea",
        "slug": "first-idea"
      },
      "meta": {
        "content": "# First Idea\nhello! first idea here!",
        "title": "<No Title>",
        "abstract": "hello! first idea here!",
        "length": 2,
        "created_at": "2023-02-04T20:34:05.000Z",
        "updated_at": null,
        "tags": [],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux Clone VS Fork.md",
        "pagePath": "/ideas/Linux Clone VS Fork",
        "slug": "Linux Clone VS Fork"
      },
      "meta": {
        "content": "",
        "title": "Linux Clone VS Fork",
        "abstract": "",
        "length": 1,
        "created_at": "2025-03-09T17:10:04.000Z",
        "updated_at": null,
        "tags": [],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux Systemd.md",
        "pagePath": "/ideas/Linux Systemd",
        "slug": "Linux Systemd"
      },
      "meta": {
        "content": "\n#Linux #systemctl #journalctl #timedatectl #BasicKnowledge #Operation \n\n每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。\n\nSystemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。\n\nsystemctl enable命令用于在上面两个目录之间，建立符号链接关系。\n\n```\n$ sudo systemctl enable clamd@scan.service\n# 等同于\n$ sudo ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service'\n```\n如果配置文件里面设置了开机启动，systemctl enable命令相当于激活开机启动。\n\n与之对应的，systemctl disable命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。\n\n```\n$ sudo systemctl disable clamd@scan.service\n```\n配置文件的后缀名，就是该 Unit 的种类，比如sshd.socket。如果省略，Systemd 默认后缀名为.service，所以sshd会被理解成sshd.service。\n\n\n### 简单例子\n\n```\n[Unit]\nDescription=Ethereum go client\nAfter=syslog.target network.target\n\n[Service]\nType=simple\nWorkingDirectory=/home/ec2-user\nExecStart=/home/ec2-user/bin/geth --http --http.addr 0.0.0.0 --http.vhosts '*' --ws --ws.addr 0.0.0.0 --ws.origins '*' --rinkeby\nUser=ec2-user\nGroup=ec2-user\nKillMode=process\nKillSignal=SIGTERM\nRestart=on-failure\n\n[Install]\nWantedBy=default.target\n```\n\n# timedatectl \n\n时间管理\n\n设置时区\n```shell\nsudo timedatectl set-timezone zone\n```\n\n查看\n```shell\ntimedatectl status\n```\n\n# 日志\n\njournalctl\n\n查看所有日志: 不加任何参数\n```\njournalctl\n```\n\n按单位\n```\njournalctl -u geth\n```\n\n\n参考：\n- https://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html",
        "title": "Linux Systemd",
        "abstract": "每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。\nSystemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。\nsystemctl enable命令用于在上面两个目录之间，建立符号链接关系。",
        "length": 76,
        "created_at": "2024-11-16T07:32:08.000Z",
        "updated_at": null,
        "tags": [
          "Linux",
          "systemctl",
          "journalctl",
          "timedatectl",
          "BasicKnowledge",
          "Operation"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 信号处理 —— Signal.md",
        "pagePath": "/ideas/Linux 信号处理 —— Signal",
        "slug": "Linux 信号处理 —— Signal"
      },
      "meta": {
        "content": "\n#Linux #Signal #BasicKnowledge #Operation \n\n信号是一种中断，为 Linux 提供一种处理异步事件的方法。\n\n [Linux 信号（signal） - 简书](https://www.jianshu.com/p/f445bfeea40a) \n\n\n### 信号的定义\n- 9: `SIGKILL`\n- 2: `SIGINT`\n- 15: `SIGTERM`\n\n### 信号的使用\n- `signal.h` 中，调用 `signal` 函数传入一个 `sighandler_t` 类型的 `handler` ，注册信号处理函数 \n- 用 `kill` 函数对一个 pid 发送一个信号。已注册信号， `kill` 发送的信号都能收到\n- `exec` 函数会将原先要捕捉的信号设置为默认动作\n- `fork` 函数的子进程会继承父进程的信号处理方式 [[Linux Clone VS Fork]]\n\n原型：\n```cpp\n#include <signal.h>\ntypedef void (*sighandler_t)(int);\nsighandler_t signal(int signum, sighandler_t handler);\n```\n\n```cpp\n#include <sys/types.h>\n#include <signal.h>\nint kill(pid_t pid, int sig);\n```\n\n### 未了解\n- [ ] 高级信号发送接收： sigaction , sigqueue\n\n\n# 一文看懂 Linux 信号处理原理与实现\n\n [一文看懂 Linux 信号处理原理与实现-linux 信号处理流程](https://www.51cto.com/article/675743.html) \n\n### 信号处理时内核态\n\n信号处理程序代码运行在用户态\nA 进程向 B 进程发送信号，发起系统调用陷入内核态，内核触发信号处理函数，返回用户态执行信号处理程序，信号处理程序执行完毕后返回到内核态执行收尾工作，再返回用户态继续执行程序\n\n![[Pasted image 20240819014740.png]]\n### 未了解\n- [ ] sigaction 详细解释\n- [ ] signal 在内核态的处理过程\n- [ ]",
        "title": "Linux 信号处理 —— Signal",
        "abstract": "信号是一种中断，为 Linux 提供一种处理异步事件的方法。\n [Linux 信号（signal） - 简书](https://www.jianshu.com/p/f445bfeea40a) \n- 9: `SIGKILL`",
        "length": 50,
        "created_at": "2024-11-16T07:32:08.000Z",
        "updated_at": "2025-03-09T17:10:04.000Z",
        "tags": [
          "Linux",
          "Signal",
          "BasicKnowledge",
          "Operation"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 内存分页、分段.md",
        "pagePath": "/ideas/Linux 内存 —— 内存分页、分段",
        "slug": "Linux 内存 —— 内存分页、分段"
      },
      "meta": {
        "content": "\n#Linux #memory #Operation #BasicKnowledge \n\n> 内存**分页**与内存**分段**是**操作系统**管理**虚拟内存**的方式。\n> \n> 分段：将进程内存分为**代码分段**、**数据分段**、**栈段**、**堆段**等逻辑分明的段。能产生连续的物理空间。容易造成内存碎片，内存交换率低。\n> 分页：将物理内存与虚拟内存都分为一个个**固定大小**的页，内存以页为单位分配、回收、释放。适配局部性原理，碎片利用率高，可以不加载整个内存。\n\n [kernel_memory_management/✍ 文章/一文带你了解，虚拟内存、内存分页、分段、段页式内存管理.md at main · 0voice/kernel_memory_management](https://github.com/0voice/kernel_memory_management/blob/main/%E2%9C%8D%20%E6%96%87%E7%AB%A0/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3%EF%BC%8C%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E3%80%81%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5%E3%80%81%E5%88%86%E6%AE%B5%E3%80%81%E6%AE%B5%E9%A1%B5%E5%BC%8F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.md) \n\n\n前置知识：[[Linux 内存 —— 虚拟内存]]\n\n## 内存分段\n\n内存分段是操作管理虚拟地址与物理地址之间关系的方式之一。\n\n> [!important]\n程序是由若干个逻辑分段组成的，可由**代码分段**、**数据分段**、**栈段**、**堆段**组成。不同的段是有不同的属性的，所以就用分段（_Segmentation_）的形式把这些段分离出来。\n\n### 如何映射\n\n分段机制下的虚拟地址由两部分组成，**段内选择子**和**段内偏移量**。\n\n- 段选择子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。\n- **段内偏移量**应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。\n\n![[Pasted image 20231110131944.png]]\n\n有一些不足之处：\n\n- 第一个就是**内存碎片**的问题。\n- 第二个就是**内存交换的效率低**的问题。\n\n### 内存碎片\n\n内存碎片的问题共有两处地方：\n\n- 外部内存碎片，也就是产生了多个不连续的小物理内存，导致新的程序无法被装载；\n- 内部内存碎片，程序所有的内存都被装载到了物理内存，但是这个程序有部分的内存可能并不是很常使用，这也会导致内存的浪费；\n\n解决外部内存碎片的问题就是**内存交换**。\n\n![[Pasted image 20231110132032.png]]\n\n步骤：\n\n- 将程序内存写到磁盘上\n- 从磁盘上读回内存，并整合内存碎片\n\n> [!info]\n> 可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的时间 256MB 空间，于是新的 200MB 程序就可以装载进来。\n\n这个内存交换空间，在 Linux 系统里，也就是我们经常看到的 **Swap 空间**，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换\n\n## 内存分页\n\n分段的好处就是能产生连续的内存空间，但是会出现内存碎片和内存交换的空间太大的问题。\n\n要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换的**写入或者从磁盘装载的数据更少**一点，这样就可以解决问题了。这个办法，也就是**内存分页**（_Paging_）。\n\n页表实际上存储在 CPU 的**内存管理单元** （_MMU_） 中，于是 CPU 就可以直接通过 MMU，找出要实际要访问的物理内存地址。\n\n> [!important]\n分页是把整个**虚拟和物理内存**空间切成一段段固定大小的**页**（_Page_）。一个页是一个连续并且尺寸固定的内存空间。在 Linux 下，**每一页的大小为 4KB**。\n\n虚拟地址与物理地址之间通过**页表**来映射，如下图：\n\n![[Pasted image 20231110132824.png]]\n\n### 如何解决内存碎片、内存交换效率低\n\n由于内存空间都是预先划分好的，也就不会像分段会产生间隙非常小的内存，这正是分段会产生内存碎片的原因。而采用了分页，那么释放的**内存**都是**以页为单位释放**的，也就不会产生无法给进程使用的小内存。\n\n如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为**换出**（_Swap Out_）。一旦需要的时候，再加载进来，称为**换入**（_Swap In_）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**内存交换的效率就相对比较高。**\n\n更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不是真的把页加载到物理内存里，而是**只有**在程序运行中，**需要用到**对应虚拟内存页里面的指令和数据时，**再加载到物理内存**里面去。\n\n### 如何映射\n\n在分页机制下，虚拟地址分为两部分，**页号**和**页内偏移**。页号作为页表的索引，**页表**包含物理页每页所在**物理内存的基地址**，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。\n\n![[Pasted image 20231110133623.png]]\n\n三个步骤：\n\n- 把虚拟内存地址，切分成页号和偏移量；\n- 根据页号，从页表里面，查询对应的物理页号；\n- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。\n\n### 局部性与 Cache\n\n在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。\n\n可把最常访问的几个页表项存储到访问速度更快的硬件，于是加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（_Translation Lookaside Buffer_） ，通常称为页表缓存、转址旁路缓存、快表等。\n\n## 段页式内存管理\n\n段页式内存管理实现的方式：\n\n- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；\n- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；\n\n![[Pasted image 20231110134320.png]]\n\n这样，地址结构就由**段号、段内页号和页内位移**三部分组成。\n\n段页式地址变换中要得到物理地址须经过三次内存访问：\n\n- 第一次访问段表，得到页表起始地址；\n- 第二次访问页表，得到物理页号；\n- 第三次将物理页号与页内位移组合，得到物理地址。\n\n可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率。",
        "title": "Linux 内存 —— 内存分页、分段",
        "abstract": "> 内存**分页**与内存**分段**是**操作系统**管理**虚拟内存**的方式。\n> \n> 分段：将进程内存分为**代码分段**、**数据分段**、**栈段**、**堆段**等逻辑分明的段。能产生连续的物理空间。容易造成内存碎片，内存交换率低。",
        "length": 114,
        "created_at": "2024-11-16T07:32:08.000Z",
        "updated_at": null,
        "tags": [
          "Linux",
          "memory",
          "Operation",
          "BasicKnowledge"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 堆和栈.md",
        "pagePath": "/ideas/Linux 内存 —— 堆和栈",
        "slug": "Linux 内存 —— 堆和栈"
      },
      "meta": {
        "content": "\n#Linux #memory #BasicKnowledge \n\n> 堆区与栈区都是程序内存分段中的一段。\n\n[[Linux 内存 —— 内存分页、分段]]\n\n## 栈\n\n- 是逻辑栈，连续\n- 有大小限制， Windows 下为 2M\n- 入栈出栈为 CPU 指令，申请、读取快\n\n### 储存内容\n\n- 下条指令地址（ pop 后运行那条指令）\n- 函数参数\n- 函数局部变量\n\n## 堆\n\n- 逻辑上是链表，不连续\n- 基本无大小限制\n- 申请慢，容易产生内存碎片\n\n### 储存内容\n\n- 堆的大小\n- 堆变量",
        "title": "Linux 内存 —— 堆和栈",
        "abstract": "> 堆区与栈区都是程序内存分段中的一段。\n[[Linux 内存 —— 内存分页、分段]]\n- 是逻辑栈，连续",
        "length": 29,
        "created_at": "2024-11-16T07:32:08.000Z",
        "updated_at": null,
        "tags": [
          "Linux",
          "memory",
          "BasicKnowledge"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 内存 —— 虚拟内存.md",
        "pagePath": "/ideas/Linux 内存 —— 虚拟内存",
        "slug": "Linux 内存 —— 虚拟内存"
      },
      "meta": {
        "content": "\n#Linux #memory #Operation #BasicKnowledge \n\n> 每个**进程**分配一套**虚拟内存**地址，由操作系统管理，**进程间独立**互不干涉。\n> 程序访问虚拟地址时，由操作系统转换成物理地址，使得这样不同进程内存不会冲突。\n\n [kernel_memory_management/✍ 文章/一文带你了解，虚拟内存、内存分页、分段、段页式内存管理.md at main · 0voice/kernel_memory_management](https://github.com/0voice/kernel_memory_management/blob/main/%E2%9C%8D%20%E6%96%87%E7%AB%A0/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3%EF%BC%8C%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E3%80%81%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5%E3%80%81%E5%88%86%E6%AE%B5%E3%80%81%E6%AE%B5%E9%A1%B5%E5%BC%8F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.md) \n \nCPU是直接操作内存的物理地址。\n\n两个线程占用内存空间不能重叠，否则会相互擦除。如果直接使用物理内存，需要各线程都要理解其他所有线程占用住的空间，违反隔离原理。因此需要一种隔离内存的机制。\n\n这个机制就是虚拟内存。\n\n> [!important]\n> 每个**进程**分配一套**虚拟内存**地址，由操作系统管理，**进程间独立**互不干涉。\n\n- 程序所使用的内存地址叫做**虚拟内存地址**（_Virtual Memory Address_）\n- 实际存在硬件内存里面的空间地址叫**物理内存地址**（_Physical Memory Address_）\n\n程序访问虚拟地址时，由操作系统转换成物理地址，使得这样不同进程内存不会冲突。\n\n操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存\n\n操作系统通过**内存分页**和**内存分段**等机制来管理虚拟内存与物理内存的映射。\n\n深入：[[Linux 内存 —— 内存分页、分段]]\n\n\n\n## Extra\n\n内存管理是以进程为单元，线程的栈在主进程的堆中。[[Linux 内存 —— 堆和栈]]",
        "title": "Linux 内存 —— 虚拟内存",
        "abstract": "> 每个**进程**分配一套**虚拟内存**地址，由操作系统管理，**进程间独立**互不干涉。\n> 程序访问虚拟地址时，由操作系统转换成物理地址，使得这样不同进程内存不会冲突。\n [kernel_memory_management/✍ 文章/一文带你了解，虚拟内存、内存分页、分段、段页式内存管理.md at main · 0voice/kernel_memory_management](https://github.com/0voice/kernel_memory_management/blob/main/%E2%9C%8D%20%E6%96%87%E7%AB%A0/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3%EF%BC%8C%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E3%80%81%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5%E3%80%81%E5%88%86%E6%AE%B5%E3%80%81%E6%AE%B5%E9%A1%B5%E5%BC%8F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.md) ",
        "length": 33,
        "created_at": "2024-11-16T07:32:08.000Z",
        "updated_at": null,
        "tags": [
          "Linux",
          "memory",
          "Operation",
          "BasicKnowledge"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 文件描述符.md",
        "pagePath": "/ideas/Linux 文件描述符",
        "slug": "Linux 文件描述符"
      },
      "meta": {
        "content": "\n[理解文件描述符与文件句柄 | 黑白之院](https://ifaceless.github.io/2019/12/19/understand-file-descriptor-and-file-description/)",
        "title": "Linux 文件描述符",
        "abstract": "[理解文件描述符与文件句柄 | 黑白之院](https://ifaceless.github.io/2019/12/19/understand-file-descriptor-and-file-description/)",
        "length": 2,
        "created_at": "2024-11-16T07:32:08.000Z",
        "updated_at": null,
        "tags": [],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/Linux 调度 —— 进程与线程.md",
        "pagePath": "/ideas/Linux 调度 —— 进程与线程",
        "slug": "Linux 调度 —— 进程与线程"
      },
      "meta": {
        "content": "\n#Linux #schedule #BasicKnowledge \n\n## 进程\n\n**进程**是计算机**资源分配**最小的单位，是CPU分配资源的基本单位，具有**独立的内存**。\n\n切换需要切换上下文\n\n- 需要切换内存映射表(MMU)\n- 进程栈大小随机，但比线程栈大\n\n\n\n\n## 线程\n\n**线程**是计算机**调度**最小的单位，也是程序执行的最小单位。\n\n线程的内存描述符直接指向父进程的描述符。线程间切换代价比较小，**不需要切换映射表**（MMU），**只是指令间的切换**即可，比较轻量级。线程的栈在主进程的堆中。\n\n- 只需切换各种寄存器\n- 线程共享地址空间\n- 线程栈固定大小，默认 8M ，默认值由操作系统确定， `ulimit -a` 查看， `ulimit -s` 修改\n\n同一进程内的线程间共享：\n- \n\n## 细节\n\n关于 MMU ： [[Linux 内存 —— 内存分页、分段]]\n\n\n内存管理是以进程为单元，线程的栈在主进程的堆中。[[Linux 内存 —— 堆和栈]]\n\n## 上下文切换开销\n\n- 寄存器状态保存、恢复\n- 缓存失效\n- 陷入内核态开销",
        "title": "Linux 调度 —— 进程与线程",
        "abstract": "**进程**是计算机**资源分配**最小的单位，是CPU分配资源的基本单位，具有**独立的内存**。\n切换需要切换上下文\n- 需要切换内存映射表(MMU)",
        "length": 40,
        "created_at": "2024-11-16T07:32:08.000Z",
        "updated_at": null,
        "tags": [
          "Linux",
          "schedule",
          "BasicKnowledge"
        ],
        "license": false
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/newest.mdx",
        "pagePath": "/ideas/newest",
        "slug": "newest"
      },
      "meta": {
        "content": "\n这里是第一行，\n然后这里是第二行。\n\n这里是一些内容。\n再来一行。\n第三行。\n\n| 名称空间      | 隔离内容                      | 内核版本 |\n| :------------ | :---------------------------- | :------- |\n| Mount         | 文件系统与路径等              | 2.4.19   |\n| UTS           | 主机的 Hostname、Domain names | 2.6.19   |\n| IPC           | 进程间通信管道                | 2.6.19   |\n| PID           | 独立的进程编号空间            | 2.6.24   |\n| Network       | 网卡、IP 地址、端口等网络资源 | 2.6.29   |\n| User          | 进程独立的用户和用户组        | 3.8      |\n| Cgroup        | CPU 时间片，内存分页等        | 4.6      |\n| Time \\<- New! | 进程独立的系统时间            | 5.6      |\n\n```mermaid\nsequenceDiagram\nAlice->>John: Hello John, how are you?\nloop Healthcheck\n    John->>John: Fight against hypochondria\nend\nNote right of John: Rational thoughts!\nJohn-->>Alice: Great!\nJohn->>Bob: How about you?\nBob-->>John: Jolly good!\n```\n\n```mermaid\npie\n\"Dogs\" : 386\n\"Cats\" : 85\n\"Rats\" : 15\n```\n\n```mermaid\ngraph LR\n\nohmy-->coll\n\n```\n\nnew lines!\n",
        "title": "Kubernetes 入门 （1）",
        "abstract": "这里是第一行，\n然后这里是第二行。\n这里是一些内容。",
        "length": 47,
        "created_at": "2022-08-13T17:45:31.000Z",
        "updated_at": "2022-08-20T14:02:18.000Z",
        "tags": [
          "Kubernetes",
          "DevOps",
          "Docker",
          "Cloud Native",
          "Cloud Computing"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/ideas/using-chart-js.mdx",
        "pagePath": "/ideas/using-chart-js",
        "slug": "using-chart-js"
      },
      "meta": {
        "content": "\n# testing for using chart js\n\nUse Chart.js in React.\n\nIntroduce:\n\n- Chart.js: https://github.com/chartjs/Chart.js\n- react-chartjs-2: https://github.com/reactchartjs/react-chartjs-2\n- remark-mdx-chartjs: https://github.com/pangelani/remark-mdx-chartjs\n\n# Chart.js\n\nDocumentation:\n\nhttps://www.chartjs.org/docs/latest/\n\nWe should use the Tree-shaking feature for react-chartjs-2.\n\nA very helpful guide about Chart.js:\n\nhttps://www.chartjs.org/docs/latest/getting-started/usage.html\n\n# react-chartjs-2\n\nDocumentation:\n\nhttps://react-chartjs-2.js.org/\n\nwe could code like this:\n\n```jsx\n<Bar\n  data={{\n    labels: [\"January\", \"February\", \"March\"],\n    datasets: [\n      {\n        label: \"Dataset 1\",\n        data: [10, 20, 30],\n        backgroundColor: \"rgba(255, 99, 132, 0.5)\",\n      },\n      {\n        label: \"Dataset 2\",\n        data: [20, 30, 40],\n        backgroundColor: \"rgba(53, 162, 235, 0.5)\",\n      },\n    ],\n  }}\n/>\n```\n\nand result:\n\n<Bar\n  data={{\n    labels: [\"January\", \"February\", \"March\"],\n    datasets: [\n      {\n        label: \"Dataset 1\",\n        data: [10, 20, 30],\n        backgroundColor: \"rgba(255, 99, 132, 0.5)\",\n      },\n      {\n        label: \"Dataset 2\",\n        data: [20, 30, 40],\n        backgroundColor: \"rgba(53, 162, 235, 0.5)\",\n      },\n    ],\n  }}\n/>\n\nAnd it is tree-shakable.\n\nhttps://react-chartjs-2.js.org/docs/migration-to-v4/#tree-shaking\n\n# remark-mdx-chartjs\n\nhttps://github.com/pangelani/remark-mdx-chartjs\n\nWith this, could write chart in markdown.\n\n````md\n```chartjs\ntype: bar\ndata:\nlabels: [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\"]\ndatasets:\n    - label: \"Dataset 1\"\n    data: [0,2,6,1,7,13,2]\n    backgroundColor: \"rgba(255, 99, 132, 0.5)\"\n\n    - label: \"Dataset 2\"\n    data: [4,2,11,8,6,1,4]\n    backgroundColor: \"rgba(53, 162, 235, 0.5)\"\n```\n````\n\n```chartjs\ntype: bar\ndata:\nlabels: [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\"]\ndatasets:\n    - label: \"Dataset 1\"\n    data: [0,2,6,1,7,13,2]\n    backgroundColor: \"rgba(255, 99, 132, 0.5)\"\n\n    - label: \"Dataset 2\"\n    data: [4,2,11,8,6,1,4]\n    backgroundColor: \"rgba(53, 162, 235, 0.5)\"\n```\n\nBut it's not written in TypeScript, and raising a TS7016 error.\nSo raise an issue, and wait for the type definition.\n",
        "title": "About Chart.js",
        "abstract": "Use Chart.js in React.\nIntroduce:\n- Chart.js: https://github.com/chartjs/Chart.js",
        "length": 114,
        "created_at": "2023-02-12T16:36:52.000Z",
        "updated_at": "2023-02-12T16:56:33.000Z",
        "tags": [],
        "license": false
      }
    }
  ],
  "learn_from_ai": [
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2024-11-29-deep-learning-model-formats.md",
        "pagePath": "/learn_from_ai/deep-learning-model-formats",
        "slug": "deep-learning-model-formats"
      },
      "meta": {
        "content": "\n> 本页面介绍了三种机器学习模型文件格式：.pth、.safetensor 和 .onnx。主要内容包括每种格式的定义、保存的内容、用途以及它们之间的区别和转换方法。\n\n# pytorch 保存的 .pth 文件里都保存了些什么东西？\n\nPyTorch中的`.pth`文件通常用于保存模型的权重（parameters）和训练过程中的优化器状态（optimizer state）。具体来说，一个`.pth`文件可能包含以下内容：\n1. **模型状态字典（Model state dictionary）**：\n   - 这是`.pth`文件中最主要的部分，它包含了模型的所有权重（即神经网络的参数），包括卷积层的滤波器、全连接层的权重和偏置等。\n   - 状态字典是一个简单的Python字典对象，其键是层的名字，值是相应的参数张量。\n2. **优化器状态字典（Optimizer state dictionary）**：\n   - 当保存模型时，通常也会保存优化器的状态，这样可以在模型训练中断后，从上次停止的地方继续训练。\n   - 优化器状态字典通常包含每个参数的梯度、动量（momentum）和其他与优化器相关的状态信息（例如Adam优化器的矩估计）。\n3. **其他可选信息**：\n   - 有时候，`.pth`文件可能还会包含其他辅助信息，比如训练的损失值、额外的元数据或者批处理大小等，但这些信息不是必须的。\n以下是一个简单的例子，展示如何在PyTorch中保存和加载`.pth`文件：\n```python\n# 保存模型和优化器状态\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    ...\n}, 'model.pth')\n# 加载模型和优化器状态\ncheckpoint = torch.load('model.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n...\n```\n在这个例子中，`model`是你要保存的模型对象，`optimizer`是训练该模型使用的优化器。通过`torch.save`函数，你可以将模型的状态字典和优化器的状态字典保存到文件中。之后，可以使用`torch.load`函数来恢复这些状态，以便继续训练或者进行推理。\n\n# .safetensor 文件中又保存了什么东西？\n\n`.safetensor` 文件格式是由 Hugging Face 的 Safety Checker 工具引入的，它是为了在模型推理时提供一个额外的安全层。`.safetensor` 文件中保存的内容主要包括：\n1. **模型权重**：\n   - 这是最核心的部分，与 PyTorch 的 `.pth` 文件类似，`.safetensor` 文件也保存了模型的权重，即神经网络中的参数。\n2. **元数据**：\n   - `.safetensor` 文件包含了一些元数据，这些数据描述了模型的架构、权重形状、数据类型等信息，这样可以在加载模型时进行验证和适配。\n3. **安全性信息**：\n   - `.safetensor` 文件可能会包含与模型安全性相关的信息，例如哪些层或参数可能需要特别的安全检查，以及可能的阈值设置等。\n具体来说，`.safetensor` 文件的内容结构可能如下：\n- `weights`: 包含模型权重的字典，通常是以张量的形式存储。\n- `metadata`: 包含模型的元数据，如版本号、架构信息等。\n- `config`: 包含模型配置信息，如模型类型、类名等。\n- `safety`: 包含模型安全性相关的信息。\n使用 `.safetensor` 文件的目的之一是为了确保在模型部署时能够进行有效的安全检查，特别是在处理生成型模型（如文本生成、图像生成等）时，可以防止生成不适当的内容。\n请注意，`.safetensor` 文件格式并不是 PyTorch 的标准保存格式，而是 Hugging Face 的特定实现。因此，它可能不如 `.pth` 文件那样被广泛使用。如果你在使用 Hugging Face 的模型和工具，可能会遇到 `.safetensor` 文件。在使用时，需要确保你的环境支持这种格式。\n\n# .onnx 文件中又保存了什么信息？\n\n`.onnx`（Open Neural Network Exchange）文件是一个开放格式，用于表示机器学习模型。它旨在提供一种标准的方式来表示深度学习模型，使得模型可以在不同的框架和工具之间进行转换和互操作。一个`.onnx`文件中通常包含以下信息：\n1. **模型图（Model Graph）**：\n   - 模型图定义了模型的计算流程，它由节点（nodes）和边（edges）组成。\n   - 节点代表计算操作，如卷积、矩阵乘法、激活函数等。\n   - 边代表节点之间的数据流，即张量（tensors）的流动。\n2. **初始值（Initializers）**：\n   - 初始值包含了模型的权重、偏置等参数的值。\n   - 这些参数是在模型图执行之前就已经确定的。\n3. **模型属性（Attributes）**：\n   - 每个节点可能有与之关联的属性，这些属性定义了节点的具体行为，例如卷积操作中的步长（stride）和填充（padding）。\n4. **输入和输出（Inputs and Outputs）**：\n   - 定义了模型的输入和输出张量的名称、数据类型和形状。\n   - 这有助于理解模型期望的输入格式以及它将产生的输出格式。\n5. **元数据（Metadata）**：\n   - 包括模型的名称、版本、作者、生产日期等信息。\n   - 这些信息有助于模型的追踪和管理。\n6. **优化信息（Optimization Information）**（可选）：\n   - 一些`.onnx`文件可能包含用于优化模型执行的额外信息，如融合操作或特定硬件上的优化指令。\n`.onnx`文件的结构大致如下：\n- `ir_version`: ONNX格式的版本号。\n- `opset_import`: 模型使用的操作集版本。\n- `producer_name`: 生成该ONNX模型的工具或库的名称。\n- `producer_version`: 生成该ONNX模型的工具或库的版本。\n- `graph`: 包含模型图的结构和属性。\n  - `node`: 计算节点列表。\n  - `input`: 输入列表。\n  - `output`: 输出列表。\n  - `initializer`: 初始化参数列表。\n  - `value_info`: 提供额外的张量信息。\n使用`.onnx`文件，可以轻松地将模型从PyTorch、TensorFlow、Caffe2等框架转换到ONNX格式，然后再部署到不同的推理引擎或硬件平台上，例如使用ONNX Runtime进行推理。这种格式提高了模型的便携性和互操作性。\n\n# 简述 .pth ， .safetensor ， .onnx 保存的信息中，有哪些是相同的？有哪些有区别？\n\n.pth, .safetensor, 和 .onnx 文件格式都用于保存机器学习模型的相关信息，但它们的设计目的、兼容性和包含的具体内容有所不同。以下是它们之间相同点和区别的概述：\n**相同点：**\n1. **模型参数/权重**：\n   - 所有三种格式都保存了模型的参数或权重，这些是训练过程中学习到的数值，对于模型的预测能力至关重要。\n2. **元数据**：\n   - 它们都可能包含一些元数据，如模型的版本、创建日期或其他描述性信息，尽管具体的元数据内容和格式可能有所不同。\n**区别：**\n1. **格式和兼容性**：\n   - `.pth` 是 PyTorch 的原生格式，专门用于保存和加载 PyTorch 模型的状态字典，包括模型权重和优化器状态。\n   - `.safetensor` 是 Hugging Face 的 Safety Checker 工具使用的格式，除了模型权重，还包含了安全性相关的信息。\n   - `.onnx` 是一个开放的、跨框架的格式，用于在不同的深度学习框架之间交换模型，不仅包含模型结构和参数，还定义了模型计算图。\n2. **包含内容**：\n   - `.pth` 文件通常包含模型的状态字典和优化器的状态字典，以及可选的其他训练信息（如损失值、迭代次数等）。\n   - `.safetensor` 文件包含模型权重、元数据以及与模型安全性相关的信息。\n   - `.onnx` 文件包含模型计算图、节点定义、边的连接、初始值（模型权重）、输入输出定义以及可选的优化信息。\n3. **使用场景**：\n   - `.pth` 文件主要用于 PyTorch 模型的训练和推理。\n   - `.safetensor` 文件用于在 Hugging Face 的生态系统中进行模型的安全检查。\n   - `.onnx` 文件用于模型在不同框架之间的转换和部署，特别是在需要跨平台或硬件优化的场景中。\n4. **开放性和互操作性**：\n   - `.onnx` 格式的设计重点在于开放性和互操作性，使其成为不同框架和工具之间模型交换的桥梁。\n   - `.pth` 和 `.safetensor` 更专注于特定框架或工具链内的使用。\n总结来说，这三种格式都用于保存模型权重，但它们在格式兼容性、包含的额外信息和使用场景上存在明显差异。`.onnx` 格式更加开放和通用，而 `.pth` 和 `.safetensor` 更专注于特定的框架或应用需求。\n\n# 简述 .pth ， .safetensor ， .onnx 保存的信息中有哪些是相同的？保存的信息中又有哪些有区别？\n\n.pth、.safetensor和.onnx文件格式虽然都是为了保存机器学习模型的相关信息，但它们在保存的内容上存在相同点和差异。\n**相同点：**\n1. **模型参数/权重**：这三种文件格式都会保存模型的参数或权重，这些权重是模型在训练过程中学习到的，对模型的预测能力至关重要。\n2. **元数据**：它们都可能包含一些元数据，例如模型的版本、创建日期或其他描述性信息，尽管具体的元数据和格式可能因文件格式而异。\n**区别：**\n1. **格式和兼容性**：\n   - .pth文件是PyTorch的原生格式，主要用于保存和加载PyTorch模型的状态字典，包括模型权重和优化器状态。\n   - .safetensor文件是Hugging Face的Safety Checker工具使用的格式，除了模型权重，还包括安全性相关的信息。\n   - .onnx文件是一种开放的、跨框架的格式，用于在不同深度学习框架之间交换模型，包含模型结构和参数以及模型计算图。\n2. **包含内容**：\n   - .pth文件通常包含模型的状态字典和优化器的状态字典，可能还包括其他训练信息，如损失值、迭代次数等。\n   - .safetensor文件包含模型权重、元数据以及与模型安全性相关的信息。\n   - .onnx文件包含模型计算图、节点定义、边的连接、初始值（模型权重）、输入输出定义以及可选的优化信息。\n3. **使用场景**：\n   - .pth文件主要用于PyTorch模型的训练和推理。\n   - .safetensor文件用于Hugging Face生态系统中的模型安全检查。\n   - .onnx文件用于模型在不同框架之间的转换和部署，特别是在需要跨平台或硬件优化的场景中。\n4. **开放性和互操作性**：\n   - .onnx格式重点在于开放性和互操作性，便于不同框架和工具之间模型交换。\n   - .pth和.safetensor文件更专注于特定框架或工具链内的使用。\n总结来说，这三种文件格式都保存模型权重，但在格式兼容性、包含的额外信息和使用场景上存在明显差异。.onnx格式更开放和通用，而.pth和.safetensor则更专注于特定框架或应用需求。\n\n# .pth ， .safetensor ， .onnx 三种格式，各自能否用于模型的 finetunning ？为什么？\n\n.pth、.safetensor 和 .onnx 三种格式在模型微调（finetuning）方面的适用性各有不同，以下是各自的适用性分析：\n**.pth 格式：**\n.pth 文件可以用于模型的微调。原因如下：\n- .pth 文件是 PyTorch 的原生格式，它保存了模型的状态字典，包括模型的权重和可选的优化器状态。\n- 在微调过程中，可以加载 .pth 文件中的模型权重，然后根据新的数据集对模型进行进一步的训练。\n- 如果 .pth 文件中包含了优化器状态，那么可以从中断点继续训练，这对于微调过程是有帮助的。\n**.safetensor 格式：**\n.safetensor 文件也可以用于模型的微调，但有一些限制：\n- .safetensor 文件主要是由 Hugging Face 的 Safety Checker 工具使用的，它保存了模型权重和与安全性相关的信息。\n- 虽然这个格式主要用于安全检查，但它仍然包含了模型权重，因此理论上可以用于微调。\n- 但是，由于 .safetensor 文件格式可能不如 .pth 那样被广泛支持，因此在微调时可能需要额外的步骤来转换格式或加载权重。\n**.onnx 格式：**\n.onnx 文件在微调方面的适用性较低，原因如下：\n- .onnx 文件是一个开放的、跨框架的格式，它不仅包含模型权重，还定义了模型计算图。\n- 虽然 .onnx 格式支持在不同的框架之间转换模型，但它不是为了微调而设计的。计算图是固定的，不易于修改。\n- 如果需要微调 .onnx 格式的模型，通常需要先将模型转换回原始框架（如 PyTorch 或 TensorFlow），然后再进行微调。\n- 因此，直接在 .onnx 格式上进行微调通常不切实际，除非有特定的工具或库支持这种操作。\n总结来说，.pth 格式最适合用于 PyTorch 模型的微调，因为它直接与 PyTorch 的训练和推理流程兼容。.safetensor 格式也可以用于微调，但可能需要额外的步骤来处理。而 .onnx 格式通常不直接用于微调，需要转换为其他框架格式后才能进行微调。\n\n# .pth ， .safetensor ， .onnx 三种格式，各自能否用于训练 LoRA ？为什么？\n\nLoRA（Low-Rank Adaptation）是一种用于微调大型预训练模型的技术，它通过引入低秩分解来减少微调所需的参数数量，从而减少存储和计算需求。以下是关于 .pth、.safetensor 和 .onnx 格式在训练 LoRA 时的适用性分析：\n**.pth 格式：**\n.pth 文件可以用于训练 LoRA。原因如下：\n- .pth 文件是 PyTorch 的原生格式，可以直接保存和加载模型的权重，包括 LoRA 引入的低秩矩阵。\n- 在训练 LoRA 时，通常需要在原始预训练模型的基础上添加额外的参数（即低秩矩阵），这些参数可以在 PyTorch 中通过自定义层或修改现有层来实现。\n- 由于 .pth 文件与 PyTorch 框架紧密集成，因此可以轻松地保存和加载 LoRA 的参数，并进行训练。\n**.safetensor 格式：**\n.safetensor 文件也可以用于训练 LoRA，但可能有一些限制：\n- .safetensor 文件是为了提高模型使用的安全性而设计的，它保存了模型权重和其他与安全性相关的信息。\n- 如果 LoRA 的参数被包含在 .safetensor 文件中，那么理论上可以用于训练。\n- 然而，由于 .safetensor 文件格式可能不如 .pth 那样被广泛支持，因此在训练 LoRA 时可能需要额外的步骤来处理文件格式。\n**.onnx 格式：**\n.onnx 文件在训练 LoRA 方面通常不适用。原因如下：\n- .onnx 文件定义了模型的计算图和权重，但它是一个静态的模型描述格式，不适合动态修改模型结构，如添加 LoRA 层。\n- LoRA 需要在预训练模型的特定层中插入低秩矩阵，这通常需要对模型结构进行修改，而 .onnx 格式不支持这种修改。\n- 如果需要使用 .onnx 文件中的模型来训练 LoRA，通常需要先将模型转换回 PyTorch 或其他支持动态图修改的框架，然后再进行训练。\n总结来说，.pth 格式最适合用于训练 LoRA，因为它与 PyTorch 框架紧密集成，支持动态图和自定义层的添加。.safetensor 格式在理论上也是可行的，但可能需要额外的处理步骤。而 .onnx 格式由于不支持动态修改模型结构，因此不适合直接用于训练 LoRA。\n\n# pth ， safetensor ， onnx ，三种格式如何两两转换？\n\n.pth、.safetensor 和 .onnx 文件格式之间的转换通常需要通过特定的工具和库来实现。以下是如何在这三种格式之间进行两两转换的概述：\n### .pth 到 .safetensor 的转换\n目前，并没有直接的标准工具来将 .pth 文件转换为 .safetensor 文件。但是，可以手动加载 .pth 文件中的权重，并将其保存为 .safetensor 格式。以下是一个示例步骤：\n1. 使用 PyTorch 加载 .pth 文件。\n2. 提取模型权重。\n3. 使用 Hugging Face 的 `transformers` 库或其他相关工具将权重保存为 .safetensor 文件。\n### .safetensor 到 .pth 的转换\n同样地，将 .safetensor 文件转换为 .pth 文件也没有直接的转换工具。以下是手动转换的步骤：\n1. 使用 Hugging Face 的 `transformers` 库或其他相关工具加载 .safetensor 文件。\n2. 提取模型权重。\n3. 使用 PyTorch 将权重保存为 .pth 文件。\n### .pth 到 .onnx 的转换\nPyTorch 提供了直接将模型转换为 .onnx 格式的工具：\n```python\nimport torch\nimport torch.onnx\n# 假设 model 是一个 PyTorch 模型\nmodel = ...  # 加载或定义模型\n# 假设输入是一个张量，其形状与模型输入兼容\ninput_sample = torch.randn(batch_size, channels, height, width)\n# 将模型转换为 ONNX 格式\ntorch.onnx.export(model,               # 模型\n                  input_sample,        # 模型输入（或一个包含输入的元组）\n                  \"model.onnx\",       # 输出文件名\n                  export_params=True,  # 是否导出参数\n                  opset_version=10,    # ONNX 算子集版本\n                  do_constant_folding=True,  # 是否执行常量折叠优化\n                  input_names=['input'],   # 输入名\n                  output_names=['output'], # 输出名\n                  dynamic_axes={'input': {0: 'batch_size'},  # 动态轴\n                                'output': {0: 'batch_size'}})\n```\n### .onnx 到 .pth 的转换\n将 .onnx 文件转换为 .pth 文件通常不直接进行，因为 .onnx 文件包含了模型的结构信息，而 .pth 文件只包含权重。如果需要，可以重新创建 PyTorch 模型结构，然后加载 .onnx 文件中的权重：\n1. 定义与 .onnx 文件对应的 PyTorch 模型结构。\n2. 使用 `onnx.load` 加载 .onnx 文件。\n3. 提取权重并将其加载到 PyTorch 模型中。\n### .safetensor 到 .onnx 的转换\n目前没有直接的方法将 .safetensor 文件转换为 .onnx 文件。如果需要，可以先将 .safetensor 文件转换为 .pth 文件，然后再按照上述方法将 .pth 文件转换为 .onnx 文件。\n### .onnx 到 .safetensor 的转换\n同样地，将 .onnx 文件转换为 .safetensor 文件也没有直接的转换工具。如果需要，可以先将 .onnx 文件转换为 .pth 文件，然后再将 .pth 文件转换为 .safetensor 文件。\n在进行这些转换时，需要注意的是，转换过程中可能会丢失一些特定的信息，如优化器状态或模型配置，因此在转换后可能需要额外的步骤来确保模型的完整性和正确性。\n\n# 我有一个从网络上下载的 .pth 文件 `001.pth` 。给我 Python 代码，将其转换为 onnx 格式。\n\n要将一个 `.pth` 文件转换为 `.onnx` 格式，你需要首先加载 `.pth` 文件中的模型，然后使用 PyTorch 的 `torch.onnx.export` 函数来进行转换。以下是一个 Python 代码示例，展示了如何进行这一转换过程。假设你已经有了对应的 PyTorch 模型定义，并且 `.pth` 文件中包含的是该模型的权重。\n请注意，以下代码假设你的模型是一个标准的 PyTorch 模型，并且你知道模型的输入尺��。你需要根据你的模型实际情况来调整输入尺寸和模型结构。\n```python\nimport torch\nimport torch.onnx\n# 假设模型结构和输入尺寸已知\n# 这里是示例模型结构和输入尺寸，你需要根据你的模型进行修改\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        # 定义模型结构\n        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.relu = torch.nn.ReLU()\n        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        # ... 添加更多层 ...\n    def forward(self, x):\n        # 定义前向传播\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        # ... 应用更多层 ...\n        return x\n# 创建模型实例\nmodel = MyModel()\n# 加载.pth文件中的权重\nmodel.load_state_dict(torch.load('001.pth'))\n# 设置模型为评估模式（如果适用）\nmodel.eval()\n# 创建一个符合模型输入尺寸的示例输入\n# 假设输入是单张图片，通道数为3，高和宽分别为224和224\n# 你需要根据你的模型调整输入尺寸\ninput_sample = torch.randn(1, 3, 224, 224)\n# 指定ONNX模型的输出文件名\noutput_file = 'model.onnx'\n# 导出模型为ONNX格式\ntorch.onnx.export(model,               # 模型\n                  input_sample,        # 模型输入（或一个包含输入的元组）\n                  output_file,         # 输出文件名\n                  export_params=True,  # 是否导出参数\n                  opset_version=10,    # ONNX 算子集版本\n                  do_constant_folding=True,  # 是否执行常量折叠优化\n                  input_names=['input'],   # 输入名\n                  output_names=['output'], # 输出名\n                  dynamic_axes={'input': {0: 'batch_size'},  # 动态轴\n                                'output': {0: 'batch_size'}})\n```\n在运行这段代码之前，请确保：\n- 你的环境中已经安装了 PyTorch 和 onnx。\n- 你已经根据你的模型结构正确地定义了 `MyModel` 类。\n- 你已经根据你的模型输入尺寸正确地设置了 `input_sample`。\n运行上述代码后，你应该会在当前目录下得到一个名为 `model.onnx` 的文件，这就是转换后的 ONNX 格式的模型。\n\n# 我没有对应的 PyTorch 模型定义。我要如何获得？\n\n如果你没有模型的 PyTorch 定义，但你有一个 `.pth` 文件，你可以尝试以下几种方法来获取或重建模型的结构：\n### 1. 使用 PyTorch 的 `torch.jit.load`\n如果你的 `.pth` 文件是一个 JIT traced 的模型，你可以使用 `torch.jit.load` 来加载它，这样就不需要原始的模型定义。\n```python\nimport torch\n# 加载JIT traced模型\ntraced_model = torch.jit.load('001.pth')\n# 使用模型（例如进行推理）\n# input_sample = ...  # 创建一个符合模型输入尺寸的示例输入\n# output = traced_model(input_sample)\n```\n### 2. 查找模型的文档或代码\n如果模型是从某个知名的代码库或论文中获得的，你可以尝试查找原始的模型定义代码。通常，论文的附录或代码库的 README 文件会包含模型的定义。\n### 3. 使用第三方库\n有些第三方库可能提供了预定义的模型结构，你可以尝试使用这些库来加载模型。\n例如，如果你有一个在 torchvision 中定义的模型，你可以这样做：\n```python\nimport torch\nimport torchvision.models as models\n# 假设你知道模型是 torchvision 中的某个模型\nmodel = models.resnet50(pretrained=False)\n# 加载权重\nmodel.load_state_dict(torch.load('001.pth'))\n```\n### 4. 反序列化状态字典\n如果你只是丢失了模型定义，但你有状态字典，你可以尝试反序列化状态字典来重建模型结构。\n```python\nimport torch\n# 加载状态字典\nstate_dict = torch.load('001.pth')\n# 假设你知道模型结构，可以创建一个空的模型实例，然后加载状态字典\n# model = ...  # 创建模型实例\n# model.load_state_dict(state_dict)\n```\n### 5. 使用模型架构搜索工具\n有一些工具和库可以帮助你根据权重文件推断出模型结构。例如，`netron` 是一个可视化神经网络工具，可以显示 `.pth` 文件中的模型结构。\n```bash\n# 安装 netron\npip install netron\n# 使用 netron 可视化模型\nnetron start 001.pth\n```\n通过这些方法，你应该能够找到或重建模型的结构。一旦你有了模型定义，你可以使用前面提供的代码示例来将 `.pth` 文件转换为 `.onnx` 格式。如果你无法确定模型结构，你可能需要联系模型的原始作者或查找相关的文档来获取模型定义。\n\n\n",
        "title": "深度学习模型文件格式对比：pth、safetensor 与 onnx",
        "abstract": "> 本页面介绍了三种机器学习模型文件格式：.pth、.safetensor 和 .onnx。主要内容包括每种格式的定义、保存的内容、用途以及它们之间的区别和转换方法。\nPyTorch中的`.pth`文件通常用于保存模型的权重（parameters）和训练过程中的优化器状态（optimizer state）。具体来说，一个`.pth`文件可能包含以下内容：\n1. **模型状态字典（Model state dictionary）**：",
        "length": 322,
        "created_at": "2024-11-29T02:00:00.000Z",
        "updated_at": "2024-11-29T02:00:00.000Z",
        "tags": [
          "PyTorch",
          "ONNX",
          "深度学习",
          "模型部署",
          "学习笔记"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2024-11-29-opencv-coordinate-system-conventions.md",
        "pagePath": "/learn_from_ai/opencv-coordinate-system-conventions",
        "slug": "opencv-coordinate-system-conventions"
      },
      "meta": {
        "content": "\n> 📝 本文是我在学习 OpenCV 时遇到的一个问题。问题的解答和中文翻译均由 AI 生成。\n\n# OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别\n\n## Q\nWhy sometimes opencv represent a point coordinate as (y,x) (e.g. the result of cv2.findContours) , and sometimes it represent as (x, y) (e.g. the arg `center` of cv2.circle)?\n\n> 为什么 OpenCV 有时用 (y,x) 表示坐标点（比如 cv2.findContours 的返回值），有时又用 (x,y) 表示（比如 cv2.circle 函数的 `center` 参数）？\n\n## A\nOpenCV uses different conventions for different functions based on their specific requirements and historical reasons:\n\n> OpenCV 基于不同函数的具体需求和历史原因，使用了不同的坐标表示约定：\n\n1. **(y, x) Convention**: This is often used when dealing with image matrices directly. In image processing, images are typically represented as 2D arrays where the first dimension is the row (height, y-coordinate) and the second dimension is the column (width, x-coordinate). Functions like `cv2.findContours` return coordinates in this order to match the array indexing.\n\n> 1. **(y, x) 约定**：这种表示方法通常用于直接处理图像矩阵时。在图像处理中，图像通常表示为二维数组，其中第一维是行（高度，y坐标），第二维是列（宽度，x坐标）。像 `cv2.findContours` 这样的函数返回这种顺序的坐标是为了匹配数组索引方式。\n\n2. **(x, y) Convention**: This is used in functions that deal with geometric shapes or points in a more traditional Cartesian coordinate system, where `x` is the horizontal axis and `y` is the vertical axis. Functions like `cv2.circle` use this convention to specify the center of the circle.\n\n> 2. **(x, y) 约定**：这种表示方法用于处理几何图形或在传统笛卡尔坐标系中的点，其中 `x` 是水平轴，`y` 是垂直轴。像 `cv2.circle` 这样的函数使用这种约定来指定圆心位置。\n\nThe inconsistency arises because OpenCV aims to balance between image matrix operations and geometric operations, each having their own conventional coordinate systems.\n\n> 这种不一致性的出现是因为 OpenCV 需要在图像矩阵运算和几何运算之间取得平衡，而这两种运算各自有其传统的坐标系统表示方法。",
        "title": "OpenCV 坐标表示：理解 (y, x) 与 (x, y) 的区别",
        "abstract": "> 📝 本文是我在学习 OpenCV 时遇到的一个问题。问题的解答和中文翻译均由 AI 生成。\nWhy sometimes opencv represent a point coordinate as (y,x) (e.g. the result of cv2.findContours) , and sometimes it represent as (x, y) (e.g. the arg `center` of cv2.circle)?\n> 为什么 OpenCV 有时用 (y,x) 表示坐标点（比如 cv2.findContours 的返回值），有时又用 (x,y) 表示（比如 cv2.circle 函数的 `center` 参数）？",
        "length": 26,
        "created_at": "2024-11-29T03:00:00.000Z",
        "updated_at": "2024-11-29T03:00:00.000Z",
        "tags": [
          "OpenCV",
          "图像处理",
          "学习笔记"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2024-12-08-cpp-rvo-and-rust-move-semantics.md",
        "pagePath": "/learn_from_ai/cpp-rvo-and-rust-move-semantics",
        "slug": "cpp-rvo-and-rust-move-semantics"
      },
      "meta": {
        "content": "\n> **参考要点:**\n> - C++的右值引用(T&&)用于捕获临时对象，移动语义通过转移资源所有权避免不必要的复制\n> - 返回值优化(RVO)是编译器优化技术，可以直接在调用方栈帧构造对象，避免复制\n> - 移动语义通过转移资源指针并将源对象指针置空来安全转移资源所有权\n> - Rust也有类似的RVO优化，但更依赖其所有权系统实现严格的移动语义\n> - Rust的借用检查器确保内存安全，防止数据竞争\n\n## Q:\n介绍C++的右值引用和移动语义，并解释函数返回右值引用是如何避免内存拷贝的。\n\n## A:\n### C++的右值引用和移动语义\n\n#### 右值引用\n\n在C++中，表达式可以分为左值（Lvalue）和右值（Rvalue）：\n\n- **左值（Lvalue）**：有名称且可以持久存在的对象，例如变量。左值可以取地址（&）。\n  ```cpp\n  int a = 10; // a 是左值\n  ```\n\n- **右值（Rvalue）**：没有名称且临时对象，例如字面量或表达式的结果。右值不能取地址。\n  ```cpp\n  int b = 20 + 5; // 20 + 5 的结果是右值\n  ```\n\n**右值引用（T&&）** 是专门设计用来捕获右值的引用类型，允许我们安全地修改或转移右值的资源。\n\n#### 移动语义\n\n移动语义是C++11引入的一个重要特性，旨在解决传统复制语义的性能问题。传统的深贝会分配新的内存空间并复制数据，导致性能开销较大，尤其是当对象包含大量资源（如动态分配的内存、大型数组或文件句柄）时。\n\n**移动语义的核心概念**：\n\n1. **移动构造函数**：通过转移资源所有权构造新对象，而不是复制资源。\n   ```cpp\n   class MyClass {\n   public:\n       MyClass(MyClass&& other) noexcept { /* 转移资源 */ }\n   };\n   ```\n\n2. **移动赋值运算符**：通过转移资源所有权赋值，而不是复制资源。\n   ```cpp\n   class MyClass {\n   public:\n       MyClass& operator=(MyClass&& other) noexcept { /* 转移资源 */ return *this; }\n   };\n   ```\n\n### 函数返回右值引用避免内存拷贝\n\n当函数返回一个右值引用时，可以避免不必要的内存拷贝，从而提高程序性能。这是因为右值引用允许直接转移临时对象的资源，而不是创建一个新的副本。\n\n#### 示例\n\n考虑以下：\n\n```cpp\n#include <iostream>\n#include <string>\n\nclass MyClass {\npublic:\n    std::string data;\n\n    MyClass(std::string d) : data(std::move(d)) {\n        std::cout << \"Constructing MyClass with data: \" << data << std::endl;\n    }\n\n    MyClass(MyClass&& other) noexcept : data(std::move(other.data)) {\n        std::cout << \"Moving MyClass with data: \" << data << std::endl;\n    }\n\n    MyClass& operator=(MyClass&& other) noexcept {\n        if (this != &other) {\n            data = std::move(other.data);\n        }\n        return *this;\n    }\n};\n\nMyClass createMyClass() {\n    MyClass temp(\"Temporary\");\n    return temp; // 返回一个右值\n}\n\nint main() {\n    MyClass obj = createMyClass(); // 触发移动构造函数\n    return 0;\n}\n```\n\n#### 解释\n\n1. **函数返回右值**：\n   ```cpp\n   MyClass createMyClass() {\n       MyClass temp(\"Temporary\");\n       return temp; // 返回一个右值\n   }\n   ```\n\n   在这个函数中，`temp`是一个局部对象，当函数返回时，`temp`会被视为一个右值。\n\n2. **移动构造函数被调用**：\n   ```cpp\n   MyClass obj = createMyClass(); // 触发移动构造函数\n   ```\n\n   由于`createMyClass()`返回的是一个右值，编译器会优先选择移动构造函数来构造`obj`，而不是复制构造函数。\n\n3. **资源转移** ：\n   在移动构造函数中，资源的所有权被转移，而不是复制：\n   ```cpp\n   MyClass(MyClass&& other) noexcept : data(std::move(other.data)) {\n       std::cout << \"Moving MyClass with data: \" << data << std::endl;\n   }\n   ```\n\n   这里使用`std::move`将`other.data`的资源转移到新对象中，避免了内存拷贝。\n\n### 总结\n\n通过使用右值引用和移动语义，C++能够有效地避免不必要的拷贝，提高程序的性能。函数返回右值引用时，可以直接利用移动构造函数转移资源，从而避免了深拷贝的开销。这对于包含大量资源的对象尤为重要。\n\n\n## Q:\n\n在你的例子里，为什么 temp 返回时赋值给 obj 可以不发生复制？ temp 应该在栈上不是吗？函数出栈时如果不发生复制，temp不会被回收吗？\n\n## A:\n\n你提出的问题非常关键，它涉及到C++中一个重要的优化机制——**返回值优化（Return Value Optimization, RVO）**和**移动语义**。\n\n### 返回值优化（RVO）\n\n在C++中，编译器通常会进行一种称为返回值优化的优化，以减少不必要的复制。当函数返回一个局部对象时，编译器可以优化代码，避免实际的复制操作。\n\n#### RVO的工作原理\n\n考虑以下函数：\n\n```cpp\nMyClass createMyClass() {\n    MyClass temp(\"Temporary\");\n    return temp;\n}\n```\n\n在这个函数中，`temp`确实是一个局部对象，存储在栈上。按照常规逻辑，当函数返回时，`temp`会被销毁。然而，编译器可以应用RVO来优化这一过程：\n\n1. **编译器优化**：编译器在生成机器代码时，可以决定直接在调用方分配`temp`所需的内存，而不是在函数内部分配。这样，temp`的构造实际上发生在调用方的栈帧上。\n\n2. **避免复制**：由于`temp`的构造直接发生在调用方的栈帧上，返回时不需要进行复制操作。这样，`temp`的生命周期自然延续到调用方的作用域中。\n\n### 移动语义\n\n即使没有RVO，C++11引入的移动语义也能显著减少复制的开销。\n\n#### 移动构造函数的作用\n\n在没有RVO的情况下，函数返回局部对象时，会触发构造函数（而不是复制构造函数），从而避免深拷贝：\n\n```cpp\nMyClass obj = createMyClass();\n```\n\n这里，`createMyClass()`返回的是一个右值（临时对象），因此编译器会调用`MyClass`的移动构造函数来构造`obj`：\n\n```cpp\nMyClass(MyClass&& other) noexcept : data(std::move(other.data)) {\n    std::cout << \"Moving MyClass with data: \" << data << std::endl;\n}\n```\n\n在构造函数中，资源的所有权被转移，而不是复制。这样，即使`temp`在`createMyClass()`函数结束时被销毁，其资源已经安全地转移到了`obj`中，避免了内存泄漏。\n\n### 结合RVO和移动语义\n\n在实际应用中，编译器会优先应用RVO来避免复制。如果RVO不可用或未能应用，编译器会退而求其次，使用移动语义来减少开销。\n\n#### 示例代码优化\n\n考虑以下示例：\n\n```cpp\n#include <iostream>\n#include <string>\n\nclass MyClass {\npublic:\n    std::string data;\n\n    MyClass(std::string d) : data(std::move(d)) {\n        std::cout << \"Constructing MyClass with data: \" << data << std::endl;\n    }\n\n    MyClass(MyClass&& other) noexcept : data(std::move(other.data)) {\n        std::cout << \"Moving MyClass with data: \" << data << std::endl;\n    }\n\n    MyClass& operator=(MyClass&& other) noexcept {\n        if (this != &other) {\n            data = std::move(other.data);\n        }\n        return *this;\n    }\n};\n\nMyClass createMyClass() {\n    MyClass temp(\"Temporary\");\n    return temp; // 可能触发RVO或移动构造\n}\n\nint main() {\n    MyClass obj = createMyClass(); // 触发RVO或移动构造\n    return 0;\n}\n```\n\n在这个示例中：\n\n1. **RVO优化**：编译器可能会应用RVO，直接在`main`函数的栈帧上构造`temp`，避免复制。\n2. **移动语义**：如果RVO未应用，`createMyClass()`返回的临时对象会触发移动构造函数，转移资源。\n\n### 总结\n\n- **RVO**：编译器优化，直接在调用方分配内存，避免复制。\n- **移动语义**：在没有RVO的情况下，通过移动构造函数转移资源，减少复制开销。\n\n通过这两种机制，C++能够有效地避免内存拷贝，提高程序性能。即使局部对象在函数结束时被销毁，其资源已经安全地转移到了调用方，避免了资源泄漏。\n\n## Q:\n\n解释一下你说的“在没有RVO情况下，即使 `temp` 在 `createMyClass()` 函数结束时被销毁，其资源已经安全地转移到了 `obj` 中。” 在我理解里，既然 `temp` 在栈上，那他的所有非指针非引用成员也都会在栈上，那函数出栈时也都会被回收。那这些资源是怎么转移到 `obj` 中而不发生复制的？\n\n## A:\n\n你的疑问非常合理，涉及到C++中资源管理的核心概念。让我们深入探讨一下在没有RVO的情况下，如何通过移动语义实现资源的转移，而不仅仅是简单的栈上内存拷贝。\n\n### 栈上对象和资源管理\n\n首先，明确一点：栈上对象的非指针非引用成员确实是在栈上分配的，并且会在函数结束时被自动销毁。然而，对于包含动态资源（如动态分配的内存、文件句柄等），资源的实际管理是通过指针或引用来实现的。\n\n### 移动语义的核心\n\n移动语义的核心在于**转移资源的所有权**，而不是复制资源。对于包含动态资源的对象，移动操作通常涉及以下步骤：\n\n1. **将源对象的资源指针赋值给目标对象**。\n2. **将源对象的资源指针置为`nullptr`**，以防止其析构时释放资源。\n\n这样，资源的所有权就从源对象转移到了目标对象，而无需进行实际的复制。\n\n### 示例解释\n\n考虑以下示例：\n\n```cpp\n#include <iostream>\n#include <string>\n\nclass MyClass {\npublic:\n    std::string data;\n\n    MyClass(std::string d) : data(std::move(d)) {\n        std::cout << \"Constructing MyClass with data: \" << data << std::endl;\n    }\n\n    // 移动构造函数\n    MyClass(MyClass&& other) noexcept : data(std::move(other.data)) {\n        std::cout << \"Moving MyClass with data: \" << data << std::endl;\n    }\n\n    // 移动赋值运算符\n    MyClass& operator=(MyClass&& other) noexcept {\n        if (this != &other) {\n            data = std::move(other.data);\n        }\n        return *this;\n    }\n};\n\nMyClass createMyClass() {\n    MyClass temp(\"Temporary\");\n    return temp; // 返回一个右值\n}\n\nint main() {\n    MyClass obj = createMyClass(); // 触发移动构造函数\n    return 0;\n}\n```\n\n#### 详细步骤\n\n1. **构造对象**：\n   ```cpp\n   MyClass temp(\"Temporary\");\n   ```\n   这里，`temp`是一个局部对象，其`data`成员是一个`std::string`，可能包含动态分配的内存。\n\n2. **返回临时对象**：\n   ```cpp\n   return temp;\n   ```\n   由于`temp`是一个右值，编译器会优先选择移动构造函数来构造返回值。\n\n3. **移动构造函数被调用**：\n   ```cpp\n   MyClass(MyClass&& other) noexcept : data(std::move(other.data)) {\n       std::cout << \"Moving MyClass with data: \" << data << std::endl;\n   }\n   ```\n   在移动构造函数中，`std::move(other.data)`将`other.data`的资源所有权转移给`data`。具体来说，`std::move`会将`other.data`内部的指针直接赋值给`data`，并将`other.data`内部的指针置为`nullptr`。\n\n4. **资源转移** - `data`现在拥有原来`temp.data`的资源。\n   - `temp.data`被置为`nullptr`，在其析构时不会释放资源。\n\n5. **函数结束时`temp`被销毁**：\n   当`createMyClass()`函数结束时，`temp`会被销毁。但由于其`data`成员的指针已经被置为`nullptr`，析构时不会释放实际资源。\n\n6. **`obj`拥有资源**：\n   ```\n   MyClass obj = createMyClass();\n   ```\n   `obj`通过移动构造函数获得了`temp`的资源，避免了深拷贝。\n\n### 总结\n\n在没有RVO的情况下，移动语义通过以下方式避免复制：\n\n- **转移资源指针**：将源对象的资源指针直接赋值给目标对象。\n- **置空源对象指针**：将源对象的资源指针置为`nullptr`，防止其析构时释放资源。\n\n这样，资源的所有权从源对象安全地到了目标对象，而无需进行实际的资源复制。即使源对象在栈上并在函数结束时被销毁，其资源已经转移到了目标对象中，不会发生资源泄漏。\n\n### 关键点\n\n- **动态资源管理**：通过指针或引用管理动态资源。\n- **移动操作**：转移资源所有权，而不是复制资源。\n- **安全转移**：确保源对象在析构时不会释放已转移的资源。\n\n通过这种方式，C++实现了高效且安全的资源管理，显著提高了性能。\n\n## Q:\n\n在 Rust 中，有没有类似的 RVO 优化或移动语义？\n\n## A:\n\n在Rust中，确实存在类似于C++中的返回值优化（RVO）和移动语义的机制，但Rust的实现方式和术语有所不同。Rust的设计理念强调内存安全和性能，因此它内置了许多优化机制来避免不必要的内存拷贝。\n\n### Rust中的RVO优化\n\nRust编译器（如`rustc`）也会进行返回值优化（RVO），尽管Rust社区不经常使用“RVO”这个术语。ust的编译器会尽量减少不必要的复制，尤其是在函数返回值时。\n\n#### 示例\n\n考虑以下Rust代码：\n\n```rust\nstruct MyStruct {\n    data: String,\n}\n\nimpl MyStruct {\n    fn new(data: String) -> MyStruct {\n        MyStruct { data }\n    }\n}\n\nfn create_my_struct() -> MyStruct {\n    MyStruct::new(\"Temporary\".to_string())\n}\n\nfn main() {\n    let obj = create_my_struct();\n}\n```\n\n在这个示例中，`create_my_struct`函数返回一个`My`实例。Rust编译器可能会应用RVO优化，直接在`main`函数的栈帧上构造`MyStruct`实例，避免不必要的复制。\n\n### Rust中的移动语义\n\nRust的移动语义是其所有权系统的一部分，用于高效地管理资源。Rust中的移动语义比C++更为严格和显式。\n\n#### 移动语义的工作原理\n\n在Rust中，当一个值被移动到另一个变量时，原变量将不再有效，防止了垂引用。\n\n#### 示例\n\n```rust\nfn main() {\n    let x = String::from(\"hello\");\n    let y = x; // x 的所有权移动到 y，x 不再有效\n\n    // 下面的代码会报错，因为 x 已经不再有效\n    // println!(\"x: {}\", x);\n    println!(\"y: {}\", y);\n}\n```\n\n在这个示例中，`x`的所有权移动到`y`，之后`x`不再有效。这种机制确保了内存，避免了悬垂引用。\n\n### Rust中的移动构造\n\nRust的移动构造是通过所有权转移实现的。当一个对象被移动时，其内部资源的所有权也会随之转移。\n\n#### 示例\n\n```rust\nstruct MyStruct {\n    data: String,\n}\n\nimpl MyStruct {\n    fn new(data: String) -> MyStruct {\n        MyStruct { data }\n    }\n}\n\nfn main() {\n    let a = MyStruct::new(\"Temporary\".to_string());\n    let b = a; // a 的所有权移动到 b， 不再有效\n\n    // 下面的代码会报错，因为 a 已经不再有效\n    // println!(\"a: {:?}\", a);\n    println!(\"b: {:?}\", b);\n}\n```\n\n在这个示例中，`a`的所有权移动到`b`，`a`不再有效。`MyStruct`的`data`成员（一个`String`）的所有权也随之转移。\n\n### Rust中的借用检查\n\nRust的借用检查器确保在任意时刻，要么只有一个可变引用，有多个不可变引用，从而防止数据竞争。\n\n#### 示例\n\n```rust\nfn main() {\n    let mut x = String::from(\"hello\");\n    let y = &x; // 不可变借用\n    let z = &mut x; // 可变借用，需要先释放 y 的借用\n\n    println!(\"y: {}\", y);\n    println!(\"z: {}\", z);\n}\n```\n\n在这个示例中，`y`是对`x`的不可变借用，`z`是对x`的可变借用。Rust的借用检查器确保在`z`的可变借用生效前，`y`的借用已经被释放。\n\n### 总结\n\n- **RVO优化**：Rust编译器会进行类似的返回值优化，减少不必要的复制。\n- **移动语义**：Rust通过所有权系统实现严格的移动语义，确保内存安全。\n- **借用检查**：Rust的借用检查器防止数据竞争，确保内存安全。\n\nRust的这些共同作用，提供了高效且安全的资源管理，避免了不必要的内存拷贝，同时确保了程序的内存安全性。\n\n",
        "title": "C++右值引用与Rust移动语义",
        "abstract": "> **参考要点:**\n> - C++的右值引用(T&&)用于捕获临时对象，移动语义通过转移资源所有权避免不必要的复制\n> - 返回值优化(RVO)是编译器优化技术，可以直接在调用方栈帧构造对象，避免复制",
        "length": 466,
        "created_at": "2024-12-08T12:00:00.000Z",
        "updated_at": "2024-12-08T12:00:00.000Z",
        "tags": [
          "C++",
          "Rust",
          "编程语言"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2024-12-08-executable-file-formats.md",
        "pagePath": "/learn_from_ai/executable-file-formats",
        "slug": "executable-file-formats"
      },
      "meta": {
        "content": "\n> 本文介绍了几种主要的可执行文件格式，包括Linux系统使用的ELF格式、Windows系统使用的PE格式、以及macOS系统使用的Mach-O格式等。文章详细讨论了每种格式的特点、使用平台和历史演变，帮助读者理解不同操作系统中二进制文件格式的差异。\n\n## Q: 解释: ELF 执行文件\n\nELF（Executable and Linkable Format）执行文件是一种用于Linux和类Unix操作系统的标准二进制文件格式。它包含了可执行程序、共享库、目标文件和核心转储文件。ELF文件格式设计灵活、可扩展，并且可移植，它定义了程序代码的加载和执行方式，包括代码段、数据段、符号表和重定位信息等。操作系统根据ELF文件中的这些信息来和执行程序。ELF格式也允许程序共享库，即多个程序可以共享同一段代码，从而减少内存占用并提高效率。在软件开发过程中，ELF文件作为目标文件格式在编译时生成，并最终由链接器组合形成可执行文件或共享库。\n\n## Q: 有与 ELF 相并列的另外的一些二进制文件格式吗？\n\n是的，除了ELF（Executable and Linking Format）之外，还有其他几种常见的二进制文件格式，它们在不同的操作系统和平台上被广泛使用。以下是一些主要的二进制文件格式：\n\n1. **PE（Portable Executable）**：\n   - **使用平台**：主要在Windows操作系统上使用。\n   - **特点**：PE格式是COFF（Common Object File Format）的扩展，用于可执行文件、目标文件、动态链接库DLLs）和驱动程序等。PE文件包含一个头部（PE header）、节表（section table）和各个节（sections），如代码节、数据节等。\n\n2. **Mach-O（Mach Object）**：\n   - **使用平台**：主要在Apple的macOS和iOS操作系统上使用。\n   - **特点**：Mach-O格式用于存储可执行文件、目标文件和动态库。它包含多个段（segments和节（sections），支持多种架构，并且可以包含多个架构的代码（称为\"fat binary\"）。\n\n3. **COFF（Common Object File Format）**：\n   - **使用平台**：早期在Unix System V和Windows NT上使用。\n   - **特点**：COFF格式是一种通用的目标文件格式，包含代码、数据和重定位信息。虽然现在较少直接使用，但它的许多概念和结构被PE和ELF继承。\n\n4. **a.out（Assembler Output）**：\n   - **使用平台**：早期Unix系统上使用。\n   - **特点**：a.out是一种较简单的二进制文件格式，包含可执行代码、数据和符号表。随着ELF格式的出现，a.out逐渐被淘汰。\n\n5. **DOS MZ（Mark Zbikowski）**：\n   - **使用平台**：早期DOS和Windows系统上使用。\n   - **特点**：DOSZ格式是DOS可执行文件的格式，以\"MZ\"（开发者Mark Zbikowski的首字母）作为文件头部的标识。它被后来的PE格式所取代。\n\n6. **COM文件**：\n   - **使用平台**：早期DOS系统上使用。\n   - **特点**：COM文件是一种非常简单的可执行文件格式，仅包含代码和数据，没有头部信息，大小限制为64KB。\n\n这些二进制文件格式各有特点，适用于不同的和场景。它们在文件结构、加载机制和功能支持上有所不同，但都旨在实现程序代码的有效存储和执行。\n\n",
        "title": "可执行文件格式对比：ELF、PE、Mach-O等",
        "abstract": "> 本文介绍了几种主要的可执行文件格式，包括Linux系统使用的ELF格式、Windows系统使用的PE格式、以及macOS系统使用的Mach-O格式等。文章详细讨论了每种格式的特点、使用平台和历史演变，帮助读者理解不同操作系统中二进制文件格式的差异。\nELF（Executable and Linkable Format）执行文件是一种用于Linux和类Unix操作系统的标准二进制文件格式。它包含了可执行程序、共享库、目标文件和核心转储文件。ELF文件格式设计灵活、可扩展，并且可移植，它定义了程序代码的加载和执行方式，包括代码段、数据段、符号表和重定位信息等。操作系统根据ELF文件中的这些信息来和执行程序。ELF格式也允许程序共享库，即多个程序可以共享同一段代码，从而减少内存占用并提高效率。在软件开发过程中，ELF文件作为目标文件格式在编译时生成，并最终由链接器组合形成可执行文件或共享库。\n是的，除了ELF（Executable and Linking Format）之外，还有其他几种常见的二进制文件格式，它们在不同的操作系统和平台上被广泛使用。以下是一些主要的二进制文件格式：",
        "length": 38,
        "created_at": "2024-12-08T02:00:00.000Z",
        "updated_at": "2024-12-08T02:00:00.000Z",
        "tags": [
          "Linux",
          "Windows",
          "macOS",
          "可执行文件",
          "学习笔记"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-05-lora-matrix-initialization-strategy.md",
        "pagePath": "/learn_from_ai/lora-matrix-initialization-strategy",
        "slug": "lora-matrix-initialization-strategy"
      },
      "meta": {
        "content": "\n> 本文是与 Deepseek-R1 模型+Search 的对话记录，详细探讨了 LoRA（低秩适应）微调技术中矩阵初始化策略的数学原理。文章分析了为什么在 LoRA 中矩阵 A 采用随机初始化而矩阵 B 采用零初始化的原因，包括保持初始模型稳定性、确保梯度更新有效性、实现逐步学习的低秩更新等方面。同时，文章也解释了为什么反向初始化策略（B随机而A为零）在理论上存在问题，会导致梯度消失和训练效率下降。这些分析有助于理解深度学习中参数高效微调技术的设计原理。\n\n## Q: 初始化 LoRA 时，为什么 A 矩阵是随机初始化而 B 矩阵是 0 初始化？\n\n> [!reasoning]-\n> 嗯，这个问题是关于LoRA（低秩适应）初始化的。为什么在初始化的时候，矩阵A是随机初始化的，而矩阵B是零初始化的呢？我需要仔细思考一下。\n>\n> 首先，我应该回忆一下LoRA的基本原理。LoRA是一种用于微调大型预训练模型的技术，它通过低秩分解来减少可训练参数的数量。具体来说，对于预训练模型中的某个权重矩阵W，LoRA引入两个较小的矩阵A和B，使得原来的权重更新ΔW可以表示为BA。这样，训练的时候只需要更新A和B，而不是整个W，从而节省了计算资源和内存。\n>\n> 那问题来了，为什么初始化的时候，A是随机的，而B是零呢？可能的解释是，这样初始化后，初始的ΔW（即BA）是零矩阵，这样在训练开始时，模型的行为和原始预训练模型保持一致，不会因为随机初始化带来大的扰动。因为如果B初始化为零，无论A是什么，BA都是零，所以ΔW初始为零，这样模型的初始输出不会改变，保证了训练的稳定性。\n>\n> 不过，这里可能有一个疑问：如果B初始化为零，那么在训练过程中，梯度是如何更新的呢？因为如果B一开始是零，那么在第一次反向传播时，B的梯度可能依赖于A的值。而A是随机初始化的，所以可能梯度会有变化。不过，因为A是随机的，初始的BA是零，这样在第一次前向传播时，ΔW不影响原始权重，因此损失函数和原始模型一样，梯度可能从误差信号开始累积，逐步调整A和B的参数。\n>\n> 另外，可能还有参数更新的考虑。例如，如果A是随机初始化，B初始化为零，那么在训练开始时，参数的变化从零开始逐步增加，这类似于残差连接的思想，即初始时没有改变，然后逐渐学习调整。这有助于训练的稳定性，避免了初始阶段的大幅度波动。\n>\n> 再想一下，如果反过来，把A初始化为零，B随机初始化，那么ΔW一开始就是零，因为A是零，这样可能无法有效训练，因为无论B如何变化，如果A是零的话，ΔW始终是零，除非A被更新。但通常来说，参数需要一定的随机性来打破对称性，使得梯度可以有效地更新不同的参数。所以可能如果A初始化为零，那么后续的更新可能会遇到问题，因为梯度可能需要同时更新A和B才能有效，而初始时A的梯度可能很小，导致训练缓慢。因此，将A随机初始化，B初始化为零，可以确保初始的ΔW为零，同时A的随机性允许梯度在反向传播时有效地更新参数，而不会导致初始阶段的大偏差。\n>\n> 总结一下，可能的原因包括：\n>\n> 1. 保持初始ΔW为零，使得模型开始训练时和原始预训练模型一致，训练稳定性。\n>\n> 2. 随机初始化A可以打破对称性，允许梯度有效更新参数，而B初始化为零则保证初始时没有改变权重，这样训练可以逐步进行。\n>\n> 3. 如果B初始化为非零，可能会导致初始ΔW不为零，从而引入不必要的噪声，影响训练初始阶段的稳定性。\n>\n> 可能还有其他原因，比如优化器的动态特性。比如，如果使用Adam这样的优化器，参数的初始步长可能会根据梯度的历史进行调整。如果B初始化为零，那么在最初的几次迭代中，B的梯度可能会较小，从而调整其学习率，而A的随机初始化可能有助于梯度方向的多样性，促进有效的参数更新。\n>\n> 另外，还需要考虑矩阵乘法的顺序。在LoRA中，ΔW = B*A，假设A的维度是d×r，B的维度是r×d，那么BA的乘积是d×d的矩阵，与原权重矩阵W的大小相同。当B初始化为零时，无论A的值如何，ΔW都是零，这样初始的模型输出不会改变。然后，在训练过程中，通过调整B和A来逐渐学习ΔW。这可能比同时随机初始化两者更稳定，因为如果两者都随机初始化，ΔW可能会有一个较大的初始值，影响模型的初始表现。\n>\n> 例如，假设A和B都是随机高斯初始化，均值为0，标准差为σ。那么BA的元素将是两个独立高斯变量的乘积之和，这可能会有较大的方差，导致ΔW的初始值较大，可能干扰预训练模型的原始权重，导致训练初期的不稳定。而将B初始化为零，则避免了这个问题，因为初始时ΔW为零，之后随着训练逐渐调整，这样更安全。\n>\n> 此外，这种初始化方式可能类似于残差网络中的残差块初始化，将残差路径初始化为零，使得初始阶段网络主要依赖原始路径，然后逐步学习残差调整。类似的思路可能被应用到了LoRA中，即初始时不改变原模型，逐步进行低秩调整。\n>\n> 还有一个角度是参数的梯度计算。当B初始化为零时，第一次前向传播中，BA为零，因此在反向传播时，损失函数对B的梯度将依赖于A的值。由于A是随机初始化的，这样B的梯度会有一定的方向性，从而允许B被有效地更新。相反，如果A初始化为零，那么B的梯度也会是零，导致无法更新参数。因此，必须将A随机初始化，以便在第一次反向传播时，B能够获得有效的梯度。\n>\n> 例如，假设损失函数L对B的梯度是∂L/∂B = (∂L/∂(BA)) * A^T。如果A初始化为零，那么梯度∂L/∂B也为零，导致B无法被更新。而如果A是随机初始化的，则梯度不为零，允许B被更新。同时，B初始化为零，那么在第一次更新时，B的值会根据梯度调整，而A也会根据梯度进行更新。这样，两者可以协同工作，逐渐构建出有效的低秩更新矩阵。\n>\n> 因此，总结可能的原因：\n>\n> - 保持初始ΔW为零，维持预训练模型的初始表现，保证训练稳定性。\n>\n> - 随机初始化A打破对称性，允许梯度有效更新B的参数。\n>\n> - 避免初始ΔW的随机性带来的干扰，只通过训练过程逐步调整。\n>\n> - 如果A初始化为非零而B为零，这样BA为零，同时允许梯度在反向传播时有效传递到B，从而启动训练过程。\n>\n> 可能还有其他因素，比如实现上的简便性，或者实验验证过这种初始化方式的效果更好。但核心的数学和优化原因应该是上述几点。\n\n在初始化LoRA（低秩适应）时，矩阵A采用随机初始化而矩阵B初始化为零，主要原因如下：\n\n1. **保持初始模型稳定性**：  \n   当B初始化为零时，无论A的值如何，低秩更新矩阵ΔW = BA初始值为零。这使得微调开始时，模型的输出与原始预训练模型完全一致，避免了初始阶段因随机扰动导致的性能波动，确保了训练的稳定性。\n\n2. **梯度更新的有效性**：  \n   随机初始化A能够打破对称性，使得反向传播时B的梯度依赖于A的随机初始值（∂L/∂B = (∂L/∂ΔW) · Aᵀ）。若A初始化为零，B的梯度将为零，导致训练无法启动；而随机A为非零，使得B的梯度在第一次更新时即可有效调整。\n\n3. **逐步学习低秩更新**：  \n   通过B初始为零，ΔW从零开始逐步积累更新量，类似于残差学习的思想。这种策略让模型优先依赖预训练权重，再逐渐学习低秩调整，避免了初始较大的随机ΔW对预训练知识的干扰。\n\n4. **优化动态的合理性**：  \n   若A和B均随机初始化，ΔW的初始值可能具有较大的方差，引入不必要的噪声。而B初始为零时，ΔW的初始方差为零，训练过程中通过协同优化A和B逐步引入更新，更符合低秩适应的设计目标。\n\n综上，这种初始化方式在数学上确保了训练的高效启动，同时在工程实践中提升了微调的稳定性和效果。\n\n## Q: B矩阵随机初始化而A矩阵初始化时设为 0 不行吗？\n\n> [!reasoning]-\n> 好的，用户问如果B矩阵随机初始化而A矩阵初始化为0行不行。我需要仔细分析这个问题。首先，回想一下之前的对话，用户已经了解了LoRA初始化时A随机，B为0的原因。现在他想反过来，可能是在思考是否有其他可能性。\n>\n> 我需要从原理出发，考虑这种初始化方式对训练的影响。如果A初始化为0，那么ΔW = BA就会变成B乘以0矩阵，也就是全0。这样在训练开始时，微调的模块不会对原模型产生影响，和之前的情况类似。但是这里的关键是梯度如何传播。\n>\n> 根据链式法则，梯度会从损失函数传到ΔW，然后分解到B和A。对于A的梯度是B的转置乘以ΔW的梯度，而B的梯度是ΔW的梯度乘以A的转置。如果A初始为0，那么在第一次反向传播时，B的梯度会是ΔW的梯度乘以0矩阵，导致B的梯度为0。这时候B无法被更新，整个训练就卡住了。\n>\n> 另外，A的参数如果初始为0，在第一次前向时，ΔW是0，所以没问题。但是第一次反向传播时，A的梯度是B转置乘以ΔW的梯度。如果B是随机初始化的，这时候A的梯度可能会有值，可以更新A。但问题在于，当A被更新后，下一次前向时ΔW = B*A会有变化。但这里可能有一个顺序问题，比如第一次迭代时A是0，导致ΔW为0，B的梯度无法更新，而A的梯度可能有效，但之后A被更新后，B才会有机会更新。这时候是否会导致训练不稳定？\n>\n> 可能需要更仔细地分析梯度流。假设A初始为0，B随机。第一次前向时，ΔW=0，不影响模型输出。反向传播时，计算梯度，对A的梯度是∂L/∂A = B^T * ∂L/∂ΔW。而B的梯度是∂L/∂B = ∂L/∂ΔW * A^T。因为A初始为0，所以∂L/∂B会是0，导致B在第一次更新时无法被优化。而A的梯度则取决于B的初始值和ΔW的梯度。如果B是随机初始化的，那么A的梯度不为0，可以更新A。这样在第二次前向时，A已经不是0了，ΔW=B*A会有非零值，此时B的梯度也会开始更新。\n>\n> 这样看起来可能可行，但存在什么问题吗？比如训练初期的不稳定，或者需要更多的迭代才能启动。不过原来的方法中，B初始为0，A随机，可以保证ΔW初始为0，同时B的梯度在第一次更新时就可以通过A的随机值得到有效更新。而反过来，A初始为0，B随机，导致ΔW初始为0，但B的梯度在第一次无法更新，而A可以更新。这样在第一次迭代后，A有了更新值，第二次B才能开始更新。这样可能需要多一个步骤才能让两个矩阵都开始学习，可能影响收敛速度。\n>\n> 另外，当A初始为0，B随机时，虽然ΔW初始为0，但此时A的梯度依赖于B的随机值，可能导致A的初始更新方向带有较大的随机性，而B本身没有被调整，这样可能会引入更多的噪声，影响训练的稳定性。相比之下，原来的方法中，A随机，B为0，这样ΔW初始为0，而B的梯度直接由A的随机值和损失梯度决定，可能更有效。\n>\n> 再考虑优化动态，原来的方法中，B初始为0，意味着在初始阶段，只有A的随机性被引入，但通过梯度下降，B可以快速调整。而反过来，A初始为0，B随机，虽然ΔW初始为0，但A的更新可能更快地引入变化，而B的调整滞后，可能导致训练初期的不稳定。\n>\n> 总结来说，虽然理论上可能存在可行的路径，但实际中原来的初始化方式在梯度更新效率和稳定性上更优。因此，将A随机、B初始化为0是更合理的选择，而反过来可能导致训练效率低下或收敛困难。\n\n将矩阵 **B** 随机初始化而矩阵 **A** 初始化为零，在理论上虽然可能，但会导致训练效率显著下降甚至失败。以下是具体分析：\n\n---\n\n### **1. 梯度消失问题：反向传播的数学限制**\n- **前向传播**：若 **A** 初始化为零，则初始的权重更新为：\n  $$\n  \\Delta W = B \\cdot A = B \\cdot 0 = 0\n  $$\n  此时模型的输出与原始预训练模型一致，与标准 LoRA 初始化类似。\n\n- **反向传播**：\n  - **B 的梯度**：  \n    $$\n    \\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial \\Delta W} \\cdot A^\\top\n    $$\n    由于 **A** 初始化为零，**B** 的梯度在第一次更新时直接为零，导致 **B** 无法被优化（梯度消失）。\n\n  - **A 的梯度**：  \n    $$\n    \\frac{\\partial L}{\\partial A} = B^\\top \\cdot \\frac{\\partial L}{\\partial \\Delta W}\n    $$\n    虽然 **A** 的梯度不为零（取决于随机初始化的 **B**），但 **A** 的更新需要与 **B** 的更新协同才能有效调整权重。由于 **B** 在初始阶段无法更新，**A** 的更新方向会受限于初始随机的 **B**，导致优化过程不稳定。\n\n---\n\n### **2. 训练效率的显著下降**\n- **B 的延迟更新**：  \n  由于 **B** 的梯度在第一次迭代时为零，必须等待 **A** 被更新为非零值后，**B** 的梯度才会开始有效更新。这会引入额外的训练延迟，显著降低收敛速度。\n\n- **参数耦合问题**：  \n  LoRA 的核心设计是 **低秩分解**（$$\\Delta W = BA$$），其有效性依赖于 **A** 和 **B** 的协同优化。若 **A** 初始为零，两者的优化动态被解耦：**A** 的更新依赖于随机初始化的 **B**，而 **B** 的更新又需要等待 **A** 被调整。这种耦合断裂会导致训练过程低效且不稳定。\n\n---\n\n### **3. 与标准 LoRA 初始化的对比**\n- **标准初始化（A随机，B=0）的优势**：  \n  - **B 的梯度有效启动**：  \n    由于 **A** 随机初始化，**B** 的梯度 $$\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial \\Delta W} \\cdot A^\\top$$ 在第一次迭代时即可非零，确保 **B** 立即参与优化。\n  - **对称性打破**：  \n    **A** 的随机性天然打破了参数对称性，避免了梯度冲突。\n  - **稳定的残差学习**：  \n    $$\\Delta W$$ 从零开始逐步增长，符合残差学习的特性，保护了预训练知识。\n\n- **反转初始化（A=0，B随机）的劣势**：  \n  - **B 的梯度消失**：初始阶段 **B** 无法更新，导致训练\"冷启动\"。\n  - **参数优化的冲突**：**A** 的更新方向受限于随机初始化的 **B**，可能引入噪声。\n\n---\n\n### **4. 实验角度的验证**\n在实际任务中（如微调 GPT 或 BERT），若将 **A** 初始化为零且 **B** 随机，通常会导致以下现象：\n1. **训练损失震荡**：由于 **A** 和 **B** 的优化动态不匹配，损失函数难以稳定下降。\n2. **收敛速度缓慢**：需要更多迭代次数才能达到与标准初始化相当的性能。\n3. **最终性能下降**：参数耦合问题可能导致模型陷入次优解。\n\n---\n\n### **结论：为什么不能将 A 初始化为零？**\n- **数学层面**：梯度消失问题导致 **B** 无法在初始阶段更新，破坏了低秩分解的优化动态。\n- **工程层面**：训练效率显著下降，且可能引入不稳定性。\n- **设计哲学**：LoRA 的目标是\"以最小的扰动高效调整预训练模型\"。标准初始化（A随机，B=0）通过梯度协同性和残差学习的特性，完美契合这一目标；而反转初始化违背了这一设计原则。\n\n因此，**A 随机初始化 + B 初始化为零**是唯一合理的初始化策略。\n\n",
        "title": "LoRA微调中的矩阵初始化策略：A随机与B零初始化",
        "abstract": "> 本文是与 Deepseek-R1 模型+Search 的对话记录，详细探讨了 LoRA（低秩适应）微调技术中矩阵初始化策略的数学原理。文章分析了为什么在 LoRA 中矩阵 A 采用随机初始化而矩阵 B 采用零初始化的原因，包括保持初始模型稳定性、确保梯度更新有效性、实现逐步学习的低秩更新等方面。同时，文章也解释了为什么反向初始化策略（B随机而A为零）在理论上存在问题，会导致梯度消失和训练效率下降。这些分析有助于理解深度学习中参数高效微调技术的设计原理。\n> [!reasoning]-\n> 嗯，这个问题是关于LoRA（低秩适应）初始化的。为什么在初始化的时候，矩阵A是随机初始化的，而矩阵B是零初始化的呢？我需要仔细思考一下。",
        "length": 153,
        "created_at": "2025-03-05T02:00:00.000Z",
        "updated_at": "2025-03-05T02:00:00.000Z",
        "tags": [
          "深度学习",
          "LoRA",
          "参数高效微调",
          "模型训练",
          "学习笔记"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-06-pytorch-backpropagation-mechanism.md",
        "pagePath": "/learn_from_ai/pytorch-backpropagation-mechanism",
        "slug": "pytorch-backpropagation-mechanism"
      },
      "meta": {
        "content": "\n> 本文为用户与 DeepSeek 的问答。主题为 PyTorch 的反向传播机制。本文从数学原理、计算图机制和实际需求三个方面深入解释其工作原理，并结合 PyTorch 的自动微分（Autograd）系统进行说明。\n\n## Q; 深入解析 PyTorch 中的反向传播机制\n\n> [!query]\n>\n> 深入解释 torch 中反向传播机制的数学原理与为何需要反向传播机制。提示：你可以使用公式、Mermaid 图表、表格等方式使内容更容易理解。\n\n> [!reasoning]-\n>\n> 嗯，用户让我深入解释 PyTorch 中的反向传播机制的数学原理以及为什么需要它。好的，我得先回忆一下反向传播的基础知识，然后结合 PyTorch 的具体实现来展开。\n>\n> 首先，反向传播是神经网络训练的核心，用于计算梯度。用户可能已经了解梯度下降，但需要更底层的数学解释。我应该从计算图开始，因为 PyTorch 使用动态图机制。每个张量的运算都会记录在计算图中，这样在反向传播时可以自动计算梯度。\n>\n> 数学原理方面，得先讲链式法则。比如，假设有一个简单的函数，y = w*x + b，然后损失函数是 L = (y - t)^2。那么求 L 对 w 的梯度就需要链式法则：dL/dw = dL/dy * dy/dw。反向传播的过程就是从输出层开始，逐层应用链式法则，将梯度传播回各层的参数。\n>\n> 然后需要解释为什么需要反向传播。手动计算梯度在复杂网络中不现实，反向传播自动高效地计算所有参数的梯度，节省了时间和减少错误。可能还需要比较前向传播和反向传播的计算量，比如反向传播如何复用前向的中间结果，提高效率。\n>\n> PyTorch 中的 autograd 机制是关键。每个 Tensor 的.grad_fn 属性记录了创建它的操作，这样在调用.backward()时，会沿着这些函数形成的计算图反向遍历，应用链式法则。需要举例说明，比如构建一个简单的计算图，然后展示梯度计算的过程。\n>\n> 可能用户对计算图的具体实现细节感兴趣，比如 PyTorch 的动态图特性，每次前向传播都会构建新的图，这样更灵活。但数学原理还是链式法则，不管图的结构如何变化，梯度计算的方法是一致的。\n>\n> 另外，可能需要提到为什么不能只用数值方法来近似梯度，比如计算资源的问题，尤其是高维参数时，数值梯度计算量太大。而反向传播通过解析法高效计算，适合深度学习的大规模参数。\n>\n> 还要注意解释叶子节点和中间节点的梯度存储情况，PyTorch 默认只保留叶子节点的梯度，中间节点的梯度会被释放以节省内存。这可能在训练大模型时很重要，用户可能需要了解如何控制梯度的保留。\n>\n> 或许用户对反向传播的数学推导有更深的需求，比如如何扩展到多层网络，或者如何处理复杂的层结构如卷积层、循环层。但根据问题，可能只需要基础数学，比如链式法则的多次应用，以及如何累积各层的梯度。\n>\n> 最后，总结反向传播的必要性：自动化、高效、准确计算梯度，使得训练深层网络可行。而 PyTorch 的自动微分机制让用户无需手动实现这些复杂的计算，只需关注模型结构。\n>\n> 可能需要用公式来展示链式法则的应用，比如多层的梯度相乘。例如，一个三层的网络，每一层的权重梯度如何计算。同时，用 Mermaid 图展示计算图的结构，说明前向和反向的路径。\n>\n> 还有可能要考虑激活函数的导数，比如 ReLU 在反向传播中的处理，这样在计算图中如何处理不可导点，但 PyTorch 的 autograd 会自动处理这些。\n>\n> 总之，我需要从数学原理（链式法则）、计算图的概念、PyTorch 的实现机制（动态图、autograd）、以及反向传播的必要性（高效、自动）这几个方面来组织内容，并用公式和图表辅助说明。\n>\n> 最后，我需要用一个具体的例子，比如一个简单的神经网络，来展示反向传播的整个过程，包括前向传播、反向传播、梯度计算和参数更新。这样用户可以直观地理解反向传播的每个步骤。\n>\n> 如果用户对 PyTorch 的代码实现感兴趣，我也可以提供一些关键代码段的解析，比如如何定义计算图、如何跟踪梯度、如何执行反向传播等。\n>\n> 总之，我需要确保解释清晰，结合数学原理和代码示例，帮助用户理解 PyTorch 中的反向传播机制。\n\n### 深入解析 PyTorch 中的反向传播机制\n\n反向传播（Backpropagation）是神经网络训练的核心算法。本文将从数学原理、计算图机制和实际需求三个方面深入解释其工作原理，并结合 PyTorch 的自动微分（Autograd）系统进行说明。\n\n---\n\n#### 一、反向传播的数学原理\n\n##### 1. 链式法则（Chain Rule）\n\n反向传播本质上是链式法则的高效实现。假设神经网络的计算流程为：\n\n$$\nL = f(g(h(x)))\n$$\n\n其中：\n\n- $$ h(x) = W_1x + b_1 $$\n- $$ g(z) = \\text{ReLU}(z) $$\n- $$ f(y) = \\frac{1}{2}(y - t)^2 $$ （均方误差损失）\n\n则梯度计算为：\n\n$$\n\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial f} \\cdot \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial W_1}\n$$\n\n##### 2. 梯度累积公式\n\n对于任意参数 $$ \\theta $$，梯度计算可表示为：\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\sum_{\\text{path } \\theta \\to \\mathcal{L}} \\left( \\prod_{i} \\frac{\\partial \\text{op}_i}{\\partial \\text{input}_i} \\right)\n$$\n\n---\n\n#### 二、PyTorch 的实现机制\n\n##### 1. 动态计算图（Dynamic Computation Graph）\n\nPyTorch 在前向传播时自动构建计算图，节点为张量，边为运算操作：\n\n```mermaid\ngraph LR\n  x((x)) --> MatMul --> h((h))\n  W1((W1)) --> MatMul\n  b1((b1)) --> Add --> z((z))\n  h --> Add\n  z --> ReLU --> y((y))\n  y --> MSE --> Loss((Loss))\n```\n\n##### 2. Autograd 核心组件\n\n| 组件                   | 作用                                     |\n| ---------------------- | ---------------------------------------- |\n| `Tensor.requires_grad` | 标记需要跟踪梯度的张量                   |\n| `Tensor.grad_fn`       | 记录创建该张量的操作（如 `MulBackward`） |\n| `Tensor.grad`          | 存储计算得到的梯度                       |\n\n##### 3. 反向传播过程\n\n```python\nimport torch\n\nx = torch.tensor([2.0], requires_grad=True)\nW = torch.tensor([3.0], requires_grad=True)\ny = W * x\ny.backward()\n\nprint(W.grad)  # 输出: tensor([2.])\nprint(x.grad)  # 输出: tensor([3.])\n```\n\n---\n\n#### 三、为何需要反向传播？\n\n##### 1. 手动计算不可行性\n\n对于包含百万参数的现代神经网络，手动计算梯度的时间复杂度为 $$ O(n^2) $$ ，而反向传播只需 $$ O(n) $$ 。\n\n##### 2. 计算效率对比\n\n| 方法                 | 时间复杂度   | 空间复杂度 | 数值稳定性 |\n| -------------------- | ------------ | ---------- | ---------- |\n| 数值微分             | $$ O(n^2) $$ | $$ O(1) $$ | 差         |\n| 符号微分             | $$ O(1) $$   | $$ O(n) $$ | 好         |\n| 自动微分（反向传播） | $$ O(n) $$   | $$ O(n) $$ | 优秀       |\n\n##### 3. 内存优化策略\n\nPyTorch 采用以下技术降低内存占用：\n\n- **梯度检查点**：只保留关键节点的中间结果\n- **即时释放中间梯度**：默认只保留叶子节点的梯度\n\n---\n\n#### 四、反向传播的数学扩展\n\n##### 1. 矩阵求导示例\n\n对于全连接层 $$ Y = XW + b $$ ，梯度计算为：\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial Y}\n$$\n\n##### 2. 复杂运算处理\n\n```python\n# PyTorch 自动处理不可导点\nx = torch.tensor([-1.0, 0.0, 2.0], requires_grad=True)\ny = torch.relu(x)\ny.backward(torch.tensor([1.0, 1.0, 1.0]))\nprint(x.grad)  # 输出: tensor([0., 0., 1.])\n```\n\n---\n\n#### 五、总结\n\n**反向传播的必要性**：\n\n1. **自动微分**：避免手工推导复杂导数\n2. **计算高效**：利用链式法则的分解特性\n3. **内存优化**：动态图的即时构建与销毁\n4. **框架支撑**：支持任意复杂网络结构\n\nPyTorch 通过动态计算图和 Autograd 系统，将反向传播的数学原理高效实现，使得深度学习模型的训练变得可行且高效。理解这一机制对于调试模型、优化内存使用和实现自定义算子至关重要。\n\n## Q: 参数的梯度累加计算公式\n\n> [!query]\n>\n> 你提到对于任意参数 $$ \\theta $$，梯度计算可表示为：\n>\n> $$\n> \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\sum_{\\text{path } \\theta \\to \\mathcal{L}} \\left( \\prod_{i} \\frac{\\partial \\text{op}_i}{\\partial \\text{input}_i} \\right)\n> $$\n>\n> 这个公式我没懂，再深入解释一下。我只有本科程度的高等数学知识与线性代数知识，建议一步一步解释推理过程。\n\n> [!reasoning]-\n>\n> 嗯，用户问的是关于反向传播中梯度计算的公式，那个公式看起来有点复杂，对吧？让我先仔细看看用户的问题。他提到，对于任意参数 θ，梯度计算是路径 θ 到 L 的和，每个路径中的各个操作的导数相乘。用户本科数学和线性代数基础，可能需要更基础的解释。\n>\n> 首先，我需要确认用户对这个公式的理解程度。公式里的求和符号和路径可能让他困惑。可能用户对链式法则有一定的了解，但路径的概念不太清楚。比如，在计算图中，参数 θ 到损失 L 可能有多个不同的路径，每个路径对应一个梯度贡献，所以梯度是这些路径贡献的总和。\n>\n> 接下来，我需要用更简单的方式来解释这个公式。或许用一个具体的例子来说明会更好。比如，假设有一个简单的计算图，其中 θ 分两个路径影响 L，每个路径都需要计算导数的乘积，然后将它们相加。\n>\n> 然后，我应该分步骤解释链式法则，以及为什么需要将不同路径的梯度相加。可能需要用图示来辅助，比如画一个简单的计算图，展示多个分支的情况，每个分支对应一个乘积项，最后相加得到总的梯度。\n>\n> 另外，用户可能对“路径”这个词不太理解，需要明确在计算图中的路径指的是什么。例如，一个参数可能在多个层中使用，每次使用都会创建一个不同的路径到损失函数，因此每个路径的梯度都需要被计算并累加。\n>\n> 还要注意，用户可能对乘积的部分有疑问，为什么是各个操作的导数相乘。这时候需要回顾链式法则的基本原理，即复合函数求导时，外层函数的导数和内层函数的导数相乘，而如果有多个中间步骤，就需要连乘这些导数。\n>\n> 可能还需要解释为什么会有多个路径，例如在神经网络中，一个参数可能在多个神经元中被使用，或者在不同的时间步（如 RNN 中），导致梯度需要从各个路径反向传播回来，然后累加起来。\n>\n> 最后，通过一个具体的数学例子，逐步计算每个步骤的导数，并将它们相乘，最后相加，来验证这个公式的正确性。这样用户可以通过实际的计算过程理解抽象的公式。\n>\n> 总结一下，我需要用具体的例子、图示，分步骤解释链式法则在计算图中的多个路径中的应用，以及梯度累加的原因，帮助用户理解这个公式的含义和推导过程。\n\n好的！我将用最直观的方式，通过一个具体例子逐步解释这个公式的数学原理。我们结合图示和分步推导来理解链式法则在反向传播中的应用。\n\n---\n\n### **一、问题场景设定**\n\n假设我们有一个简单的计算图，其中参数 $$\\theta$$ 通过两条路径影响最终损失 $$L$$ ：\n\n```mermaid\ngraph LR\n    θ((θ)) --> Op1 --> A\n    θ --> Op2 --> B\n    A --> Op3 --> L\n    B --> Op3 --> L\n```\n\n- **路径 1**：$$\\theta \\xrightarrow{\\text{Op1}} A \\xrightarrow{\\text{Op3}} L$$\n- **路径 2**：$$\\theta \\xrightarrow{\\text{Op2}} B \\xrightarrow{\\text{Op3}} L$$\n\n目标是计算 $$\\frac{\\partial L}{\\partial \\theta}$$。\n\n---\n\n### **二、分步推导**\n\n#### **步骤 1：前向传播表达式**\n\n假设具体操作为：\n\n- $$A = \\theta^2$$ （Op1：平方操作）\n- $$B = 2\\theta$$ （Op2：线性操作）\n- $$L = A + B$$ （Op3：加法操作）\n\n则最终损失为：\n\n$$\nL = \\theta^2 + 2\\theta\n$$\n\n#### **步骤 2：直接求导（对比验证）**\n\n直接对 $$L$$ 求导：\n\n$$\n\\frac{\\partial L}{\\partial \\theta} = 2\\theta + 2\n$$\n\n---\n\n#### **步骤 3：反向传播视角**\n\n根据计算图，梯度需要沿着两条路径反向传播：\n\n##### **路径 1 的梯度计算（θ → A → L）**\n\n1. $$A = \\theta^2 \\quad \\Rightarrow \\quad \\frac{\\partial A}{\\partial \\theta} = 2\\theta$$\n2. $$L = A + B \\quad \\Rightarrow \\quad \\frac{\\partial L}{\\partial A} = 1$$\n3. 路径 1 的梯度贡献：\n   $$\n   \\frac{\\partial L}{\\partial \\theta} \\bigg|_{\\text{路径1}} = \\frac{\\partial L}{\\partial A} \\cdot \\frac{\\partial A}{\\partial \\theta} = 1 \\cdot 2\\theta = 2\\theta\n   $$\n\n##### **路径 2 的梯度计算（θ → B → L）**\n\n1. $$B = 2\\theta \\quad \\Rightarrow \\quad \\frac{\\partial B}{\\partial \\theta} = 2$$\n2. $$L = A + B \\quad \\Rightarrow \\quad \\frac{\\partial L}{\\partial B} = 1$$\n3. 路径 2 的梯度贡献：\n   $$\n   \\frac{\\partial L}{\\partial \\theta} \\bigg|_{\\text{路径2}} = \\frac{\\partial L}{\\partial B} \\cdot \\frac{\\partial B}{\\partial \\theta} = 1 \\cdot 2 = 2\n   $$\n\n##### **总梯度 = 路径 1 + 路径 2**\n\n$$\n\\frac{\\partial L}{\\partial \\theta} = 2\\theta + 2\n$$\n\n---\n\n### **三、通用公式解析**\n\n原公式：\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\sum_{\\text{path } \\theta \\to \\mathcal{L}} \\left( \\prod_{i} \\frac{\\partial \\text{op}_i}{\\partial \\text{input}_i} \\right)\n$$\n\n#### **1. 符号解释**\n\n- **路径（Path）**：参数 $$\\theta$$ 到损失 $$\\mathcal{L}$$ 的所有可能数据流动路径。\n- **乘积项 $$\\prod$$**：沿某条路径的所有操作（op）的导数连乘（链式法则）。\n- **求和项 $$\\sum$$**：将所有路径的梯度贡献相加（多路径时梯度累加）。\n\n#### **2. 关键性质**\n\n- **链式法则**：复合函数的导数 = 外层导数 × 内层导数。\n- **多路径叠加**：如果参数影响输出的多个分支，梯度是各分支贡献的总和。\n\n---\n\n### **四、复杂案例验证**\n\n考虑一个更复杂的计算图：\n\n```mermaid\ngraph LR\n    θ((θ)) --> Op1 --> C\n    θ --> Op2 --> D\n    C --> Op3 --> E\n    D --> Op4 --> E\n    E --> Op5 --> L\n```\n\n#### **前向传播表达式**\n\n- $$C = \\theta^3$$（Op1）\n- $$D = \\sin(\\theta)$$（Op2）\n- $$E = C \\cdot D$$（Op3）\n- $$L = \\ln(E)$$（Op4）\n\n#### **反向传播计算**\n\n##### **路径 1：θ → C → E → L**\n\n$$\n\\frac{\\partial L}{\\partial \\theta} \\bigg|_{\\text{路径1}} = \\frac{\\partial L}{\\partial E} \\cdot \\frac{\\partial E}{\\partial C} \\cdot \\frac{\\partial C}{\\partial \\theta} = \\frac{1}{E} \\cdot D \\cdot 3\\theta^2\n$$\n\n##### **路径 2：θ → D → E → L**\n\n$$\n\\frac{\\partial L}{\\partial \\theta} \\bigg|_{\\text{路径2}} = \\frac{\\partial L}{\\partial E} \\cdot \\frac{\\partial E}{\\partial D} \\cdot \\frac{\\partial D}{\\partial \\theta} = \\frac{1}{E} \\cdot C \\cdot \\cos(\\theta)\n$$\n\n##### **总梯度**\n\n$$\n\\frac{\\partial L}{\\partial \\theta} = \\frac{D \\cdot 3\\theta^2 + C \\cdot \\cos(\\theta)}{E}\n$$\n\n---\n\n### **五、反向传播的本质**\n\n1. **自动化的链式法则**：框架通过计算图自动追踪操作路径。\n2. **梯度累加规则**：当一个参数影响多个分支时，梯度是所有路径贡献的总和。\n3. **计算效率**：避免重复计算，时间复杂度与网络深度成线性关系（$$O(n)$$）。\n\n---\n\n### **六、总结**\n\n- **为什么需要求和（$$\\sum$$）**：因为参数可能通过多个路径影响损失，每个路径贡献一部分梯度。\n- **为什么需要连乘（$$\\prod$$）**：链式法则要求将路径上的所有局部导数相乘。\n- **反向传播的优势**：将复杂的全局求导分解为局部导数的连乘与路径求和，大幅降低计算复杂度。\n\n通过这种分路径计算再累加的方式，反向传播巧妙地利用计算图的结构信息，实现了高效的梯度计算。\n\n## Q: 从 PyTorch 代码层面深入解释一下 PyTorch 的反向传播如何实现\n\n> [!query]\n>\n> 从 PyTorch 代码层面深入解释一下 PyTorch 的反向传播如何实现\n\n> [!reasoning]-\n>\n> 嗯，用户之前已经了解了反向传播的数学原理和为什么需要它，现在他们想从 PyTorch 代码层面深入理解反向传播的实现。用户可能有一定的编程基础，但可能对 PyTorch 的内部机制不太熟悉。我需要确保解释清晰，结合代码示例，同时避免过于复杂的术语。\n>\n> 首先，用户提到了之前的公式，关于梯度的路径求和，现在需要将这一数学概念与 PyTorch 的具体实现联系起来。他们可能想知道 PyTorch 是如何动态构建计算图的，如何跟踪梯度，以及 backward()方法具体做了什么。\n>\n> 接下来，我应该分步骤解释 PyTorch 的关键组件，比如 Tensor 的 requires_grad 属性、grad_fn、grad 属性，以及动态计算图的构建过程。需要举例说明，比如一个简单的计算图，并展示梯度是如何累积的。同时，用户可能对梯度累积和内存管理感兴趣，比如 PyTorch 如何释放中间变量，如何通过 retain_grad()来保留梯度。\n>\n> 另外，用户可能对多路径梯度累加的情况有疑问，比如当多个操作影响同一个参数时，梯度是如何相加的。这时候需要举例说明，比如一个参数通过两条路径影响损失函数，PyTorch 如何处理这种情况。\n>\n> 还需要提到 Autograd 引擎的工作原理，比如如何遍历计算图，执行反向传播，以及如何优化计算效率。可能还需要讨论一些高级话题，比如 detach()的作用，或者如何自定义反向传播函数，但根据用户的知识水平，可能需要保持基础。\n>\n> 用户可能存在的误区是认为 PyTorch 会保存所有中间变量的梯度，但实际上默认只保留叶子节点的梯度。这需要特别说明，并解释如何通过 retain_grad()来查看中间节点的梯度。\n>\n> 最后，应该总结 PyTorch 反向传播的关键点，强调其动态计算图的优势，以及如何高效管理内存和计算资源。确保用户理解为什么 PyTorch 适合动态图模型，以及在实际调试中如何利用这些机制。\n\n以下是关于 PyTorch 反向传播实现机制的代码级深入解析，结合关键代码段和底层原理说明：\n\n---\n\n### **一、PyTorch 反向传播的三大核心组件**\n\n#### **1. Tensor 的梯度追踪机制**\n\n```python\nimport torch\n\n# 创建需要梯度追踪的张量\nx = torch.tensor([2.0], requires_grad=True)  # 叶子节点\nW = torch.tensor([3.0], requires_grad=True)  # 叶子节点\n\n# 执行计算（自动记录操作）\ny = W * x           # grad_fn=<MulBackward0>\nz = y + 1           # grad_fn=<AddBackward0>\nloss = z.mean()      # grad_fn=<MeanBackward0>\n\nprint(loss.grad_fn)  # 输出: <MeanBackward0 object>\n```\n\n- **`requires_grad=True`**：标记需要追踪梯度的张量（叶子节点）\n- **`grad_fn`**：记录创建该张量的操作（反向传播时知道如何计算梯度）\n\n#### **2. 计算图的动态构建**\n\nPyTorch 的计算图是**动态构建的**，每次前向传播都会新建一个图：\n\n```python\n# 示例：动态图特性\ndef forward(x):\n    if x.sum() > 0:\n        return x * 2\n    else:\n        return x * (-1)\n\nx = torch.tensor([3.0], requires_grad=True)\nout = forward(x)  # 根据输入值动态创建不同的计算图\nout.backward()    # 自动适应不同的分支路径\n```\n\n#### **3. 反向传播引擎（Autograd Engine）**\n\n当调用 `.backward()` 时，PyTorch 会执行以下操作：\n\n```python\nloss.backward()  # 触发反向传播\n```\n\n- **反向遍历计算图**：从 `loss` 的 `grad_fn` 开始，沿着 `next_functions` 递归\n- **计算局部梯度**：在每个操作节点调用已注册的 `backward()` 方法\n- **梯度累加**：将计算结果累积到对应张量的 `.grad` 属性中\n\n---\n\n### **二、代码级反向传播流程分解**\n\n#### **步骤 1：前向传播构建计算图**\n\n```python\na = torch.tensor(2.0, requires_grad=True)\nb = torch.tensor(3.0, requires_grad=True)\nc = a * b      # grad_fn=<MulBackward0>\nd = c + 1      # grad_fn=<AddBackward0>\nloss = d**2    # grad_fn=<PowBackward0>\n```\n\n此时的计算图结构：\n\n```mermaid\ngraph LR\n  a --> Mul --> c\n  b --> Mul\n  c --> Add --> d\n  d --> Pow --> loss\n```\n\n#### **步骤 2：执行反向传播**\n\n```python\nloss.backward()  # 触发反向传播\n```\n\n##### **底层操作分解**：\n\n1. **初始化梯度**：设置 `loss.grad = 1.0`（标量损失的默认梯度）\n2. **反向传播到 d**：\n   - 调用 `PowBackward0` 的 `backward()` 方法：\n     ```python\n     d_grad = 2 * d  # 导数公式：d(loss)/d(d) = 2d\n     ```\n3. **反向传播到 c**：\n   - 调用 `AddBackward0` 的 `backward()` 方法：\n     ```python\n     c_grad = d_grad * 1  # 加法操作的导数为1\n     ```\n4. **反向传播到 a 和 b**：\n   - 调用 `MulBackward0` 的 `backward()` 方法：\n     ```python\n     a_grad = c_grad * b  # 导数公式：d(c)/d(a) = b\n     b_grad = c_grad * a  # 导数公式：d(c)/d(b) = a\n     ```\n5. **梯度累加**：\n   ```python\n   a.grad += a_grad  # 实际为 in-place 操作\n   b.grad += b_grad\n   ```\n\n#### **步骤 3：查看结果**\n\n```python\nprint(a.grad)  # 输出: 2*d* b = 2*(2*3+1)*3 = 42\nprint(b.grad)  # 输出: 2*d* a = 2*(2*3+1)*2 = 28\n```\n\n---\n\n### **三、PyTorch 反向传播的关键实现细节**\n\n#### **1. 梯度计算规则注册**\n\n每个 `torch` 操作都有预定义的梯度计算规则：\n\n```python\n# 示例：自定义操作的梯度规则\nclass MyAdd(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, y):\n        ctx.save_for_backward(x, y)  # 保存前向传播的输入\n        return x + y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, y = ctx.saved_tensors\n        return grad_output * 1, grad_output * 1  # 加法操作的导数\n\n# 使用自定义操作\na = torch.tensor(2.0, requires_grad=True)\nb = torch.tensor(3.0, requires_grad=True)\nc = MyAdd.apply(a, b)\nc.backward()\n```\n\n#### **2. 梯度累加机制**\n\nPyTorch 默认会**累加梯度**，因此每次反向传播前需要手动清零：\n\n```python\noptimizer.zero_grad()  # 标准训练循环中的必要操作\nloss.backward()        # 梯度会累加到 .grad 属性中\noptimizer.step()\n```\n\n#### **3. 非标量张量的反向传播**\n\n当输出为非标量时，需要提供 `gradient` 参数：\n\n```python\nx = torch.tensor([1.0, 2.0], requires_grad=True)\ny = x * 2\n\n# 假设 y 是向量，需要提供梯度初始值\ny.backward(gradient=torch.tensor([0.1, 0.2]))\nprint(x.grad)  # 输出: tensor([0.2, 0.4])\n```\n\n#### **4. 内存优化策略**\n\n- **中间变量释放**：默认只保留叶子节点的梯度\n- **梯度检查点**：通过 `torch.utils.checkpoint` 减少内存占用\n- **禁用梯度计算**：使用 `with torch.no_grad():` 或 `@torch.inference_mode()`\n\n---\n\n### **四、可视化反向传播过程**\n\n#### **示例代码**\n\n```python\nimport torchviz\n\na = torch.tensor(3.0, requires_grad=True)\nb = torch.tensor(4.0, requires_grad=True)\nc = a * b\nd = c.sin()\nd.backward()\n\n# 生成计算图可视化\ntorchviz.make_dot(d, params={'a':a, 'b':b}).render(\"graph\")\n```\n\n#### **生成的计算图结构**\n\n```mermaid\ngraph LR\n  a((a)) --> Mul --> c\n  b((b)) --> Mul\n  c --> Sin --> d((d))\n  d --> Backward\n```\n\n---\n\n### **五、PyTorch 反向传播的性能优化**\n\n#### **1. 异步执行**\n\n```python\n# 使用 CUDA 流实现异步计算\nwith torch.cuda.stream(torch.cuda.Stream()):\n    loss.backward()  # 在非默认流中执行反向传播\n```\n\n#### **2. 混合精度训练**\n\n```python\nscaler = torch.cuda.amp.GradScaler()\n\nwith torch.cuda.amp.autocast():\n    output = model(input)\n    loss = loss_fn(output, target)\n\nscaler.scale(loss).backward()  # 自动缩放梯度\nscaler.step(optimizer)\nscaler.update()\n```\n\n#### **3. 梯度裁剪**\n\n```python\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n```\n\n---\n\n### **六、调试技巧**\n\n#### **1. 检查梯度是否存在**\n\n```python\nprint([p.requires_grad for p in model.parameters()])  # 检查参数是否启用梯度\n```\n\n#### **2. 梯度数值检查**\n\n```python\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f\"{name} 梯度均值: {param.grad.mean().item()}\")\n```\n\n#### **3. 保留中间梯度**\n\n```python\ny = x * 2\ny.retain_grad()  # 强制保留中间变量的梯度\ny.backward()\nprint(y.grad)    # 可以查看中间梯度\n```\n\n---\n\n### **七、底层实现原理**\n\n#### **1. C++ 核心引擎**\n\nPyTorch 的反向传播引擎主要由以下 C++ 组件构成：\n\n- **`torch::autograd::Engine`**：管理反向传播任务的执行\n- **`torch::autograd::Node`**：表示计算图中的操作节点\n- **`torch::autograd::Edge`**：表示节点之间的连接关系\n\n#### **2. 反向传播任务调度**\n\n```cpp\n// 伪代码：引擎执行流程\nvoid Engine::execute(const EdgeList& roots) {\n    // 1. 构建任务队列\n    // 2. 使用线程池并行执行反向任务\n    // 3. 处理依赖关系（拓扑排序）\n    // 4. 调用每个 Node 的 apply() 方法\n}\n```\n\n---\n\n### **八、总结：PyTorch 反向传播的设计哲学**\n\n1. **动态计算图**：即时构建、即时销毁，灵活适应可变网络结构\n2. **延迟计算**：只在需要时执行梯度计算，优化内存使用\n3. **分离式设计**：前向传播记录操作，反向传播时执行实际计算\n4. **可扩展性**：通过注册自定义 Function 支持任意复杂操作\n\n理解这些底层机制，可以帮助开发者：\n\n- 更高效地调试梯度相关问题\n- 合理控制内存使用（如使用 `detach()` 截断计算图）\n- 实现自定义神经网络层和损失函数\n- 优化训练过程的性能\n\n## Q: 深入讲解几种微分方法的复杂度与数值稳定性\n\n> [!query]\n>\n> 你之前提到数值微分、符号微分、自动微分的时间空间复杂度以及数值稳定性的对比。这些复杂度是如何计算出的结果？数值稳定性又如何评估？\n\n> [!reasoning]-\n>\n> 好的，我现在需要仔细理解用户的问题。用户之前询问了 PyTorch 的反向传播机制，现在针对数值微分、符号微分和自动微分的时间空间复杂度以及数值稳定性对比提出疑问，想知道这些复杂度是如何计算的，以及数值稳定性如何评估。用户可能在学习这些微分方法时，对背后的数学原理和评估方法感到困惑，需要更深入的解释。\n>\n> 首先，我需要回忆数值微分、符号微分和自动微分的基本概念。数值微分是通过有限差分近似导数，符号微分是通过代数规则求导表达式，自动微分（如反向传播）则是通过分解计算过程，利用链式法则高效计算梯度。用户已经知道它们的复杂度对比表格，但需要了解这些复杂度是如何得出的，以及数值稳定性的评估方法。\n>\n> 接下来，时间复杂度的计算。对于数值微分，假设有 n 个参数，每个参数都需要单独计算梯度，每次计算需要两次前向传播（f(x+h)和 f(x-h)），所以总的时间复杂度是 O(n)的前向计算乘以 n 个参数，得到 O(n^2)。而自动微分只需要一次前向和一次反向传播，反向传播的时间复杂度与前向相当，所以总的是 O(n)。符号微分则可能因为表达式膨胀导致最坏情况指数级复杂度，但通常情况是线性的，但需要存储整个表达式，空间复杂度高。这些需要详细解释。\n>\n> 空间复杂度方面，数值微分不需要存储中间变量，所以是 O(1)。自动微分需要存储计算图中的中间结果，所以是 O(n)。符号微分需要存储整个符号表达式，可能随着求导次数增加而膨胀，所以空间复杂度是 O(n)。这里可能需要更具体的例子说明。\n>\n> 数值稳定性方面，数值微分因为使用了有限差分，容易受到舍入误差影响，尤其是当 h 很小时。符号微分因为是精确求导，稳定性好。自动微分通过链式法则计算，精度与数学推导相同，所以稳定性优秀。需要解释为什么不同方法会有不同的稳定性，比如舍入误差和表达式膨胀的影响。\n>\n> 用户可能没有完全理解这些复杂度是如何推导出来的，比如为什么自动微分的时间是线性的，或者符号微分的空间复杂度如何。需要分步骤解释，可能用具体的例子，比如一个有 n 个参数的函数，如何计算每种方法所需的步骤数，从而得出时间复杂度。\n>\n> 此外，用户可能对数值稳定性的评估方法不清楚。需要解释数值稳定性通常通过误差分析，比如舍入误差和截断误差的影响。例如，数值微分的误差来源包括选择 h 的大小，过大会导致截断误差，过小导致舍入误差。而符号微分没有数值误差，自动微分则因为使用浮点运算可能有舍入误差，但相对于数值微分更稳定。\n>\n> 需要注意用户的数学背景是本科高等数学和线性代数，所以需要用他们能理解的语言，避免过于复杂的数学推导，但又要足够严谨。可能需要使用具体的例子，比如一个简单的函数，比较三种方法在计算梯度时的步骤，从而说明时间复杂度的差异。\n>\n> 例如，考虑函数 f(w)=w1*x1 + w2*x2 + ... + wn\\*xn，有 n 个参数 w1 到 wn。数值微分需要对每个 wi 计算两次 f 的值，所以总计算次数是 2n 次，每次计算需要 O(n)时间（因为计算点积），所以总时间复杂度是 O(n^2)。自动微分则计算一次前向传播（O(n)时间），反向传播每个节点的梯度计算是 O(1)，n 个节点，所以总时间是 O(n)。符号微分则生成导数表达式，可能有 n 项，每个项是 xi，所以导数是 xi，存储这些需要 O(n)空间，但如果是更复杂的函数，比如多层复合函数，符号表达式可能膨胀，导致空间增加。\n>\n> 在数值稳定性方面，比如用数值微分计算 f(w)=sin(w)，当 w 很大时，计算导数时的小变化可能导致较大的舍入误差。而自动微分直接计算 cos(w)，没有这种误差。\n>\n> 总结，用户需要分步骤的复杂度推导过程和数值稳定性的评估方法，结合具体例子和数学解释，确保他们能理解为什么不同方法有不同的复杂度和稳定性。同时，可能需要指出这些复杂度是理论上的最坏情况还是典型情况，比如符号微分的表达式膨胀在某些情况下可能更严重。\n\n以下是关于数值微分、符号微分和自动微分的时间/空间复杂度计算原理，以及数值稳定性评估方法的详细解释：\n\n---\n\n### 一、时间复杂度计算原理\n\n#### 1. 数值微分（Numerical Differentiation）\n\n**实现方式**：通过有限差分近似导数\n$$ f'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h} $$\n\n**时间复杂度分析**：\n\n- **单参数计算**：每次计算需要 **2 次前向传播**\n- **n 个参数**：需要 **2n 次前向传播**\n- **前向传播复杂度**：假设单次前向传播为 $$ O(k) $$（k 为计算步骤数）\n- **总时间复杂度**：$$ O(2n \\times k) = O(nk) $$\n\n**关键结论**：时间复杂度与参数数量成线性关系，但前向传播的 $$ k $$ 通常与网络规模相关，实际复杂度为 $$ O(n^2) $$\n\n> 示例：计算 $$ f(w*1,w_2,...,w*{100}) $$ 的梯度，需要 200 次前向传播\n\n---\n\n#### 2. 符号微分（Symbolic Differentiation）\n\n**实现方式**：通过代数规则推导闭式表达式\n\n**时间复杂度分析**：\n\n- **表达式膨胀**：对复合函数 $$ f(g(h(x))) $$ 求导会产生嵌套乘积项\n- **最坏情况**：每层操作导致表达式长度指数级增长 $$ O(2^d) $$（d 为网络深度）\n- **优化情况**：现代系统会进行表达式简化，典型复杂度为 $$ O(d) $$\n\n> 示例：对 $$ f(x) = \\sin(e^{x^2}) $$ 求导：\n> $$ f'(x) = \\cos(e^{x^2}) \\cdot e^{x^2} \\cdot 2x $$\n> 表达式长度随求导次数线性增长\n\n---\n\n#### 3. 自动微分（Automatic Differentiation）\n\n**实现方式**：基于计算图分解求导过程\n\n**时间复杂度分析**：\n\n- **前向传播**：$$ O(d) $$（d 为计算图深度）\n- **反向传播**：每个节点执行固定时间的梯度计算，复杂度 $$ O(d) $$\n- **总时间复杂度**：$$ O(d) + O(d) = O(d) $$\n\n**关键优化**：计算图节点数通常与参数数量 $$ n $$ 成线性关系，因此复杂度可表示为 $$ O(n) $$\n\n> 示例：全连接网络的反向传播时间与参数数量成正比\n\n---\n\n### 二、空间复杂度计算原理\n\n#### 对比表格\n\n| 方法     | 空间复杂度 | 原因                                                       |\n| -------- | ---------- | ---------------------------------------------------------- |\n| 数值微分 | $$ O(1) $$ | 只需存储当前参数值和扰动结果                               |\n| 符号微分 | $$ O(n) $$ | 需要存储完整的符号表达式（可能指数膨胀，但优化后线性增长） |\n| 自动微分 | $$ O(n) $$ | 需要存储计算图中所有中间变量的前向值                       |\n\n**详细解释**：\n\n- **数值微分**：仅需保持原始参数和扰动后的参数副本，内存占用恒定\n- **符号微分**：导数的符号表达式可能非常庞大，例如：\n  $$ \\frac{\\partial}{\\partial x} \\prod*{i=1}^n x_i = \\sum*{j=1}^n \\prod\\_{i \\neq j} x_i $$\n  需要存储 $$ n $$ 个乘积项\n- **自动微分**：需要保存前向传播的所有中间结果用于反向计算，例如：\n  ```python\n  # 前向计算需要保存这些值\n  a = W1 * x\n  b = a + b1\n  c = relu(b)\n  ```\n\n---\n\n### 三、数值稳定性评估方法\n\n#### 1. 数值微分稳定性\n\n**误差来源**：\n\n- **截断误差**：$$ O(h^2) $$（来自泰勒展开的近似）\n- **舍入误差**：$$ O(\\epsilon/h) $$（$$ \\epsilon $$ 为机器精度）\n\n**稳定性分析**：\n$$ \\text{总误差} = O(h^2) + O(\\epsilon/h) $$\n\n- **最优步长选择**：当 $$ h \\approx \\sqrt[3]{\\epsilon} $$ 时误差最小\n- **典型问题**：对于病态条件数的问题（如指数函数），小步长会放大舍入误差\n\n> 示例：计算 $$ f(x) = e^x $$ 在 $$ x=20 $$ 处的导数：\n>\n> - 真实导数：$$ e^{20} \\approx 4.85 \\times 10^8 $$\n> - 数值计算可能产生显著相对误差\n\n---\n\n#### 2. 符号微分稳定性\n\n**特点**：\n\n- **精确计算**：理论上没有数值误差\n- **实际限制**：\n  - 符号化简可能导致数值不稳定表达式\n  - 转换为浮点运算时仍会引入舍入误差\n\n**评估方法**：\n\n- **表达式复杂度**：检查导数表达式是否包含病态操作（如大数相减）\n  $$ \\frac{\\partial}{\\partial x} (e^x - e^{-x}) = e^x + e^{-x} $$\n  直接计算可能在 $$ x $$ 较大时出现精度损失\n\n---\n\n#### 3. 自动微分稳定性\n\n**误差来源**：\n\n- **前向传播舍入误差**：与符号微分相同量级\n- **反向传播稳定性**：取决于梯度计算路径\n\n**稳定性优势**：\n\n- **避免误差累积**：通过保持原始计算路径的数值特性\n- **典型对比**：\n  - 直接计算 $$ f(x) = \\sqrt{x^2 + 1} - x $$ 的导数\n  - 符号微分可能得到不稳定的 $$ \\frac{x}{\\sqrt{x^2+1}} - 1 $$\n  - 自动微分在计算时保持原始表达式结构\n\n---\n\n### 四、综合对比与工程选择\n\n#### 1. 方法选择决策树\n\n```mermaid\ngraph TD\n    A[需要微分] --> B{参数规模}\n    B -->|\"小规模(n<100)\"| C[符号微分]\n    B -->|中大规模| D{是否需要最高精度}\n    D -->|是| E[自动微分]\n    D -->|否| F[数值微分]\n    C --> G[获得精确表达式]\n    E --> H[高效计算梯度]\n    F --> I[快速但低精度]\n```\n\n#### 2. 实际框架实现\n\n- **PyTorch/TensorFlow**：采用自动微分（反向传播）\n- **Mathematica**：使用符号微分\n- **有限差分库**：常用于验证其他方法的正确性\n\n---\n\n### 五、深度学习中的特殊考量\n\n#### 1. GPU 内存瓶颈\n\n- **自动微分优化**：PyTorch 的 `checkpoint` 机制通过牺牲计算换内存\n  ```python\n  # 减少中间变量存储\n  from torch.utils.checkpoint import checkpoint\n  x = checkpoint(layer1, x)\n  x = checkpoint(layer2, x)\n  ```\n\n#### 2. 混合精度训练\n\n- **稳定性挑战**：float16 类型下的梯度计算需要特殊处理\n- **解决方案**：梯度缩放（Gradient Scaling）\n  ```python\n  scaler = torch.cuda.amp.GradScaler()\n  with torch.cuda.amp.autocast():\n      output = model(input)\n  scaler.scale(loss).backward()\n  ```\n\n---\n\n### 六、数学证明示例\n\n#### 数值微分误差分析\n\n设真实导数为 $$ f'(x) $$，数值近似为：\n$$ \\tilde{f}'(x) = \\frac{f(x+h) - f(x-h)}{2h} $$\n\n**泰勒展开**：\n$$ f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(ξ_1) $$\n$$ f(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - \\frac{h^3}{6}f'''(ξ_2) $$\n\n**相减得**：\n$$ \\tilde{f}'(x) = f'(x) + \\frac{h^2}{12}[f'''(ξ_1) + f'''(ξ_2)] $$\n\n**误差项**：\n$$ \\text{截断误差} = O(h^2) $$\n\n---\n\n### 七、总结理解\n\n1. **复杂度计算本质**：\n\n   - 时间复杂度：计算代价随问题规模的增长率\n   - 空间复杂度：内存需求随问题规模的增长率\n\n2. **稳定性评估核心**：\n\n   - 对舍入误差和截断误差的敏感度\n   - 保持数值计算精度的能力\n\n3. **工程实践指导**：\n   - 小规模问题：优先符号微分（如物理仿真）\n   - 大规模可微编程：必选自动微分（深度学习框架）\n   - 快速验证：使用数值微分（原型开发阶段）\n\n> [!NOTE] 亮曰：\n>\n> - 数值微分：参数做 Delta ，计算最终值的差，没有反向传播过程\n> - 符号微分：直接推导出导数表达式，再代入计算，表达式复杂\n> - 自动微分：前向传播计算每个节点的值，再反向传播用链式法则计算每个节点的梯度。\n>\n> 自动微分可以算是一种动态规划，暂存计算中重复用到的中间数据，用空间换时间。更进一步，反向传播时，用层序遍历计算图，当节点的所有入边（反向时的出边）都计算完后，即可将当前节点从队列中移除，从而节省内存。\n",
        "title": "PyTorch 自动微分与反向传播机制原理解析",
        "abstract": "> 本文为用户与 DeepSeek 的问答。主题为 PyTorch 的反向传播机制。本文从数学原理、计算图机制和实际需求三个方面深入解释其工作原理，并结合 PyTorch 的自动微分（Autograd）系统进行说明。\n> [!query]\n>",
        "length": 952,
        "created_at": "2025-03-06T15:50:00.000Z",
        "updated_at": "2025-03-06T15:50:00.000Z",
        "tags": [
          "PyTorch",
          "深度学习",
          "反向传播",
          "自动微分",
          "学习笔记"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-07-golang-new-and-memory-management.md",
        "pagePath": "/learn_from_ai/golang-new-and-memory-management",
        "slug": "golang-new-and-memory-management"
      },
      "meta": {
        "content": "\n> 本文为与 GPT-4o 的对话记录，深入探讨 GoLang 中 new() 函数的使用以及相关的内存管理机制。\n\n# Q: Golang 中， new() 如果内存申请不成功会发生什么？\n\n在 Go 语言中，`new()` 函数用于分配内存并返回指向该内存的指针。`new()` 函数的行为是非常简单且安全的：它总是返回一个指向零值的指针。Go 语言的内存分配器会确保内存分配成功，因此在使用 `new()` 时，不会出现内存分配失败的情况。\n\n换句话说，`new()` 函数在 Go 中不会返回 `nil`，因为它总是能够成功分配内存并返回一个有效的指针。Go 语言的设计目标之一就是提供一个安全的内存管理机制，因此开发者不需要担心内存分配失败的问题。\n\n如果你有任何其他关于 Go 语言的问题，欢迎随时提问！\n\n# Q: 那如果机器上的内存已经满了，调用 new() 会发生什么？\n\n在 Go 语言中，如果机器上的内存已经满了，调用 `new()` 或其他内存分配函数（如 `make()` 或 `append()`）时，Go 运行时会尝试分配内存。如果内存确实不足，Go 运行时会引发一个运行时错误（runtime panic），通常是 `runtime: out of memory`。\n\n这种情况会导致程序崩溃，因为 Go 语言的内存分配器无法满足内存请求。Go 语言的设计目标之一是简化内存管理，因此它不提供手动处理内存分配失败的机制。相反，Go 依赖于垃圾回收和自动内存管理来尽量减少内存不足的情况。\n\n因此，在编写 Go 程序时，开发者通常不需要直接处理内存分配失败的情况，但需要注意程序的内存使用情况，尤其是在处理大数据集或长时间运行的服务时，以避免内存耗尽。\n\n# Q: new 返回的对象一定在堆上吗？如果逃逸分析发现这个对象没有逃逸，也会在堆上吗？\n\n在 Go 语言中，`new()` 返回的对象不一定总是在堆上分配。Go 编译器使用一种称为逃逸分析（escape analysis）的技术来决定对象是分配在堆上还是栈上。\n\n如果逃逸分析发现一个对象没有逃逸出其函数的作用域（即，它不会在函数返回后继续被使用），那么编译器可能会选择在栈上分配该对象。这是因为在栈上分配和释放内存的成本通常比在堆上更低。\n\n因此，即使使用 `new()` 分配内存，如果编译器通过逃逸分析确定该对象不会逃逸，它可能会在栈上分配该对象以提高性能。\n\n总结来说，`new()` 返回的对象不一定在堆上，具体取决于逃逸分析的结果。如果对象没有逃逸，编译器可能会选择在栈上分配它。\n",
        "title": "GoLang 中 new() 与内存管理机制详解",
        "abstract": "> 本文为与 GPT-4o 的对话记录，深入探讨 GoLang 中 new() 函数的使用以及相关的内存管理机制。\n在 Go 语言中，`new()` 函数用于分配内存并返回指向该内存的指针。`new()` 函数的行为是非常简单且安全的：它总是返回一个指向零值的指针。Go 语言的内存分配器会确保内存分配成功，因此在使用 `new()` 时，不会出现内存分配失败的情况。\n换句话说，`new()` 函数在 Go 中不会返回 `nil`，因为它总是能够成功分配内存并返回一个有效的指针。Go 语言的设计目标之一就是提供一个安全的内存管理机制，因此开发者不需要担心内存分配失败的问题。",
        "length": 29,
        "created_at": "2025-03-07T16:00:00.000Z",
        "updated_at": "2025-03-07T16:00:00.000Z",
        "tags": [
          "GoLang",
          "Memory Management"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-07-pytorch-model-save-and-load.md",
        "pagePath": "/learn_from_ai/pytorch-model-save-and-load",
        "slug": "pytorch-model-save-and-load"
      },
      "meta": {
        "content": "\n> 本文为与 DeepSeek 对话记录，深入探讨 PyTorch 中模型的保存和加载机制，包括不同的保存方式、最佳实践以及常见问题的解决方案。\n\n# Q: PyTorch 中所谓的保存模型/加载模型，指的是保存/加载什么？是 state_dict 吗？\n\n> [!reasoning]-\n>\n> 嗯，我现在要弄清楚 PyTorch 中保存和加载模型到底是怎么回事。用户问是不是保存和加载 state_dict，我需要仔细想一想这个问题。\n>\n> 首先，我记得在 PyTorch 里训练模型的时候，模型的可学习参数，比如各层的权重和偏置，是存放在模型的 parameters 中的。而 state_dict 应该是一个保存这些参数的东西。对，每个 torch.nn.Module 都有 state_dict，它是一个字典，将每个层映射到对应的参数张量。\n>\n> 那保存模型的时候，有两种常见的方法。一种是直接保存整个模型，使用 torch.save(model, 'model.pth')。这样的话，保存的是整个模型对象，包括结构定义和参数。不过这种方法可能有缺点，比如当模型定义所在的文件路径变化时，加载的时候可能会找不到类定义，导致错误。所以通常更推荐另一种方法，就是只保存 state_dict，这样需要的时候再加载到一个定义好的模型结构中。\n>\n> 所以用户的问题是说保存模型是不是保存 state_dict，对吧？是的，通常最佳做法是保存 state_dict，而不是整个模型对象。因为 state_dict 只包含参数，不包含模型结构，这样在加载的时候更灵活。比如，当你想在不同项目中使用同一个模型参数，或者模型类有修改时，加载 state_dict 会更方便。\n>\n> 那加载的时候，就需要先实例化模型，然后用 load_state_dict 方法加载参数。比如：\n>\n> model = MyModelClass(\\*args, \\*\\*kwargs)\n>\n> model.load_state_dict(torch.load('model.pth'))\n>\n> 这样参数就被正确加载到模型里了。而如果保存的是整个模型，加载的时候可以直接用 model = torch.load('model.pth')，但这时候模型类必须在当前作用域可访问，否则会出错。\n>\n> 所以总结一下，保存模型通常指的是保存 state_dict，而加载模型就是加载这个 state_dict 到已经定义好的模型结构中。当然，PyTorch 也允许保存整个模型，但不够灵活，所以推荐用 state_dict 的方式。\n>\n> 另外，可能还有其他需要保存的内容，比如优化器的 state_dict，这样在恢复训练的时候可以接着之前的优化器状态继续训练。这时候保存的 checkpoint 可能包括 model_state_dict、optimizer_state_dict、epoch、loss 等信息。加载的时候需要分别处理这些部分。\n>\n> 但用户的问题主要是关于保存和加载模型本身，所以核心答案应该是 state_dict。不过要说明保存整个模型的情况也存在，但 state_dict 更推荐。所以回答的时候应该涵盖这两种情况，但强调 state_dict 是更常用的方法。\n\n在 PyTorch 中，保存和加载模型的**核心是 `state_dict`**，但具体操作可以灵活选择。以下是详细说明：\n\n---\n\n### 1. **`state_dict` 是什么？**\n\n- **`state_dict`** 是一个 Python 字典对象，它将模型的每一层（如卷积层、全连接层等）映射到对应的参数张量（权重和偏置）。\n- 例如，一个简单的神经网络可能包含：\n  ```python\n  model.state_dict() = {\n      'conv1.weight': tensor(...),\n      'conv1.bias': tensor(...),\n      'fc1.weight': tensor(...),\n      'fc1.bias': tensor(...),\n      ...\n  }\n  ```\n- **优化器**（如 `torch.optim.SGD`）也有自己的 `state_dict`，保存优化器的状态（如动量缓冲等）。\n\n---\n\n### 2. **保存模型的两种方式**\n\n#### **方式 1：仅保存 `state_dict`（推荐）**\n\n- **代码示例**：\n  ```python\n  torch.save(model.state_dict(), 'model_weights.pth')\n  ```\n- **优点**：\n  - 文件体积小，只保存参数，不保存模型结构。\n  - 加载灵活：需先实例化模型结构，再加载参数。\n  - 避免因代码变更（如类名修改）导致的加载失败。\n\n#### **方式 2：保存整个模型（不推荐）**\n\n- **代码示例**：\n  ```python\n  torch.save(model, 'entire_model.pth')\n  ```\n- **缺点**：\n  - 文件体积大，包含模型结构和参数。\n  - 依赖原始代码环境（模型类的定义必须存在），否则加载时会报错。\n\n---\n\n### 3. **加载模型的对应方法**\n\n#### **加载 `state_dict`（需先构建模型结构）**\n\n```python\nmodel = MyModelClass()  # 必须与保存时的模型结构一致\nmodel.load_state_dict(torch.load('model_weights.pth'))\nmodel.eval()  # 切换到推理模式（关闭 dropout 等）\n```\n\n#### **加载整个模型（直接使用）**\n\n```python\nmodel = torch.load('entire_model.pth')\nmodel.eval()\n```\n\n---\n\n### 4. **实际场景中的最佳实践**\n\n- **保存训练检查点（Checkpoint）**：\n  通常需要保存 `state_dict` 之外的信息（如优化器状态、训练步数等）：\n\n  ```python\n  checkpoint = {\n      'model_state_dict': model.state_dict(),\n      'optimizer_state_dict': optimizer.state_dict(),\n      'epoch': 100,\n      'loss': loss,\n  }\n  torch.save(checkpoint, 'checkpoint.pth')\n  ```\n\n- **加载检查点**：\n  ```python\n  checkpoint = torch.load('checkpoint.pth')\n  model.load_state_dict(checkpoint['model_state_dict'])\n  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n  epoch = checkpoint['epoch']\n  ```\n\n---\n\n### 5. **关键总结**\n\n- **保存模型**：本质是保存模型的 `state_dict`（推荐），但也可以保存整个模型。\n- **加载模型**：\n  - 若保存的是 `state_dict`，需先实例化模型结构，再加载参数。\n  - 若保存的是整个模型，直接加载但需保证代码兼容性。\n- **灵活性**：使用 `state_dict` 更安全、更灵活，适合跨项目或模型结构可能变化的场景。\n\n# Q: model 的 config 又是什么东西？是超参吗？\n\n> [!reasoning]-\n>\n> 嗯，用户之前问过 PyTorch 中保存和加载模型的概念，现在又接着问\"model 的 config 是什么东西？是超参吗？\"我需要仔细分析这个问题，确保理解用户的真实需求。\n>\n> 首先，用户已经了解 state_dict，现在转向模型配置（config）。可能他们在实际项目中遇到了需要保存或管理模型结构的情况，或者在阅读代码时遇到了 config 这个术语。用户想知道 config 具体指什么，是否包括超参数。\n>\n> 接下来，我需要明确\"model config\"的定义。在机器学习中，模型配置通常指的是定义模型结构的参数，比如神经网络的层数、每层的神经元数量、激活函数类型等。而超参数可能包括学习率、批量大小、优化器类型等训练过程中的设置。但有时候这两者之间的界限并不严格，特别是当某些结构参数也被视为超参数时。\n>\n> 用户可能混淆了模型配置和超参数，需要区分这两者。需要举例说明，比如在 Transformer 模型中，config 可能包含隐藏层大小、头数等，而超参数则是训练时的设置。但有时，例如在 HuggingFace 的库中，模型配置类（如 BertConfig）确实包含了模型结构的所有参数，这可能让用户产生疑问。\n>\n> 另外，用户可能想知道为什么需要单独保存 config。可能他们在保存模型时，除了 state_dict 之外，还需要保存模型结构的信息，以便重新实例化模型。比如，如果只保存 state_dict，加载时需要先有模型类的定义和 config 参数，才能正确创建模型实例，然后加载权重。\n>\n> 需要解释 config 的作用：它定义了模型的结构，是重建模型实例的必要信息。与超参数不同，config 主要涉及模型架构，而超参数涉及训练过程。但有些情况下，两者可能有重叠，比如学习率可能被包含在训练配置中，而层数在模型配置中。\n>\n> 还需要考虑用户可能的实际应用场景。比如，使用 HuggingFace 的 Transformers 库时，模型配置被明确分离，用户需要保存 config.json 和 model.bin。这时候，config 保存模型结构，model.bin 保存 state_dict。用户可能在这种情况下遇到 config 的概念，需要解释这种设计的原因和好处。\n>\n> 此外，用户可能想知道如何管理 config，比如使用 YAML 或 JSON 文件来存储模型结构参数，方便版本控制和实验复现。这也是为什么分开保存 config 和 state_dict 更灵活，因为修改结构时只需调整 config 而不影响训练好的权重。\n>\n> 最后，要确保回答结构清晰，分点说明，给出代码示例，并比较 config 与超参数的区别和联系。同时，注意用户可能的技术水平，避免使用过于专业的术语，保持解释的易懂性。\n\n在机器学习中，**模型的 `config`（配置）** 通常指**定义模型结构的所有参数**，而超参（Hyperparameters）更多指**控制训练过程的参数**。这两者有时会有重叠，但核心区别在于作用对象。以下是详细解释：\n\n---\n\n### 1. **什么是模型的 `config`？**\n\n- **模型配置**（Model Configuration）是定义**模型结构**的一组参数，例如：\n\n  - 神经网络的层数、每层的神经元数量\n  - 卷积核大小、步长（Stride）、填充（Padding）\n  - 激活函数类型（如 ReLU、Sigmoid）\n  - 注意力机制的头数（如 Transformer 中的 `num_heads`）\n  - 词嵌入维度（如 NLP 模型中的 `hidden_size`）\n\n- **示例**（以 Transformer 模型为例）：\n  ```python\n  config = {\n      \"num_layers\": 12,          # Transformer 层数\n      \"hidden_size\": 768,         # 隐藏层维度\n      \"num_attention_heads\": 12,  # 注意力头数\n      \"intermediate_size\": 3072,  # FeedForward 层维度\n      \"vocab_size\": 30522,        # 词表大小\n  }\n  ```\n\n---\n\n### 2. **模型 `config` 与超参数的区别**\n\n| **类别**     | **模型配置（Config）**                 | **超参数（Hyperparameters）**                                  |\n| ------------ | -------------------------------------- | -------------------------------------------------------------- |\n| **作用对象** | 模型结构                               | 训练过程                                                       |\n| **典型参数** | 层数、维度、注意力头数、卷积核大小     | 学习率、批量大小（Batch Size）、优化器类型、训练轮数（Epochs） |\n| **保存目的** | 重建模型结构（如加载 `state_dict` 前） | 复现训练过程（如调参、实验对比）                               |\n| **修改频率** | 低频（模型结构稳定后很少改动）         | 高频（需反复调整优化）                                         |\n\n---\n\n### 3. **为什么需要单独管理 `config`？**\n\n- **场景 1：重建模型**  \n  若仅保存 `state_dict`（模型权重），加载时必须**先根据 `config` 重建模型结构**，才能加载权重：\n\n  ```python\n  # 保存\n  torch.save(model.state_dict(), \"model_weights.pth\")\n  with open(\"model_config.json\", \"w\") as f:\n      json.dump(config, f)\n\n  # 加载\n  with open(\"model_config.json\", \"r\") as f:\n      config = json.load(f)\n  model = MyModel(**config)  # 根据 config 实例化模型\n  model.load_state_dict(torch.load(\"model_weights.pth\"))\n  ```\n\n- **场景 2：版本控制与实验复现**  \n  将 `config` 保存为 JSON/YAML 文件，便于记录不同模型结构的实验效果。\n\n- **场景 3：共享模型**  \n  开源社区常用模式（如 HuggingFace Transformers）：\n  - `config.json`：定义模型结构\n  - `pytorch_model.bin`：保存 `state_dict`\n\n---\n\n### 4. **`config` 与超参数的重叠情况**\n\n某些参数可能同时影响模型结构和训练，例如：\n\n- **Dropout 比率**：既是模型结构的一部分，也被视为超参数。\n- **词嵌入维度**：影响模型结构，但可能因训练资源调整（如显存不足时调小维度）。\n\n此时，可以**将两者统一管理**，但建议分开存储：\n\n```python\n# 统一配置文件（config.yaml）\nmodel:\n  num_layers: 12\n  hidden_size: 768\ntraining:\n  learning_rate: 1e-4\n  batch_size: 32\n```\n\n---\n\n### 5. **实际代码示例**\n\n#### 定义模型时使用 `config`：\n\n```python\nclass TransformerModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embedding = nn.Embedding(config[\"vocab_size\"], config[\"hidden_size\"])\n        self.layers = nn.ModuleList([TransformerBlock(config) for _ in range(config[\"num_layers\"])])\n        # ...\n\n# 初始化模型\nconfig = {\"vocab_size\": 30000, \"hidden_size\": 512, \"num_layers\": 6}\nmodel = TransformerModel(config)\n```\n\n#### 保存与加载完整模型（含结构和权重）：\n\n```python\n# 保存（不推荐，但某些场景需要）\nfull_model = {\"config\": config, \"state_dict\": model.state_dict()}\ntorch.save(full_model, \"full_model.pth\")\n\n# 加载\nloaded = torch.load(\"full_model.pth\")\nmodel = TransformerModel(loaded[\"config\"])\nmodel.load_state_dict(loaded[\"state_dict\"])\n```\n\n---\n\n### 6. **总结**\n\n- **模型 `config`**：定义模型结构的参数（如层数、维度），是**重建模型**的必需信息。\n- **超参数**：控制训练过程的参数（如学习率、批量大小），影响模型如何学习。\n- **最佳实践**：\n  - 始终保存 `config`（如 JSON 文件），确保模型可复现。\n  - 分开管理模型配置和训练超参数，避免混淆。\n\n> [!info] 亮曰\n>\n> - Config 主要是定义模型结构。会根据 Config 决定有多少层等。\n> - 超参主要是训练过程中的学习率、 Epochs 等，复现训练过程时需要，但与模型结构无关，只与训练过程有关。\n\n# Q: 所谓\"权重\"就是指 state_dict 中的内容吗？\n\n> [!reasoning]-\n>\n> 嗯，用户问：\"所谓'权重'就是指 state_dict 中的内容吗？\"这个问题看起来是想确认 PyTorch 中的权重和 state_dict 之间的关系。首先，我需要回顾一下之前的对话，确保回答的一致性。\n>\n> 之前已经解释了 state_dict 是保存模型参数的核心，里面包含了各层的权重和偏置。用户可能现在对\"权重\"这个术语的具体指向有疑问，需要明确权重是否等同于 state_dict 中的所有内容。\n>\n> 首先，我应该明确\"权重\"在机器学习中的定义。通常，权重指的是模型中的可学习参数，比如线性层中的权重矩阵和偏置向量。而 state_dict 不仅包括这些权重，还可能包括其他参数，比如 BatchNorm 层的均值和方差，或者优化器的状态。因此，state_dict 的内容可能比单纯的\"权重\"更广泛。\n>\n> 接下来，需要区分 state_dict 中的不同部分。模型和优化器都有自己的 state_dict。模型的 state_dict 包含各层的参数，而优化器的 state_dict 包含优化器的状态，如动量缓存等。用户的问题可能集中在模型的 state_dict 中的权重部分，而是否包括其他参数需要澄清。\n>\n> 另外，用户可能对如何正确保存和加载模型有更深入的理解需求。比如，保存 state_dict 时，是否只保存了权重，还是还有其他必要的参数？需要说明 state_dict 中的内容对于模型重建的重要性，以及权重在其中的角色。\n>\n> 可能用户在实际操作中遇到了问题，比如加载模型时结构不匹配，或者参数未正确加载，因此需要明确 state_dict 中的内容。需要举例说明，比如一个简单的线性模型，展示其 state_dict 中的键值对，明确哪些是权重，哪些是偏置，或者其他类型的参数。\n>\n> 还需要考虑用户可能的误解，比如认为 state_dict 仅包含权重，而忽略了其他必要的参数，导致模型无法正确运行。因此，强调 state_dict 包含所有必要的参数，而权重是其中的一部分，但还有其他如偏置、BatchNorm 的参数等。\n>\n> 此外，用户可能关心如何仅保存或操作权重，比如迁移学习中的部分参数加载。这时候需要说明如何从 state_dict 中选择性地提取或加载特定的权重部分。\n>\n> 总结来说，回答需要明确以下几点：\n>\n> 1. 权重是模型中的可学习参数，如线性层的权重矩阵。\n>\n> 2. state_dict 包含模型的所有参数，包括权重、偏置以及其他层特定的参数。\n>\n> 3. 模型的 state_dict 与优化器的 state_dict 不同，后者包含优化器状态。\n>\n> 4. 正确加载模型需要确保 state_dict 的结构与当前模型结构匹配。\n>\n> 5. 示例说明 state_dict 中的具体内容，帮助用户直观理解。\n>\n> 同时，要注意用词准确，避免混淆，比如\"参数\"和\"权重\"的区别，确保用户能清晰理解两者的包含关系。可能还需要提到在保存和加载时的最佳实践，比如如何结合 config 来重建模型结构，再加载 state_dict 中的参数，以确保模型的正确性。\n>\n> 最后，检查是否有遗漏点，比如是否提到 eval()模式的影响，或者训练/推理时加载参数的注意事项，但根据问题范围，可能不需要展开，除非用户有进一步的问题。\n\n在 PyTorch 中，**\"权重\"确实是 `state_dict` 中的核心内容**，但 `state_dict` 包含的信息可能比\"权重\"更广泛。以下是详细解释：\n\n---\n\n### 1. **\"权重\"是什么？**\n\n- **权重（Weights）** 是模型中**可学习的参数**，例如：\n  - 全连接层（`nn.Linear`）中的权重矩阵（`weight`）和偏置（`bias`）。\n  - 卷积层（`nn.Conv2d`）的卷积核参数。\n  - 嵌入层（`nn.Embedding`）的词向量矩阵。\n- 这些参数通过反向传播和优化器（如 SGD、Adam）在训练过程中逐步更新。\n\n---\n\n### 2. **`state_dict` 中的内容**\n\n`state_dict` 是 PyTorch 中模型或优化器的**状态字典**，具体包含：\n\n- **对模型而言**：\n  - 所有权重（`weight`）、偏置（`bias`）。\n  - 其他**与层相关的参数**，例如：\n    - `BatchNorm` 层的 `running_mean` 和 `running_var`（在推理时使用的均值和方差）。\n    - `LSTM` 或 `GRU` 中的隐藏状态缓存（如果模型设计时显式存储了这些状态）。\n  ```python\n  # 示例：一个简单模型的 state_dict\n  model = nn.Sequential(\n      nn.Linear(10, 20),\n      nn.ReLU(),\n      nn.BatchNorm1d(20)\n  )\n  print(model.state_dict().keys())\n  # 输出：\n  # ['0.weight', '0.bias', '2.weight', '2.bias', '2.running_mean', '2.running_var', '2.num_batches_tracked']\n  ```\n- **对优化器而言**：\n  - 优化器的状态（如动量缓存、历史梯度等）。\n  ```python\n  optimizer = torch.optim.Adam(model.parameters())\n  print(optimizer.state_dict().keys())\n  # 输出：\n  # ['state', 'param_groups']\n  ```\n\n---\n\n### 3. **\"权重\"与 `state_dict` 的关系**\n\n- **`state_dict` 包含权重，但不仅限于权重**：\n  - 权重是 `state_dict` 中的一部分（如 `conv1.weight`, `fc.bias`）。\n  - `state_dict` 还可能包含**非可学习的参数**（如 `BatchNorm` 的统计量 `running_mean`）。\n  - 对于复杂模型（如 Transformer），还可能包含位置编码、注意力掩码等参数。\n- **总结**：\n  - **权重是 `state_dict` 的核心内容**，但 `state_dict` 是更全面的模型状态表示。\n\n---\n\n### 4. **为什么需要 `state_dict` 而不仅仅是权重？**\n\n- **完整恢复模型状态**：\n  如果仅保存权重，忽略 `BatchNorm` 的 `running_mean` 等参数，在推理时会导致统计量错误，影响模型性能。\n- **兼容性**：\n  加载模型时需确保 `state_dict` 的键（如层名称）与当前模型结构严格匹配，否则会报错 `Missing keys` 或 `Unexpected keys`。\n- **灵活性**：\n  可以手动修改 `state_dict`，例如迁移学习中仅加载部分层的权重。\n\n---\n\n### 5. **操作 `state_dict` 的常见场景**\n\n#### **(1) 查看模型权重**\n\n```python\nfor name, param in model.named_parameters():\n    print(f\"层名: {name}, 参数形状: {param.shape}\")\n```\n\n#### **(2) 选择性加载部分权重（迁移学习）**\n\n```python\npretrained_dict = torch.load('pretrained.pth')\nmodel_dict = model.state_dict()\n\n# 筛选出名称和形状匹配的权重\npretrained_dict = {\n    k: v for k, v in pretrained_dict.items()\n    if k in model_dict and v.shape == model_dict[k].shape\n}\nmodel_dict.update(pretrained_dict)\nmodel.load_state_dict(model_dict)\n```\n\n#### **(3) 手动修改权重**\n\n```python\n# 将第一层卷积的权重置零\nwith torch.no_grad():\n    model.conv1.weight.copy_(torch.zeros_like(model.conv1.weight))\n```\n\n---\n\n### 6. **总结**\n\n- **权重**是模型的可学习参数，是 `state_dict` 的核心内容。\n- **`state_dict`** 是模型状态的完整表示，包括：\n  - 所有权重、偏置。\n  - 非可学习的运行时参数（如 `BatchNorm` 的统计量）。\n- **最佳实践**：\n  - 保存模型时使用 `torch.save(model.state_dict(), ...)`。\n  - 加载时需先构建相同结构的模型，再调用 `model.load_state_dict()`。\n  - 若需调整模型结构，可选择性加载匹配的权重。\n\n> [!info] 亮曰\n>\n> state_dict 包括 Parameter 与 Buffer ，模型的可学习参数——**权重**主要指其中的 Parameter 。\n",
        "title": "PyTorch 模型的保存与加载机制详解",
        "abstract": "> 本文为与 DeepSeek 对话记录，深入探讨 PyTorch 中模型的保存和加载机制，包括不同的保存方式、最佳实践以及常见问题的解决方案。\n> [!reasoning]-\n>",
        "length": 433,
        "created_at": "2025-03-07T13:00:00.000Z",
        "updated_at": "2025-03-07T13:00:00.000Z",
        "tags": [
          "Deep Learning",
          "PyTorch",
          "Model Management"
        ],
        "license": true
      }
    },
    {
      "pathMapping": {
        "filePath": "public/content/learn_from_ai/2025-03-07-resnet-math-and-gradient-vanishing.md",
        "pagePath": "/learn_from_ai/resnet-math-and-gradient-vanishing",
        "slug": "resnet-math-and-gradient-vanishing"
      },
      "meta": {
        "content": "\n> 本文深入探讨了 ResNet 的数学原理，分析了深度神经网络中的梯度消失问题及其解决方案。通过数学推导和直观解释，帮助读者理解残差连接的作用机制，以及它如何有效地解决了深度神经网络训练中的关键问题。\n\n## Q: ResNet 的数学原理与作用\n\n> [!query]\n>\n> 深入解释 ResNet 的数学原理与作用。提示：你可以使用公式、Mermaid 图表、表格等方式使内容更容易理解。\n\n> [!reasoning]-\n>\n> 嗯，用户让我深入解释 ResNet 的数学原理和作用，还提示可以用公式、图表、表格等。我得先回忆一下 ResNet 的基本概念，然后想想怎么组织内容。\n>\n> 首先，ResNet 的核心是残差学习，对吧？我记得何恺明他们提出的，解决了深度网络中的梯度消失问题。那残差块的结构是关键，应该要详细解释。可能需要画个残差块的示意图，用 Mermaid 的话，应该可以画个简单的框图，比如输入分成两部分，一个经过卷积层，另一个是 shortcut，然后相加。\n>\n> 数学原理方面，残差块的公式是 H(x) = F(x) + x，这里 F(x)是残差函数，目标是让网络学习 F(x) = H(x) - x，而不是直接学习 H(x)。这样在深层网络中，梯度更容易回传，因为加法操作让梯度可以直接流过，对吧？这部分需要用公式展示，可能用 LaTeX 写出来。\n>\n> 然后，为什么残差有效？可能得从反向传播的角度分析，比如导数的计算。当损失函数对 x 求导时，会有两条路径，一个是残差层的导数，另一个是恒等映射的导数，这样即使残差层的梯度很小，至少还有 1 的部分，防止梯度消失。这点需要详细说明，可能用链式法则的公式。\n>\n> 作用方面，ResNet 允许训练更深的网络，比如 ResNet-152，而不会出现退化问题。传统网络层数加深时，准确率会饱和甚至下降，而 ResNet 通过残差学习解决了这个问题。可能需要用图表比较传统网络和 ResNet 在深度增加时的表现差异，或者引用论文中的实验数据。\n>\n> 另外，ResNet 的结构变体，比如不同的残差块设计，如 BasicBlock 和 Bottleneck。Bottleneck 用了 1x1 卷积来降维和升维，减少计算量，这里可以用表格对比两者的结构参数，比如卷积层的大小、通道数变化等。\n>\n> 可能用户还想知道 ResNet 的应用场景，比如在图像分类、目标检测等任务中的效果。不过问题主要是数学原理和作用，这部分可以简要提及，但重点还是原理。\n>\n> 需要检查有没有遗漏的关键点，比如恒等映射的重要性，如果 shortcut 需要调整维度怎么办？这时候可能用 1x1 卷积来匹配维度，这也应该在结构部分说明。\n>\n> 还有，初始化权重的问题，残差网络是否让初始化更容易？可能提到参数初始化的影响减小，因为残差结构本身有助于稳定训练。\n>\n> 总结一下，结构部分用 Mermaid 图，数学部分用公式和导数推导，作用部分用对比和实验数据，结构变体用表格。这样内容会比较全面，也符合用户的要求。\n\n好的！我将从数学原理、网络结构、作用机制等方面深入解释 ResNet，并结合公式、图表和示例进行说明。\n\n---\n\n### 1. ResNet 的核心思想：残差学习\n\nResNet（残差网络）的核心思想是通过引入 **残差块（Residual Block）** 来解决深度神经网络中的梯度消失和网络退化问题。其数学原理可以概括为：\n\n#### 残差公式\n\n假设我们希望网络学习的映射为 $$ H(x) $$，ResNet 将其拆解为：\n\n$$\nH(x) = F(x) + x\n$$\n\n其中：\n\n- $$ x $$ 是输入\n- $$ F(x) = H(x) - x $$ 是残差函数\n\n网络只需学习残差 $$ F(x) $$，而不是直接学习 $$ H(x) $$。这种设计使得深层网络的训练更加稳定。\n\n---\n\n### 2. 残差块的数学实现\n\n#### 残差块结构\n\n一个典型的残差块由以下组件构成：\n\n1. **卷积层**（用于特征提取）\n2. **跳跃连接（Shortcut Connection）**（将输入直接传递到输出）\n\n#### 结构示例（Mermaid 流程图）：\n\n```mermaid\ngraph LR\n    A[输入 x] --> B[卷积层]\n    B --> C[激活函数 ReLU]\n    C --> D[卷积层]\n    D --> E[ReLU]\n    E --> F[\"残差 F(x)\"]\n    A --> G[跳跃连接]\n    F --> H[\"F(x) + x\"]\n    H --> I[\"输出 H(x)\"]\n```\n\n#### 数学表达式\n\n对于第 $$ l $$ 层的残差块：\n\n$$\nx\\_{l+1} = f(x_l + \\mathcal{F}(x_l, W_l))\n$$\n\n其中：\n\n- $$ \\mathcal{F}(x_l, W_l) $$ 是残差函数（由卷积层和激活函数组成）\n- $$ f $$ 是激活函数（通常为 ReLU）\n\n---\n\n### 3. 为什么残差有效？\n\n#### 梯度传播分析\n\n假设损失函数为 $$ L $$，反向传播时梯度计算为：\n\n$$\n\\frac{\\partial L}{\\partial x*l} = \\frac{\\partial L}{\\partial x*{l+1}} \\cdot \\left( 1 + \\frac{\\partial \\mathcal{F}(x_l, W_l)}{\\partial x_l} \\right)\n$$\n\n- 即使 $$ \\frac{\\partial \\mathcal{F}}{\\partial x_l} $$ 趋近于 0，梯度仍然可以通过 $$ 1 $$ 传递（避免梯度消失）\n- 这种设计使得超深层网络（如 ResNet-152）的训练成为可能。\n\n#### 与传统网络的对比\n\n| 特性         | 传统网络         | ResNet           |\n| ------------ | ---------------- | ---------------- |\n| 梯度传播     | 逐层衰减         | 通过残差保持稳定 |\n| 退化问题     | 随深度增加而显著 | 显著缓解         |\n| 最大有效深度 | ~20 层           | 1000+ 层         |\n\n---\n\n### 4. ResNet 的网络结构\n\n#### 典型变体（以 ImageNet 为例）\n\n| 模型       | 层数 | 残差块类型 | 参数数量 |\n| ---------- | ---- | ---------- | -------- |\n| ResNet-18  | 18   | BasicBlock | 11.7M    |\n| ResNet-34  | 34   | BasicBlock | 21.8M    |\n| ResNet-50  | 50   | Bottleneck | 25.6M    |\n| ResNet-101 | 101  | Bottleneck | 44.5M    |\n| ResNet-152 | 152  | Bottleneck | 60.2M    |\n\n#### 两种残差块设计\n\n1. **BasicBlock**（用于浅层网络）：\n\n   ```mermaid\n   graph LR\n       A[输入] --> B[3x3 卷积]\n       B --> C[ReLU]\n       C --> D[3x3 卷积]\n       D --> E[与输入相加]\n       E --> F[输出]\n   ```\n\n2. **Bottleneck**（用于深层网络，减少计算量）：\n   ```mermaid\n   graph LR\n       A[输入] --> B[1x1 卷积降维]\n       B --> C[3x3 卷积]\n       C --> D[1x1 卷积升维]\n       D --> E[与输入相加]\n       E --> F[输出]\n   ```\n\n---\n\n### 5. ResNet 的作用\n\n#### 主要优势\n\n1. **解决梯度消失**：通过跳跃连接传递原始信号。\n2. **缓解网络退化**：即使网络加深，性能不会显著下降。\n3. **灵活性**：适用于多种任务（分类、检测、分割等）。\n\n#### 实验对比（ImageNet 2015）\n\n| 模型       | Top-5 错误率 | 深度 |\n| ---------- | ------------ | ---- |\n| VGG-19     | 8.43%        | 19   |\n| ResNet-34  | 5.71%        | 34   |\n| ResNet-152 | 4.49%        | 152  |\n\n---\n\n### 6. 数学扩展：残差网络的通用性\n\n残差思想可推广到其他操作：\n\n$$\nH(x) = F(x) + \\mathcal{G}(x)\n$$\n\n其中 $$ \\mathcal{G}(x) $$ 可以是：\n\n- 卷积（当输入输出维度不同时）\n- 池化（用于下采样）\n\n---\n\n### 总结\n\nResNet 通过引入残差学习，解决了深度神经网络的训练难题，其核心数学原理是通过跳跃连接保留原始信号，使网络能够高效地学习残差映射。这一设计不仅提升了模型性能，还推动了深度学习在计算机视觉领域的进一步发展。\n\n## Q: 深度网络梯度消失的原理及解决方案\n\n> [!query]\n>\n> 深度网络中，梯度消失是指什么？为什么会出现梯度消失？深入讲述一下其原理。提示：你可以使用公式、Mermaid 图表、表格等方式使内容更容易理解。\n\n> [!reasoning]-\n>\n> 嗯，用户问的是深度网络中的梯度消失问题，需要详细解释它的原理，还有为什么会出现。首先，我得回想一下梯度消失的基本概念。梯度消失是指在训练深度神经网络时，反向传播过程中梯度逐渐变小，导致浅层网络的权重更新缓慢甚至停止，使得模型难以训练。这可能是因为激活函数的选择，比如 sigmoid 或者 tanh，它们的导数在输入值较大或较小时会趋近于零，导致梯度逐层衰减。\n>\n> 接下来，我得考虑如何组织内容。用户希望深入讲解原理，可能需要用公式来说明梯度是如何在反向传播中累积的。比如，链式法则中的各层导数相乘，如果每个导数都小于 1，多次相乘后会趋近于零。这时候可以举一个具体的例子，比如用 sigmoid 激活函数，其导数最大为 0.25，所以经过多层后梯度会指数级下降。\n>\n> 然后，需要分析为什么会出现这种情况。可能的原因包括激活函数的选择、权重初始化不当、网络结构过深等。这里可以对比不同的激活函数，比如 ReLU 如何缓解梯度消失，因为它的导数是 1 或 0，不会导致梯度衰减。另外，初始化方法如 He 初始化也有帮助，但根本原因还是在于链式法则中的乘积效应。\n>\n> 用户还提到可以使用公式、图表或表格，所以可能需要画一个简单的计算图，或者用数学公式展示梯度如何逐层传递。例如，假设每一层的导数是 σ’(z)，那么经过 n 层后，梯度就是初始梯度乘以(σ’(z))^n，如果 σ’(z) < 1，就会指数下降。\n>\n> 另外，可能需要提到 LSTM 和残差连接（ResNet）是如何解决梯度消失的。比如 LSTM 通过门控机制保持梯度流动，而残差连接提供了短路路径，使得梯度可以直接回传，减少了相乘的次数。\n>\n> 在结构安排上，可能需要先定义梯度消失，然后分析原因，再深入数学原理，最后讨论解决方案。需要注意用词准确，比如区分梯度消失和梯度爆炸，虽然都是梯度不稳定问题，但方向不同。\n>\n> 还需要检查是否有遗漏的关键点，比如网络深度的影响，或者不同优化器的效果，比如 Adam 是否有助于缓解梯度消失。不过可能主要原因是激活函数和链式法则，优化器可能影响不大，但可以简单提及。\n>\n> 最后，确保解释清晰，逐步引导读者理解梯度消失的机制，并通过示例或图表帮助理解。可能还需要比较不同激活函数的导数，比如 sigmoid、tanh 和 ReLU，用表格或图表显示它们的导数范围，从而说明为什么 ReLU 更适合深层网络。\n\n### 深度网络中梯度消失的原理分析\n\n#### 1. 梯度消失的定义\n\n梯度消失（Vanishing Gradient）是指深度神经网络在反向传播过程中，梯度随着层数的增加呈现指数级衰减的现象，导致浅层网络的参数更新幅度极小甚至无法更新。\n\n---\n\n#### 2. 梯度消失的数学原理\n\n**反向传播链式法则**是核心机制。假设网络有 $$ L $$ 层，损失函数为 $$ \\mathcal{L} $$，第 $$ l $$ 层的权重梯度为：\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial a^{(L)}} \\cdot \\prod\\_{k=l}^{L-1} \\left( \\frac{\\partial a^{(k+1)}}{\\partial a^{(k)}} \\right) \\cdot \\frac{\\partial a^{(l)}}{\\partial W^{(l)}}\n$$\n\n其中 $$ a^{(k)} $$ 是第 $$ k $$ 层的激活值。梯度由多个雅可比矩阵乘积组成，若每项 $$ \\frac{\\partial a^{(k+1)}}{\\partial a^{(k)}} < 1 $$，则梯度会指数衰减。\n\n---\n\n#### 3. 梯度消失的根源\n\n##### (1) 激活函数的选择\n\n- **Sigmoid/Tanh 函数**：导数范围分别为 $$ (0, 0.25] $$ 和 $$ (0, 1] $$，导致梯度逐层缩小。\n  $$\n  \\text{Sigmoid 导数: } \\sigma'(x) = \\sigma(x)(1-\\sigma(x))\n  $$\n  $$\n  \\text{Tanh 导数: } \\tanh'(x) = 1 - \\tanh^2(x)\n  $$\n- **ReLU 函数**：导数为 0 或 1，梯度可稳定传递（但可能引发神经元死亡）。\n\n##### (2) 权重初始化问题\n\n- 若权重 $$ W $$ 初始值过小，则 $$ \\prod W $$ 的乘积会导致输出收缩。\n- 例如：使用均值为 0、方差过小的正态分布初始化时，激活值会逐层坍缩。\n\n##### (3) 深层网络的链式乘积\n\n- 梯度计算涉及 $$ \\prod\\_{k=l}^L \\frac{\\partial a^{(k)}}{\\partial a^{(k-1)}} $$。\n- 假设每层梯度因子为 $$ \\alpha < 1 $$，则总梯度为 $$ \\alpha^{L-l} $$，指数级衰减。\n\n---\n\n#### 4. 梯度消失的直观演示\n\n```mermaid\ngraph LR\n  A[输入层] --> B[隐藏层1: 梯度=0.8]\n  B --> C[隐藏层2: 梯度=0.6]\n  C --> D[...]\n  D --> E[隐藏层N: 梯度=0.2]\n  E --> F[输出层]\n\n  style A fill:#f9f\n  style F fill:#f9f\n```\n\n---\n\n#### 5. 关键参数对比表\n\n| 因素             | 对梯度的影响             | 典型示例                        |\n| ---------------- | ------------------------ | ------------------------------- |\n| 激活函数导数范围 | 导数越小，梯度衰减越快   | Sigmoid（最大导数 0.25）        |\n| 网络深度         | 层数越多，乘积效应越显著 | 100 层 vs 5 层网络              |\n| 权重初始化方差   | 方差过小导致激活值坍缩   | He 初始化 vs 零均值小方差初始化 |\n\n---\n\n#### 6. 解决方案\n\n1. **激活函数优化**：使用 ReLU、Leaky ReLU 等梯度保持能力强的函数。\n2. **残差连接（ResNet）**：通过跳跃连接绕过非线性变换，直接传递梯度。\n3. **批归一化（BatchNorm）**：稳定激活值分布，减缓梯度衰减。\n4. **梯度裁剪/自适应优化器**：间接缓解梯度不稳定问题（如 Adam）。\n\n---\n\n#### 7. 数学示例\n\n假设 5 层网络使用 Sigmoid 激活，每层梯度因子为 0.25：\n\n$$\n\\text{总梯度} = \\text{初始梯度} \\times (0.25)^5 = \\text{初始梯度} \\times 0.00098\n$$\n\n浅层权重更新几乎停滞。\n\n---\n\n通过理解梯度消失的数学机制和工程优化方法，可有效设计更稳定的深度网络结构。\n",
        "title": "ResNet 的数学原理与梯度消失问题分析",
        "abstract": "> 本文深入探讨了 ResNet 的数学原理，分析了深度神经网络中的梯度消失问题及其解决方案。通过数学推导和直观解释，帮助读者理解残差连接的作用机制，以及它如何有效地解决了深度神经网络训练中的关键问题。\n> [!query]\n>",
        "length": 309,
        "created_at": "2025-03-07T10:00:00.000Z",
        "updated_at": "2025-03-07T10:00:00.000Z",
        "tags": [
          "Deep Learning",
          "Neural Networks",
          "ResNet",
          "Mathematics"
        ],
        "license": true
      }
    }
  ]
}